[{"id":"19444a03784a5e93538b32ece3254a9f","title":"AI绘画之复原神秘的楼兰古国【从辉煌到遍地黄沙】","content":"楼兰，楼兰国是西域古国名（遗址在今新疆罗布泊西北岸）。西汉昭帝（公元前77年）前的楼兰既指楼兰国，而西汉昭帝后的楼兰则指的是楼兰城（今亦称楼兰故城）。\n公元前3世纪时，当时楼兰受月氏统治。公元前177年至公元前176年，匈奴打败了月氏，楼兰又为匈奴所管辖。公元前77年楼兰国王尉屠耆更名鄯善国，并迁都伊循城，向汉朝称臣，原都城楼兰古城则由汉朝派兵屯田。西南通且末、精绝、拘弥、于阗，北通车师，西北通焉耆，东当白龙堆，通敦煌，扼丝绸之路的要冲。由于孔雀河的改道，罗布泊水萎缩，生存环境日益恶劣。公元422年以后，楼兰城民众迫于严重干旱，于前凉末遗弃楼兰城，逐渐南移。此后楼兰先后并吞了若羌、小宛、精绝、且末等国，成为西域七强之一。公元448年鄯善归属北魏，后实行郡县制治理。公元492年被高车所灭。1900年，瑞典探险家斯文·赫定发现楼兰故城遗迹。\n!grid-0019.jpg!grid-0010.jpg!\n","slug":"aigc4","date":"2023-10-20T15:36:31.930Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"King's OvO²"},{"id":"ffbd048b2d61dd97e5d6fd9f1e2a77c3","title":"AIGC与大模型技术汇总","content":"\r\nAIGC项目展示\r\n  -->\r\n\r\n\r\n\r\n\r\n\r\n\r\n一、AIGC技术\r\n这里收集了关于AIGC的各种精选教程和资源，既适合初学者也适合进阶AI爱好者。\r\n📜 目录\r\n\r\n👋 入门\r\n💬 大语言模型\r\n\r\n💡 提示工程\r\n🔧 大语言模型实践\r\n🔬 大语言模型理论\r\n\r\n🎨 AI绘画\r\n\r\n🧑‍🎨 艺术基础与AI绘画技巧\r\n🌊 Stable\r\nDiffusion原理与应用\r\n\r\n🔊 AI音频\r\n🌈 多模态\r\n🧠 深度学习\r\n💻 AI系统\r\n🗂 其他\r\n\r\n👋 入门\r\n\r\nAI\r\nfor Everyone - 吴恩达  \r\nPractical\r\nAI for Teachers and Students - 沃顿商学院  \r\nArtificial\r\nIntelligence for Beginners - 微软 \r\nGenerative AI\r\nlearning path - 谷歌  \r\n\r\n💡 提示工程\r\n\r\nChatGPT\r\nPrompt Engineering for Developers - DeepLearning.AI  \r\nBuilding\r\nSystems with the ChatGPT API - DeepLearning.AI  \r\nLangChain\r\nfor LLM Application Development - DeepLearning.AI  \r\nLangChain:\r\nChat with Your Data - DeepLearning.AI  \r\nPrompt\r\nEngineering for ChatGPT - 范德堡大学  \r\nLearn Prompting \r\nLangChain\r\nAI Handbook - James Briggs, Francisco Ingham  \r\n\r\n🔧 大语言模型实践\r\n\r\nLLM\r\nBootcamp - The Full Stack  \r\nFinetuning\r\nLarge Language Models - DeepLearning.AI  \r\n\r\n🔬 大语言模型理论\r\n\r\nCS324 -\r\nAdvances in Foundation Models - 斯坦福大学 \r\n\r\n斯坦福大学关于大模型的新课，主要材料是一些notes，介绍了大语言模型的基础知识、能力范围、训练部署以及一些大模型相关的问题（数据安全、法律、危害等），总体来说比较简单，适合入门。2023年的版本对课纲进行了更新，增加了关于图像-文本和多模态的大模型内容。\r\n\r\nCS\r\n601.471/671 NLP: Self-supervised Models - 约翰霍普金斯大学 \r\n\r\nJHU也是NLP大牛校，这门课难度适中，课程主页上各类资源还挺多的，建议大家看一看。\r\n\r\nCS224N: Natural\r\nLanguage Processing with Deep Learning - 斯坦福大学  \r\n\r\n这门课Christopher\r\nManning在斯坦福开了很多年，很经典的课程。前面是NLP的基础知识，后面几节课会涉及到大语言模型。\r\n\r\nSpeech and\r\nLanguage Processing - Dan Jurafsky and James H. Martin  \r\n\r\n最经典的NLP教材，本来计划在大概三四年前就完稿的，但是由于近几年NLP领域发展实在太快，作者干脆就不设DDL了，一直在持续更新中。\r\n\r\nCOS\r\n597G (Fall 2022): Understanding Large Language Models - 普林斯顿大学\r\n\r\n\r\nDanqi\r\nChen的课，课程难度较高，主要材料是PPT和相关的论文，适合深入LLM某个方向的同学。\r\n\r\n\r\n🎨 AI绘画\r\n🧑‍🎨 艺术基础与AI绘画技巧\r\n\r\n系列讲座:每周一个关于艺术基础的有趣话题\r\n- Niji Academy [中文版]\r\n\r\nAIGCTalk-Midjourney学习手册\r\n\r\n【Midjourney】保姆级AI绘画创作系列教学课程\r\n- 莱森 \r\n\r\n\r\n🌊 Stable Diffusion原理与应用\r\n\r\n【AI绘画】Stable\r\nDiffusion 系列教程  \r\n\r\n秋葉aaaki大神喂饭级别Stable Diffusion 系列教程\r\n\r\nHow\r\nDiffusion Models Work - DeepLearning.AI  \r\n扩散模型\r\n- Diffusion Model - 李宏毅  \r\n\r\n偏宏观，比较通俗易懂\r\n\r\nDiffusion扩散模型\r\n- 唐宇迪  \r\n\r\n唐宇迪老师讲stable diffusion和dalle推理讲的比较清楚\r\n\r\nHugging\r\nFace Diffusion Models Course \r\n\r\n🔊 AI音频\r\n\r\nHugging\r\nFace Audio Course \r\nCS224S: Spoken\r\nLanguage Processing - 斯坦福大学 \r\n\r\n🌈 多模态\r\n\r\nTutorial\r\non MultiModal Machine Learning (ICML 2023) - 卡耐基梅隆大学  \r\n11-777:\r\nMultiModal Machine Learning (Fall 2022) - 卡耐基梅隆大学  \r\n11-877:\r\nAdvanced Topics in MultiModal Machine Learning (Fall 2022) -\r\n卡耐基梅隆大学 \r\n\r\n🧠 深度学习\r\n\r\nNeural\r\nNetworks/Deep Learning - StatQuest  \r\nNeural\r\nNetworks - 3Blue1Brown  \r\nNeural Networks:\r\nZero to Hero - Andrej Karpathy  \r\nPractical Deep Learning for Coders\r\n- fast.ai  \r\nDeep\r\nLearning Specialization - 吴恩达  \r\n6.S191: Introduction to\r\nDeep Learning - 麻省理工学院  \r\nCS25: Transformers\r\nUnited V2 - 斯坦福大学  \r\nDeep\r\nLearning Lecture Series 2020 - DeepMind x 伦敦大学学院  \r\nReinforcement\r\nLearning Lecture Series 2021 - DeepMind x 伦敦大学学院  \r\n\r\n💻 AI系统\r\n\r\nAI-Sys-Sp22\r\nMachine Learning Systems - 加州大学伯克利分校 \r\nDeep Learning Systems: Algorithms\r\nand Implementation - Tianqi Chen, Zico Kolter  \r\nCS 329S: Machine\r\nLearning Systems Design - 斯坦福大学 \r\n15-849: Machine\r\nLearning Systems - 卡耐基梅隆大学 \r\nComputer\r\nScience 598D - Systems and Machine Learning - 普林斯顿大学 \r\n\r\n二、中国大模型列表\r\n参考来自：https://github.com/wgwang/LLMs-In-China/tree/main\r\n大模型列表\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n序号\r\n公司\r\n大模型\r\n省市\r\n类别\r\n官网\r\n说明\r\n\r\n\r\n\r\n\r\n1\r\n百度\r\n文心一言\r\n北京\r\n通用\r\n✔\r\n有APP，衍生灵医Bot\r\n\r\n\r\n2\r\n智谱华章\r\n清言\r\n北京\r\n通用\r\n✔\r\n有APP，开源小模型ChatGLM-6B和ChatGLM2-6B\r\n\r\n\r\n3\r\n百川智能\r\n百川\r\n北京\r\n通用\r\n✔\r\n开源小模型baichuan-7B和Baichuan-13B，baichuan-2\r\n\r\n\r\n4\r\n达观数据\r\n曹植\r\n上海\r\n工业\r\n✔\r\n试用需账号\r\n\r\n\r\n5\r\n上海人工智能实验室\r\n书生\r\n上海\r\n通用\r\n✔\r\n开源小模型书生·浦语，OpenMEDLab浦医\r\n\r\n\r\n6\r\n科大讯飞\r\n星火\r\n安徽合肥\r\n通用\r\n✔\r\n试用需账号,有APP\r\n\r\n\r\n7\r\n稀宇科技\r\nABAB\r\n上海\r\n通用\r\n✔\r\nGLOW虚拟社交,MiniMax\r\n\r\n\r\n8\r\n商汤科技\r\n日日新\r\n上海\r\n通用\r\n✔\r\n\r\n\r\n\r\n9\r\n春田知韵（抖音）\r\n豆包\r\n北京\r\n通用\r\n✔\r\n开源多模态7B小模型BuboGPT，豆包是云雀的聊天机器人\r\n\r\n\r\n10\r\n中国科学院自动化研究所\r\n紫东·太初\r\n北京\r\n通用\r\n✔\r\n紫东太初2.0号称100B参数，全模态\r\n\r\n\r\n11\r\n阿里云\r\n通义千问\r\n浙江杭州\r\n通用\r\n✔\r\n试用需账号,开源小模型Qwen-7B和Qwen-7B-Chat\r\n\r\n\r\n12\r\n华为\r\n盘古,盘古气象,盘古-Σ\r\n广东深圳\r\n工业\r\n✔\r\n华为+鹏城,华为云盘古\r\n\r\n\r\n13\r\n复旦大学\r\nMOSS\r\n上海\r\n科研\r\n✔\r\n试用需账号\r\n\r\n\r\n14\r\n智源人工智能研究院\r\n悟道·天鹰,悟道·EMU\r\n北京\r\n通用\r\n✔\r\n悟道3.0,视界视觉，AQUILA天鹰座，Aquila-7B,AquilaChat-7B,AquilaCode-7B-NV,AquilaCode-7B-TS,HuggingFace,EMU基于LLaMA\r\n\r\n\r\n15\r\n浙江大学\r\n启真,TableGPT,智海-录问,智海-三乐,PromptProtein\r\n浙江杭州\r\n垂直\r\n✔\r\n医学大模型提供基于LLaMA-7B、CaMA-13B和ChatGLM-6B\r\n三个版本,用于PromptProtein的模型，法律大模型智海-录问基于Baichuan-7B，智海-三乐基于Qwen-7B\r\n\r\n\r\n16\r\nOpenBMB\r\nCPM,CPM-Bee\r\n北京\r\n通用\r\n✔\r\n面壁智能,CPM-Bee-10B\r\n\r\n\r\n17\r\n元象科技\r\nXVERSE-13B\r\n广东深圳\r\n通用\r\n✔\r\n模型下载\r\n\r\n\r\n18\r\n腾讯\r\n混元\r\n广东深圳\r\n通用\r\n✔\r\n\r\n\r\n\r\n19\r\n云知声\r\n山海\r\n北京\r\n医学\r\n✔\r\n\r\n\r\n\r\n20\r\n东北大学\r\nTechGPT,PICA\r\n辽宁沈阳\r\n科研\r\n✔\r\nTechGPT-&gt;BELLE-&gt;LLaMA，图谱构建和阅读理解问答;PICA-&gt;ChatGLM2-6B情感大模型\r\n\r\n\r\n21\r\nIDEA研究院\r\n封神榜MindBot,ziya-coding\r\n广东深圳\r\n通用\r\n✔\r\n姜子牙系列模型\r\n,ziya-coding代码大模型\r\n\r\n\r\n22\r\n贝壳\r\nBELLE\r\n北京\r\n垂直\r\n✔\r\n基于BLOOMZ或LLaMA的多个模型\r\n\r\n\r\n23\r\n360\r\n智脑,一见\r\n北京\r\n通用\r\n✔\r\n\r\n\r\n\r\n24\r\n哈尔滨工业大学\r\n本草,活字\r\n黑龙江哈尔滨\r\n医学\r\n✔\r\n医学，本草基于LLaMA；另有基于\r\nChatGLM 的Med-ChatGLM，活字基于BLOOM-7B\r\n\r\n\r\n25\r\n北京大学信息工程学院\r\nChatLaw\r\n北京\r\n法律\r\n✔\r\nChatLaw-13B基于Ziya-LLaMA-13B-v1-&gt;LLaMA,ChatLaw-33B基于Anima33B-&gt;Guanaco-&gt;LLaMA\r\n\r\n\r\n26\r\n港中文深圳\r\n华佗，凤凰\r\n广东深圳\r\n医学\r\n✔\r\n香港中文大学（深圳）和深圳市大数据研究院，医学,Demo,华佗和凤凰都基于BLOOMZ\r\n\r\n\r\n27\r\n中国科学院计算技术研究所\r\n百聆\r\n北京\r\n科研\r\n✔\r\n基于LLaMA，权重Diff下载7B和13B,demo\r\n\r\n\r\n28\r\n好未来\r\nMathGPT\r\n北京\r\n教育\r\n✔\r\n学而思\r\n\r\n\r\n29\r\n晓多科技+国家超算成都中心\r\n晓模型XPT\r\n四川成都\r\n客服\r\n✔\r\n试用申请\r\n\r\n\r\n30\r\n网易有道\r\n子曰\r\n北京\r\n教育\r\n✔\r\n推荐有道速读,读论文的利器\r\n\r\n\r\n31\r\n中国科学院成都计算机应用研究所\r\n聚宝盆\r\n四川成都\r\n金融\r\n✔\r\n基于LLaMA的金融大模型\r\n\r\n\r\n32\r\n华南理工大学\r\n扁鹊,灵心SoulChat\r\n广东广州\r\n医学\r\n✔\r\n\r\n\r\n\r\n33\r\n虎博科技\r\nTigerBot\r\n上海\r\n金融\r\n✔\r\n基于BLOOM\r\n\r\n\r\n34\r\n度小满\r\n轩辕\r\n北京\r\n金融\r\n✔\r\n基于BLOOM\r\n\r\n\r\n35\r\n北京交通大学\r\n致远\r\n北京\r\n交通\r\n✔\r\nTransGPT・致远，基于LLaMA-7B\r\n\r\n\r\n36\r\n恒生电子\r\nLightGPT\r\n浙江杭州\r\n金融\r\n✘\r\n与浙大合作的NL2SQL\r\n\r\n\r\n37\r\n上海交通大学\r\nK2,白玉兰\r\n上海\r\nK2:地球科学，白玉兰:科学\r\n✔\r\nDemo，GeoLLaMA，基于LLaMA，HuggingFace\r\n\r\n\r\n38\r\n左手医生\r\n左医GPT\r\n北京\r\n医学\r\n✔\r\n医疗，试用需Key\r\n\r\n\r\n39\r\n上海科技大学\r\nDoctorGLM\r\n上海\r\n医学\r\n✔\r\n医学大模型，论文\r\n\r\n\r\n40\r\n华东师范大学\r\nEmoGPT,EduChat\r\n上海\r\n教育\r\n✘\r\nEmoGPT是上海市心理健康与危机干预重点实验室与镜象科技公司合作完成,\r\n教学教育大模型EduChat基于BELLE（BELLE基于LLaMA）\r\n\r\n\r\n41\r\n艾写科技\r\nAnima\r\n浙江杭州\r\n营销\r\n✔\r\n基于Guanaco-&gt;基于LLaMA，使用QLoRA\r\n\r\n\r\n42\r\n澳门理工大学\r\nXrayGLM,IvyGPT\r\n澳门\r\n医疗\r\n✔\r\nIvyGPT基于ChatGLM2，XrayGLM基于VisualGLM-6B\r\n\r\n\r\n43\r\n北京语言大学\r\n桃李\r\n北京\r\n教育\r\n✔\r\n基于LLaMA,北语+清华+东北、北京交大\r\n\r\n\r\n44\r\n中工互联\r\n智工\r\n北京\r\n工业\r\n✘\r\n与复旦NLP实验室联合，工业领域\r\n\r\n\r\n45\r\n创业黑马\r\n天启\r\n北京\r\n创投\r\n✘\r\n创业黑马与360合作,科创服务行业\r\n\r\n\r\n46\r\n追一科技\r\n博文Bowen\r\n广东深圳\r\n客服\r\n✘\r\n\r\n\r\n\r\n47\r\n智慧眼\r\n砭石\r\n湖南长沙\r\n医学\r\n✘\r\n医疗领域\r\n\r\n\r\n48\r\n香港科技大学\r\n罗宾Robin\r\n香港\r\n科研\r\n✔\r\n基于LLaMA,港科大开源LMFlow\r\n\r\n\r\n49\r\n昆仑万维\r\n天工\r\n北京\r\n客服\r\n✔\r\n与奇点智源联合研发\r\n\r\n\r\n50\r\n智媒开源研究院\r\n智媒\r\n广东深圳\r\n媒体\r\n✔\r\n基于LLaMA，面向自媒体\r\n\r\n\r\n51\r\n医疗算网\r\nUni-talk\r\n上海\r\n医学\r\n✘\r\n上海联通+华山医院+上海超算中心+华为\r\n\r\n\r\n52\r\n蚂蚁集团\r\n贞仪,CodeFuse\r\n浙江杭州\r\n金融\r\n✔\r\nCodeFuse代码大模型\r\n\r\n\r\n53\r\n硅基智能\r\n炎帝\r\n江苏南京\r\n文旅\r\n✘\r\n\r\n\r\n\r\n54\r\n西湖心辰\r\n西湖\r\n浙江杭州\r\n科研\r\n✔\r\n\r\n\r\n\r\n55\r\n国家超级计算天津中心\r\n天河天元\r\n天津\r\n通用\r\n✘\r\n\r\n\r\n\r\n56\r\n星环科技\r\n无涯、求索\r\n上海\r\n金融\r\n✘\r\n无涯——金融；求索——大数据分析\r\n\r\n\r\n57\r\n清博智能\r\n先问\r\n北京\r\n农业\r\n✘\r\n基于结构化数据\r\n\r\n\r\n58\r\n智子引擎\r\n元乘象\r\n江苏南京\r\n客服\r\n✔\r\n\r\n\r\n\r\n59\r\n拓世科技\r\n拓世\r\n江西南昌\r\n金融\r\n✘\r\n\r\n\r\n\r\n60\r\n循环智能\r\n盘古\r\n北京\r\n客服\r\n✔\r\n循环智能,清华大学,华为\r\n\r\n\r\n61\r\n慧言科技+天津大学\r\n海河·谛听\r\n天津\r\n科研\r\n✘\r\n\r\n\r\n\r\n62\r\n第四范式\r\n式说\r\n北京\r\n客服\r\n✔\r\n\r\n\r\n\r\n63\r\n拓尔思\r\n拓天\r\n北京\r\n媒体\r\n✘\r\nTRSGPT\r\n\r\n\r\n64\r\n出门问问\r\n序列猴子\r\n北京\r\n营销\r\n✔\r\n\r\n\r\n\r\n65\r\n数说故事\r\nSocialGPT\r\n广东广州\r\n社交\r\n✘\r\n\r\n\r\n\r\n66\r\n云从科技\r\n从容\r\n广东广州\r\n政务\r\n✔\r\n\r\n\r\n\r\n67\r\n浪潮信息\r\n源\r\n山东济南\r\n通用\r\n✘\r\n源\r\n\r\n\r\n68\r\n中国农业银行\r\n小数ChatABC\r\n北京\r\n金融\r\n✘\r\n\r\n\r\n\r\n69\r\n麒麟合盛\r\n天燕AiLMe\r\n北京\r\n运维\r\n✔\r\n\r\n\r\n\r\n70\r\n台智云\r\n福尔摩斯FFM\r\n台湾\r\n工业\r\n✔\r\n华硕子公司\r\n\r\n\r\n71\r\n医联科技\r\nmedGPT\r\n四川成都\r\n医学\r\n✘\r\n\r\n\r\n\r\n72\r\n电信智科\r\n星河\r\n北京\r\n通信\r\n✘\r\n通用视觉，中国电信\r\n\r\n\r\n73\r\n深思考人工智能\r\nDongni\r\n北京\r\n媒体\r\n✔\r\n\r\n\r\n\r\n74\r\n文因互联\r\n文因\r\n安徽合肥\r\n金融\r\n✘\r\n金融大模型\r\n\r\n\r\n75\r\n印象笔记\r\n大象GPT\r\n北京\r\n媒体\r\n✘\r\n\r\n\r\n\r\n76\r\n中科闻歌\r\n雅意\r\n北京\r\n媒体\r\n✘\r\n\r\n\r\n\r\n77\r\n澜舟科技\r\n孟子\r\n北京\r\n金融\r\n✔\r\n\r\n\r\n\r\n78\r\n京东\r\n言犀\r\n北京\r\n商业\r\n✘\r\n\r\n\r\n\r\n79\r\n香港中文大学\r\nPointLLM\r\n香港\r\n通用\r\n✔\r\n港中文+上海AI实验室+浙大\r\n\r\n\r\n80\r\n清华大学\r\nNowcastNet\r\n北京\r\n科研\r\n✔\r\n气象,临近预报大模型\r\n\r\n\r\n81\r\n鹏城实验室\r\n鹏城·脑海\r\n广东深圳\r\n科研\r\n✘\r\nPeng Cheng Mind\r\n\r\n\r\n82\r\n宇视科技\r\n梧桐\r\n浙江杭州\r\n运维\r\n✘\r\nAIoT行业\r\n\r\n\r\n83\r\n智臻智能\r\n华藏\r\n上海\r\n客服\r\n✘\r\n小i机器人\r\n\r\n\r\n84\r\n美亚柏科\r\n天擎\r\n福建厦门\r\n安全\r\n✘\r\n公共安全\r\n\r\n\r\n85\r\n山东大学\r\n夫子•明察\r\n山东济南\r\n司法\r\n✔\r\n山东大学+浪潮云+中国政法大学，基于ChatGLM，无监督司法语料（各类判决文书、法律法规等）与有监督司法微调数据（包括法律问答、类案检索）训练而成\r\n\r\n\r\n86\r\n数慧时空\r\n长城\r\n北京\r\n地球科学\r\n✘\r\n自然资源，遥感\r\n\r\n\r\n87\r\n佳都科技\r\n佳都知行\r\n广东广州\r\n交通\r\n✘\r\n交通领域\r\n\r\n\r\n88\r\n知乎\r\n知海图\r\n北京\r\n媒体\r\n✘\r\n知乎和面壁科技合作\r\n\r\n\r\n89\r\n网易伏羲\r\n玉言\r\n广东广州\r\n通用\r\n✘\r\n\r\n\r\n\r\n90\r\n清睿智能\r\nArynGPT\r\n江苏苏州\r\n教育\r\n✘\r\n\r\n\r\n\r\n91\r\n微盟\r\nWAI\r\n上海\r\n商业\r\n✔\r\n\r\n\r\n\r\n92\r\n西北工业大学+华为\r\n秦岭·翱翔\r\n陕西西安\r\n工业\r\n✘\r\n流体力学大模型,湍流+流场\r\n\r\n\r\n93\r\n奇点智源\r\n天工智力\r\n北京\r\n通用\r\n✔\r\n瑶光和天枢\r\n\r\n\r\n94\r\n联汇科技\r\n欧姆\r\n浙江杭州\r\n通用\r\n✔\r\nOmModel欧姆多模态（视觉语言）大模型\r\n\r\n\r\n95\r\n中国联通\r\n鸿湖\r\n北京\r\n通信\r\n✘\r\n\r\n\r\n\r\n96\r\n思必驰\r\nDFM-2\r\n江苏苏州\r\n工业\r\n✘\r\n\r\n\r\n\r\n97\r\n理想科技\r\n大道Dao\r\n北京\r\n运维\r\n✘\r\n运维大模型\r\n\r\n\r\n98\r\n电科太极\r\n小可\r\n北京\r\n政务\r\n✘\r\n党政企行业应用\r\n\r\n\r\n99\r\n中国移动\r\n九天\r\n北京\r\n通信\r\n✘\r\n\r\n\r\n\r\n100\r\n中国电信\r\nTeleChat\r\n北京\r\n通信\r\n✘\r\n\r\n\r\n\r\n101\r\n容联云\r\n赤兔\r\n北京\r\n客服\r\n✘\r\n客服，营销\r\n\r\n\r\n102\r\n云天励飞\r\n天书\r\n广东深圳\r\n政务\r\n✘\r\n\r\n\r\n\r\n103\r\n乐言科技\r\n乐言\r\n上海\r\n客服\r\n✘\r\n\r\n\r\n\r\n104\r\n沪渝人工智能研究院\r\n兆言\r\n重庆\r\n科研\r\n✘\r\n也称：上海交通大学重庆人工智能研究院\r\n\r\n\r\n105\r\n中央广播电视总台\r\n央视听\r\n北京\r\n媒体\r\n✘\r\n央视听媒体大模型CMG Media GPT\r\n\r\n\r\n106\r\n超对称技术公司\r\n乾元\r\n北京\r\n金融\r\n✔\r\n\r\n\r\n\r\n107\r\n蜜度\r\n文修\r\n上海\r\n媒体\r\n✘\r\n智能校对\r\n\r\n\r\n108\r\n中国电子云\r\n星智\r\n湖北武汉\r\n政务\r\n✘\r\n政务大模型\r\n\r\n\r\n109\r\n理想汽车\r\nMindGPT\r\n北京\r\n工业\r\n✘\r\n\r\n\r\n\r\n110\r\n阅文集团\r\n妙笔\r\n上海\r\n文旅\r\n✘\r\n网文大模型\r\n\r\n\r\n111\r\n携程\r\n问道\r\n上海\r\n文旅\r\n✘\r\n旅游行业大模型\r\n\r\n\r\n112\r\n实在智能\r\n塔斯\r\n浙江杭州\r\n客服\r\n✘\r\nTARS\r\n\r\n\r\n113\r\n瑞泊\r\nVIDYA\r\n北京\r\n工业\r\n✔\r\n\r\n\r\n\r\n114\r\n有连云\r\n麒麟\r\n上海\r\n金融\r\n✘\r\n\r\n\r\n\r\n115\r\n维智科技\r\nCityGPT\r\n上海\r\n公共服务\r\n✘\r\n城市大模型\r\n\r\n\r\n116\r\n用友\r\nYonGPT\r\n北京\r\n企业服务\r\n✘\r\n\r\n\r\n\r\n117\r\n天云数据\r\nElpis\r\n北京\r\n金融\r\n✘\r\n证券法律法规\r\n\r\n\r\n118\r\n孩子王\r\nKidsGPT\r\n江苏南京\r\n教育\r\n✘\r\n\r\n\r\n\r\n119\r\n企查查\r\n知彼阿尔法\r\n江苏苏州\r\n商业\r\n✘\r\n\r\n\r\n\r\n120\r\n今立方\r\n12333\r\n福建厦门\r\n政务\r\n✘\r\n人社领域\r\n\r\n\r\n121\r\n阳光保险集团\r\n正言\r\n广东深圳\r\n金融\r\n✘\r\n\r\n\r\n\r\n122\r\n中科创达\r\n魔方Rubik\r\n北京\r\n工业\r\n✘\r\n\r\n\r\n\r\n123\r\n聆心智能\r\nCharacterGLM\r\n北京\r\n游戏\r\n✘\r\n\r\n\r\n\r\n124\r\n大经中医\r\n岐黄问道\r\n江苏南京\r\n医疗\r\n✘\r\n\r\n\r\n\r\n125\r\n蒙牛\r\nMENGNIU.GPT\r\n内蒙古呼和浩特\r\n食品\r\n✘\r\n\r\n\r\n\r\n126\r\n快商通\r\n汉朝\r\n福建厦门\r\n营销\r\n✘\r\n\r\n\r\n\r\n127\r\n众合科技\r\nUniChat\r\n浙江杭州\r\n交通\r\n✘\r\n\r\n\r\n\r\n128\r\n金蝶\r\n苍穹\r\n广东深圳\r\n企业服务\r\n✘\r\n\r\n\r\n\r\n129\r\n云问科技\r\n云中问道\r\n江苏南京\r\n营销\r\n✘\r\n与西安未来AI计算中心联合发布\r\n\r\n\r\n130\r\n天壤智能\r\n小白\r\n上海\r\n通用\r\n✘\r\n\r\n\r\n\r\n131\r\n小米\r\nMiLM-6B\r\n北京\r\n商业\r\n✘\r\n\r\n\r\n\r\n132\r\n长虹\r\n长虹超脑\r\n四川绵阳\r\n媒体\r\n✘\r\n\r\n\r\n\r\n133\r\n开普云\r\n开悟\r\n广东东莞\r\n政务\r\n✔\r\n\r\n\r\n\r\n134\r\n赛灵力科技\r\n达尔文\r\n广东广州\r\n医学\r\n✘\r\n赛灵力,清华珠三角研究院,赛业生物,大湾区科技创新服务中心\r\n\r\n\r\n135\r\n航旅纵横\r\n千穰大模型\r\n北京\r\n民航\r\n✘\r\n航旅纵横APP上需要PLUS会员才能使用\r\n\r\n\r\n136\r\n奇安信\r\nQ-GPT\r\n北京\r\n信息安全\r\n✘\r\n\r\n\r\n\r\n137\r\n车之谷\r\n叆谷\r\n山东青岛\r\n汽车\r\n✘\r\n汽车后服务加油站场景\r\n\r\n\r\n138\r\n索贝时代\r\n明眸\r\n四川成都\r\n媒体\r\n✘\r\n\r\n\r\n\r\n139\r\n海尔\r\nHomeGPT\r\n山东青岛\r\n智能家居\r\n✘\r\n\r\n\r\n\r\n140\r\n马上消费\r\n天镜\r\n重庆\r\n金融\r\n✘\r\n零售金融\r\n\r\n\r\n141\r\n白海科技\r\n白聚易\r\n北京\r\n营销\r\n✘\r\n营销传播专家多模态预训练模型IMC-GPT（白聚易）\r\n\r\n\r\n142\r\n二元工业\r\n妆舟\r\n江苏苏州\r\n日化\r\n✘\r\n回答化妆、护肤和服饰搭配等问题，日化行业从业人员提供从产品开发、行业服务到品牌建设等指导\r\n\r\n\r\n143\r\n格创东智\r\n章鱼智脑\r\n广东广州\r\n工业制造\r\n✘\r\n工业智能大模型引擎底座——章鱼智脑OctopusGPT\r\n\r\n\r\n144\r\n创业邦\r\nBangChat\r\n北京\r\n创投\r\n✘\r\n产业、企业和投资行业\r\n\r\n\r\n145\r\n新华三H3C\r\n百业灵犀\r\n浙江杭州\r\n工业\r\n✘\r\n\r\n\r\n\r\n146\r\n作业帮\r\n银河\r\n广东广州\r\n教育\r\n✘\r\n\r\n\r\n\r\n147\r\n电科数字\r\n智弈\r\n上海\r\n水利\r\n✘\r\n\r\n\r\n\r\n148\r\n绿盟\r\n风云卫\r\n北京\r\n网络安全\r\n✘\r\nNSFGPT\r\n\r\n\r\n149\r\n江苏欧软\r\nWISE\r\n江苏苏州\r\n工业\r\n✘\r\nWISE工业大模型\r\n\r\n\r\n150\r\n创新奇智\r\n奇智孔明\r\n山东青岛\r\n工业\r\n✘\r\n工业大模型AInno-15B，ChatRobot，ChatBI，ChatDoc\r\n\r\n\r\n151\r\n大汉软件\r\n星汉\r\n江苏南京\r\n政务\r\n✘\r\n“星汉”Galaxy大模型\r\n\r\n\r\n152\r\n零点有数\r\n零点楷模\r\n北京\r\n政务\r\n✘\r\n\r\n\r\n\r\n153\r\n国农生猪大数据中心\r\nPIGGPT\r\n重庆\r\n农业\r\n✘\r\n\r\n\r\n\r\n154\r\n微脉\r\nCareGPT\r\n浙江杭州\r\n医疗\r\n✘\r\n\r\n\r\n\r\n155\r\n吉大正元\r\n昆仑\r\n吉林长春\r\n信息安全\r\n✘\r\n\r\n\r\n\r\n156\r\n武汉大学\r\nCheeseChat\r\n湖北武汉\r\n教育\r\n✘\r\n内测招募，仅限武汉大学在校师生申请\r\n\r\n\r\n157\r\n方正电子\r\n魔方\r\n北京\r\n媒体\r\n✘\r\n聚焦媒体市场需求\r\n\r\n\r\n158\r\n似然实验室\r\nTraderGPT\r\n广东广州\r\n金融\r\n✘\r\n金融持仓分析大模型\r\n\r\n\r\n159\r\n网易智企\r\n商河\r\n广东广州\r\n客服\r\n✘\r\n客服领域行业大模型\r\n\r\n\r\n160\r\n深圳供电局\r\n祝融2.0\r\n广东深圳\r\n电力\r\n✘\r\n电力行业首个多模态预训练大模型\r\n\r\n\r\n161\r\n万兴科技\r\n天幕\r\n西藏拉萨\r\n媒体\r\n✘\r\n以视频创意应用为核心\r\n\r\n\r\n162\r\n惟远智能\r\n千机百智\r\n广东深圳\r\n客服\r\n✘\r\n\r\n\r\n\r\n163\r\n兔展智能\r\n兔灵\r\n广东深圳\r\n营销\r\n✘\r\n\r\n\r\n\r\n164\r\n中国科学技术大学\r\nUniDoc\r\n安徽合肥\r\n通用\r\n✘\r\n中科大&amp;字节,统一的文字-图像理解大模型\r\n\r\n\r\n165\r\n钢谷网\r\n谷蚁\r\n陕西西安\r\n电商\r\n✘\r\n钢铁行业电商\r\n\r\n\r\n166\r\n浪潮海岳\r\ninGPT\r\n山东济南\r\n企业服务\r\n✘\r\n\r\n\r\n\r\n167\r\n木卫四科技\r\n蝴蝶\r\n北京\r\n汽车\r\n✘\r\n\r\n\r\n\r\n168\r\n汇通达网络\r\n汇通达\r\n江苏南京\r\n企业服务\r\n✘\r\n下沉市场零售行业企业客户的交易和服务的互联网平台,农村电商服务\r\n\r\n\r\n\r\n国外大模型\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n公司\r\n大模型\r\n说明\r\n\r\n\r\n\r\n\r\nOpenAI\r\nChatGPT\r\nChatGPT-4支持Plugins，Code\r\nInterpreter\r\n\r\n\r\n微软\r\nBing\r\nChat\r\n搜索增强，有三种模式\r\n\r\n\r\nGoogle\r\nPaLM2,Bard,Gemini\r\nBard支持图片内容识别，包括OCR等\r\n\r\n\r\nAnthropic\r\nClaude\r\nClaude\r\n2,支持读入pdf、txt、csv等文件进行分析、总结和问答等\r\n\r\n\r\nMeta\r\nLLaMA,LLaMA-2, CodeLLaMA\r\n最强开源开放大模型，月活用户小于7亿的组织和个人可随意商用\r\n\r\n\r\nStability AI\r\nStableLM\r\n\r\n\r\n\r\nAmazon\r\nTitan\r\n\r\n\r\n\r\nBloomberg\r\nBloombergGPT\r\n\r\n\r\n\r\nMosaicML\r\nMPT\r\n\r\n\r\n\r\nIntel\r\nAurora genAI\r\n\r\n\r\n\r\nUC Berkeley, Microsoft Research\r\nGorilla\r\n\r\n\r\n\r\ninflection.ai\r\nInflection-1\r\n\r\n\r\n\r\nxAI\r\n\r\n从OpenAI\r\n到xAI\r\n\r\n\r\ncohere\r\nCohere\r\n\r\n\r\n\r\nScale AI\r\nScale\r\n\r\n\r\n\r\ncharacter ai\r\nCharacter\r\n\r\n\r\n\r\nColossal-AI\r\nColossalChat\r\n\r\n\r\n\r\n\r\n三、AIGC视频会议&amp;访谈\r\n智源社区\r\n【论文分享】【AugGPT：利用ChatGPT进行文本数据增强\r\n】[link]\r\n【论文分享】【ChatGPT的鲁棒性探究——对抗性和分布外泛化的视角\r\n】[link]\r\n【论文分享】【传统检索模型和大语言模型在信息搜索中的应用和对比\r\n】[link]，[paper]，[code]，[blog]\r\n访谈&amp;视频\r\n【访谈】【OpenAI 的核心研发人员 Jack Rae 在参加\r\nStanford MLSys Seminar 的访谈时进行了一个名为 Compression for\r\nAGI的主题分享 】[访谈记录]\r\n【访谈】【万字长文：想训大模型？这里有一份避坑指南】[访谈记录]\r\n【访谈】【微软Bing版ChatGPT表明想做人类，并且对纽约时报专栏作家表达爱意】[访谈记录]\r\n【访谈】【Midjourney创始人David\r\nHolz关于生成式AI的访谈】[访谈记录]\r\n【访谈】【OpenAI创始人：GPT-4的研究起源和构建心法】[访谈记录]\r\n【访谈】【ABC News\r\n专访OpenAI首席执行官萨姆·奥尔特曼：AI风险和重塑社会的问题】[访谈记录]\r\n【访谈】【OpenAI联合创始人Ilya\r\nSutskever等专访：开源人工智能是不明智的】[访谈记录]\r\n【访谈】【OpenAI董事长、CTO Greg Brockman专访\r\n：GPT-4 并不完美，不过人无完人】[访谈记录]\r\n【访谈】【图灵奖获得者 Yoshua Bengio 认为 ChatGPT\r\n是一个“警钟”】[访谈记录]\r\n【访谈】【《麻省理工科技评论》对 ChatGPT\r\n幕后团队，进行了一次深入的独家专访】[访谈记录]\r\n【访谈】【口述历史，探析ChatGPT的创造历程，ChatGPT内部故事】[访谈记录]\r\n【访谈】【对话ChatGPT之父！AI会改变什么？不会改变什么？】[访谈记录]\r\n【访谈】【对话OpenAI研究科学家：他们是如何让GPT4更像人的？】[访谈记录]\r\n【视频】【邱锡鹏教授介绍以ChatGPT为核心的大规模语言模型的相关知识及未来的发展方向\r\n】[B站]\r\nLLM体验效果&amp;专业评估\r\n【LLM效果对比】【360智脑_VS_讯飞星火】[blog]\r\n【LLM效果对比】【阿里通义千问_VS_讯飞星火】[blog]\r\n【LLM效果对比】【Bard_VS_Baize-7B_VS_文心一言】[blog]\r\n【LLM效果对比】【Bard_VS_Bing_VS_ChatGPT】[blog]\r\n【LLM效果对比】【Bard_VS_文心一言】[blog]\r\n【LLM效果对比】【ChatGPT_VS_GPT4】[blog]\r\n【LLM效果对比】【OpenAssistant_VS_百度文心一言】[blog]\r\n【LLM效果对比】【文心一言新闻发布会内容复现】[blog]\r\n【LLM效果对比】【文心一言_VS_ChatGLM-6B】[blog]\r\n【LLM效果对比】【文心一言 VS GPT-4：20道问答PK】[blog]\r\n【LLM效果对比】【文心一言 vs GPT-4实测！】[blog]\r\n【LLM效果对比】【讯飞星火_VS_文心一言】[blog]\r\n【ChatGPT专业评估】【一文看遍各行业对ChatGPT的专业评估】[blog]\r\n【ChatGPT专业评估】【ChatGPT关于推理、幻觉和交互的多任务、多语言、多通道评估\r\n】[paper]\r\n【ChatGPT专业评估】【如何评价 OpenAI 的超级对话模型\r\nChatGPT ？】[paper]\r\n【ChatGPT专业评估】【用ChatGPT参加计算机科学考试】[paper]\r\n【LLM知识评估】【C-Eval：构造中文大模型的知识评估基准】[主页]，[paper]，[code]，[blog]\r\n【MLLM幻觉评估】【多模态大模型的幻觉问题与评估】[blog]，[paper]，[code]\r\n【各大大模型评测】【粗看大模型ChatGLM、MOSS、Bloomz在中文垂域评测中的性能表现：医学、法律、心理学、教育等四大类试题下的测试报告介绍】[paper]，[code]，[blog]\r\n【国内大模型评测】【评测国内各种对标 ChatGPT\r\n的大语言模型】[blog]，[code]\r\n【大模型排行榜】【OpenLLM大模型排行榜】[主页]，[blog]，[最新进展blog]\r\n【大模型排行榜】【斯坦福发布LLM排行榜AlpacaEval，微软WizardLM登顶开源模型第一】[blog]，[主页]，[code]\r\nLLM垂直领域大模型\r\n法律\r\n【再看基于LLaMA的最新微调模型变体：CaMA、ExpertLLaMA以及第四个中文法律微调模型LexiLaw】[blog]\r\n【基于中文法律知识的大语言模型——LaWGPT】[blog]\r\n医疗\r\n【AD-AutoGPT：用于阿尔茨海默病信息流行病学的自主GPT】[paper]\r\n【MedQA-ChatGLM - 基于真实医疗对话数据在ChatGLM上进行微调】[code]，[主页]\r\n【谷歌医疗大模型登Nature，Med-PaLM重磅揭秘！AI医生成绩比肩人类】[blog]，[paper]\r\n【PULSE：中文医疗大语言模型】[code]\r\n金融\r\n【FinGPT：一个「专用于金融领域」的开源大语言模型（LLM）框架，源码公开！】[blog]，[paper]，[code]\r\n环境\r\n【清华&amp;中国气象局大模型登Nature：预报时效首次达3小时】[blog]，[paper]\r\n网络安全\r\n【专用于网络攻击的模型FraudGPT】[blog]\r\n交通\r\n【北交大开源交通大模型TransGPT·致远，可免费商用】[blog]，[code]\r\n其他\r\n【南洋理工开源海外中文大语言模型Panda LLM |\r\n探索数据因素和训练策略如何影响大模型性能表现】[paper]，[code]，[blog]\r\nLLM文本检测\r\n【论文&amp;代码】【美国麻省大学&amp;谷歌研究院：改写文本可以避开AI生成文本的检测器，但检索则是一种有效的防御】[paper]，[code]\r\n【论文】【人工智能生成的文本能被可靠地检测出来吗？】[paper]，[blog]\r\n【论文】【DetectGPT（斯坦福大学）：利用概率曲率检测文本是否大模型生成】[paper]，[blog]，[code&amp;data]\r\n【论文】【Detecting LLM-Generated-Text综述】[paper]，[blog]\r\n【论文】【一个专为教育者打造的全新\r\nAI 检测模型】[blog]\r\n【论文】【OpenAI重磅发布官方「ChatGPT检测器」】[blog]\r\n【论文】【斯坦福最新研究：不要过度依赖GPT生成内容，其检测器可能存在不利于非母语英语写作者的偏见】[paper]\r\nLLM长文本解决方案\r\n【苏剑林】【Transformer升级之路：一种全局长度外推的新思路】[blog]\r\n【博客】【ChatGPT能写长篇小说了，ETH提出RecurrentGPT实现交互式超长文本生成】[paper]，[code]，[blog]，[demo1]，[demo2]\r\n【博客】【语言大模型100K上下文窗口的秘诀】[blog]\r\n【博客】【RoPE可能是LLM时代的Resnet】[blog]\r\nLLM可控性与安全\r\n【可控性】【微软提出Control-GPT：用GPT-4实现可控文本到图像生成！】[paper]，[blog]\r\n【可控性】【AIGC如何安全可控?中山大学等最新《AIGC中对隐私和安全的挑战及其补救措施：探索隐私计算、区块链潜在应用》全面阐述】[paper]，[blog]\r\n【可控性】【ControlVideo:\r\n可控的Training-free的文本生成视频】[blog]，[paper]，[code]\r\n【安全】【大模型切脑后变身PoisonGPT，虚假信息案例】[blog]，[code]\r\n【安全】【ChatGPT羊驼家族全沦陷！CMU博士击破LLM护栏，人类毁灭计划脱口而出】[blog]，[paper]，[code]\r\nLLM训练、微调、优化以及部署\r\n【LLM学习网站】【训练、微调、优化和部署大模型最新技术LLM\r\nLearning Lab】[官网]\r\nLLM训练\r\n【LLM训练】【DeepSpeed的Tutorials】[主页]，[DeepSpeed\r\nGetting Starte]\r\n【LLM训练】【如何使用 Megatron-LM 训练语言模型】[blog]\r\n【LLM训练】【Muti Query Attention 和 Attention with\r\nLinear Bias（附源码）】[blog]，[paper]\r\nLLM微调\r\n【LLM微调】【PEFT:\r\n在低资源硬件上对十亿规模模型进行参数高效微调 】[blog]\r\n【LLM微调】【大语言模型（LLM）微调技术笔记】[code]\r\n【LLM微调】【大模型LLM-微调经验分享&amp;总结】[code]，[blog]\r\n【LLM微调】【LoRA：卷完图像生成领域，卷文本生成领域的东西，到时是个啥？】[blog]，[code]\r\n【LLM微调】【Washington大学2023年5月新提出一种高效的微调方法QLoRA，通过降低显存使用，实现在单个48GB\r\nGPU上对65B参数的大模型进行微调，只需微调12个小时就可以达到97%的ChatGPT水平。同时只用int4就可以保持fp16精度的效果。】[paper]\r\n【LLM微调】【华盛顿大学提出全新量化和微调方法，在DB-GPT上享受33B参数的LLM】[blog]\r\n【LLM微调】【陈丹琦团队提出低内存高效零阶优化器MeZO，单卡A100可训练300亿参数模型】[paper]，[code]，[blog]\r\nLLM优化\r\n【LLM优化】【LLM，压缩即泛化，泛化即智能】[blog]\r\n【LLM优化】【LLM-Pruner: 剪枝+少量数据+少量训练 =\r\n高效的LLM压缩】[blog]\r\n【LLM优化】【邱锡鹏团队提出新优化器LOMO｜650亿参数，8块GPU全参数微调】[blog]，[paper]\r\n【LLM优化】【伯克利开源LLM推理与服务库：GPU减半、吞吐数十倍猛增】[中文blog]，[英文blog]\r\n【LLM优化】【LLM\r\nAccelerator：使用参考文本无损加速大语言模型推理】[blog]，[paper]，[code]\r\n【LLM优化】【大模型推理性能优化之KV Cache解读】[blog]\r\n【LLM优化】【CAME：大模型训练成本降低近一半】[blog]\r\nLLM部署\r\n【LLM部署】【工程实践！以LLAMA为例的大模型部署方案】[blog]\r\n【LLM部署】【大模型部署框架FastLLM解析，支持X86/Arm/CUDA\r\n3种架构的硬件！】[blog]，[code]\r\nLLM博客、论文以及代码\r\n【综述】【中文大语言模型汇总：医疗、法律、金融、教育、数学微调，\r\n目前已1.1K星】[code]\r\n【综述】【大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写】[paper]\r\n【综述】【大语言模型综述全新出炉：51页论文带你盘点LLM领域专业化技术】[paper]，[blog]\r\n【综述】【AIGC综述:\r\n从GAN到ChatGPT的生成式人工智能简史】[paper]\r\n【综述】【大模型综述来了！一文带你理清全球AI巨头的大模型进化史】[paper]，[code]\r\n【复旦大学】【复旦大学教授肖仰华：ChatGPT\r\n浪潮下，面向大模型如何做数据治理？】[blog]\r\n【谷歌】【面向决策的基础模型: 问题、方法与机会】[paper]\r\n【谷歌】【较大语言模型上下文学习的方式有所不同】[paper]\r\n【谷歌】【通用语音识别大模型已经支持100+语言】[blog]\r\n【谷歌】【发布5620亿参数多模态模型PaLM-E，机器人操控再上台阶】[paper]，[blog]，[twitter]，[video]\r\n【Huawei】【PanGu-Σ:\r\n稀疏异构计算万亿参数语言模型研究参数语言模型】[paper]\r\n【剑桥大学】【奖励聊天机器人在现实世界中与数以百万计的用户进行互动】[paper]\r\n【LeCun】【人工智能系统最终是否需要以现实为基础，而不仅仅是从语言中学习？】[blog]\r\n【LeCun】【大型语言模型是否需要感官基础来理解意义和理解？】[slices]\r\n【LeCun】【ChatGPT是「外星人」，所以才会胡说八道】[paper]，[blog]\r\n【LeCun】【AI聊天机器人并不关注用户的社交属性】[blog]\r\n【LeCun】【LeCun和马库斯齐喷ChatGPT：大语言模型果然是邪路？】[blog]\r\n【LeCun】【ChatGPT无法实现通用人工智能，但ALM技术路线也许可以】[blog]\r\n【LeCun】【「增强语言模型」的综述 】[paper]\r\n【LeCun】【自回归LLM的缺陷之一，大语言模型必须知道的8个要点】[paper]\r\n【MIT】【从词模型到世界模型：从自然语言到思维概率语言的转变】[paper]\r\n【李开复】【AI进入2.0时代，所有应用都会被重写一遍\r\n】[blog]\r\n【纽约大学】【提出ILF（从语言反馈中模仿学习）：利用语言反馈大规模训练语言模型】[paper]\r\n【OpenAI】【GPT就是GPT：大模型对劳动力市场影响潜力的早期研究】[paper]\r\n【OpenAI】【ABC News\r\n专访OpenAI首席执行官萨姆·奥尔特曼：AI风险和重塑社会的问题】[blog]\r\n【OpenAI】【最新发布通用人工智能路线图！AGI比想象中来得更快！】[blog]\r\n【OpenAI】【Sam\r\nAltman 担心“潜在的可怕的”人工智能工具以及“未来的人们如何看待我们” 】[blog]\r\n【OpenAI】【The Age of\r\nAI：拾象大模型及OpenAI投资思考】[blog]\r\n【OpenAI】【为什么ChatGPT用强化学习而非监督学习？】[blog]\r\n【OpenNLPLab】【为什么ChatGPT用强化学习而非监督学习？】[blog]，[paper]，[codel]\r\n【PWC】【ChatGPT和生成式AI的11大安全趋势】[blog]\r\n【人大】【人大最新大语言模型综述，51页全面回顾大语言模型】[paper]\r\n【清华大学】【张学工教授：AI技术前沿——从ChatGPT到更多突破】[blog]\r\n【斯坦福】【研究大语言模型反映了谁的观点？】[paper]，[code]\r\n【斯坦福】【大模型及其公平使用】[paper]\r\n【斯坦福】【构建大模型生态系统图，用于跟踪大模型的足迹】[blog]\r\n【斯坦福】【斯坦福报告：基础模型的机遇与风险】[blog]\r\n【微软】【一种新的大语言模型NLG评估框架】[paper]\r\n【微软】【低代码LLM: LLM的可视化编程】[paper]\r\n【微软】【微软提出LLMA:大型语言模型的无损加速,可以无损地加速带有引用的大型语言模型\r\n(LLM) 推理】[paper]\r\n【微软 &amp;\r\nMeta】【ART：大型语言模型的自动多步骤推理和工具使用】[paper]\r\n【EleutherAI&amp;耶鲁大学】【提出Pythia：\r\n跨越训练和扩展的大型语言模型分析套件】[paper]，[code]\r\n【博客】【ChatGPT的底层逻辑】[blog]\r\n【博客】【智慧信息的压缩：模型智能的涌现之道】[blog]\r\n【博客】【拨动大模型的琴弦｜Delta Tuning 成果登上\r\nNature子刊封面！】[blog]\r\n【博客】【大型人工智能模型中出现的不可预测的能力】[blog\r\n)]\r\n【博客】【为什么现在的大语言模型（LLM）都是Decoder-only的架构？】[blog]\r\n【博客】【大型语言模型的涌现能力】[blog]\r\n【博客】【大型语言模型成本分析】[blog]\r\n【博客】【超越ChatGPT：大模型的智能极限 】[blog]\r\n【博客】【Nature：AI模型越大越好吗? 】[blog]\r\n【博客】【一场关于ChatGPT话语权的深度思考：人类会在大模型中迷失自我吗？】[blog]，[blog译文]\r\n【博客】【马斯克强调的TruthGPT 是什么】[blog]\r\n【博客】【对话式AI搜索的技术路线猜想】[blog]\r\n【博客】【AI走过多少路，才迎来了ChatGPT？】[blog]\r\n【博客】【如何负责任地创建、发布和共享生成式 AI】[blog]\r\n【博客】【大模型时代的“Linux”生态，开启人工智能新十年】[blog]\r\n【博客】【揭秘ChatGPT背后的AI“梦之队”：90后科研“后浪”展示强大创新能力｜智谱研究报告】[blog]\r\n【博客】【In-Context Learning玩法大全 】[blog]\r\n【博客】【一文理解“上下文学习”----大语言模型突现能力】[blog]\r\n【博客】【回应吴军老师 |\r\n\"ChatGPT不算新技术革命\"】[blog]\r\n【博客】【Poe向所有开发者推出Poe\r\nAPI，以便广泛获取基于LLM的服务】[code]\r\n【博客】【【LLM系列之底座模型对比】LLaMA、Palm、GLM、BLOOM、GPT模型结构对比】[blog]\r\n【博客】【大模型实践总结】[blog]\r\n【博客】【【LLM系列之GPT】GPT（Generative\r\nPre-trained Transformer）生成式预训练模型】[blog]\r\n【博客】【【LLM系列之Tokenizer】如何科学地训练一个LLM分词器】[blog]\r\n【博客】【大模型词表扩充必备工具SentencePiece】[blog]\r\n【博客】【大模型知识&amp;推理评估基准】[blog]\r\n【博客】【万字长文说清大模型在自动驾驶领域的应用】[blog]\r\n【博客】【一文速览大语言模型在推荐系统中的应用】[blog]\r\n【博客】【NAACL &amp;\r\nACL：大模型的两种知识继承方案】[方案一]，[方案二]\r\n【博客】【a16Z：大模型应用程序的新兴架构】[中文blog]，[英文blog]\r\n【论文】【RetNet：MSRA提出Transformer全新替代大模型基础架构，推理速度8倍提升，内存占用减少70%】[blog]，[paper]\r\n【论文】【大模型微调指南：当GPU资源不足时的有效解决方案】[paper]\r\n【论文】【TaskMatrix.AI: Completing Tasks by\r\nConnecting Foundation Models with Millions of APIs 】[paper]\r\n【论文】【AnnoLLM: Making Large Language\r\nModels to Be Better Crowdsourced Annotators 】[paper]\r\n【论文】【南加州大学:大语言模型统计偏好的挑战和危险】[paper]\r\n【论文】【卡内基·梅隆大学 |\r\n语言生成模型可能造成危害：那么我们能做些什么呢？】[paper]\r\n【论文】【鹏程实验室等最新《大规模多模态预训练模型》全面综述】[paper]\r\n【论文】【预训练基础模型综合调研：从 BERT 到 ChatGPT\r\n的历史 】[paper]\r\n【论文】【洛桑联邦理工学院提出REFINER框架，用于微调大规模语言模型】[paper]\r\n【论文】【LLM-Adapters：\r\n用于大型语言模型的参数高效微调的适配器系列】[paper]\r\n【论文】【大型语言模型的涌现记忆和可预测记忆】[paper]\r\n【论文】【机器心理学：使用心理学方法研究大型语言模型中的涌现能力和行为】[paper]\r\n【论文】【Chameleon：使用大型语言模型进行即插即用的组合推理】[paper]\r\n【代码】【大型语言模型相关文献资源列表】[code]\r\nLLM数据集\r\n【COIG-PC】【智源研究院发布国内首个大规模、可商用中文开源指令数据集COIG：最大规模中文多任务指令集，上新千个中文数据集】[blog]，[paper]，[COIG-PC数据下载地址]，[COIG数据下载地址]\r\n【Instruct/Prompt\r\nTuning可用数据】【总结当前开源可用的Instruct/Prompt\r\nTuning数据】[blog]\r\n【MiniGPT-4】【GPT-4平替版：MiniGPT-4，支持图像理解和对话，现已开源】[dataset]\r\n【Multimodal\r\nC4】【多模态C4：一个开放的、10亿规模的、与文本交错的图像语料库】[paper]，[code]\r\n【Mind2Web】【Mind2Web:\r\n首个全面衡量大模型上网能力的数据集】[blog]\r\n【OpenAssistant\r\nConversations】【该数据集是一个由人工生成、人工注释的助理式对话语料库，覆盖了广泛的主题和写作风格，由\r\n161443 条消息组成，分布在 66497 个会话树中，使用 35\r\n种不同的语言。该语料库是全球众包工作的产物，涉及超过 13500\r\n名志愿者。为了证明 OpenAssistant Conversations\r\n数据集的有效性，该研究还提出了一个基于聊天的助手\r\nOpenAssistant，其可以理解任务、与第三方系统交互、动态检索信息。】[dataset]，[paper]，[code]\r\n【Panda LLM】【为了让Panda\r\nLLM在中文数据集上获得强大的性能，作者使用了强大的指令微调instruction-tuning技术，将LLaMA基础模型在五个开源的中文数据集进行混合训练，其中包括来自各种语言领域的1530万个样本，例如维基百科语料，新闻语料，百科问答语料，社区问答语料，和翻译语料。】[blog]\r\n【RedPajama】【RedPajama开源项目｜复制超过1.2万亿个令牌的LLaMA训练数据集】[原始blog]，[中文blog]，[dataset]，[code]\r\nPrompt工程\r\n【博客】【OpenAI 应用人工智能研究负责人Lilian\r\nWeng新博文：关于提示工程的介绍】[blog]\r\n【博客】【Prompt Engineering全面自动化】[blog]\r\n【博客】【ChatGPT提示示例集合】[地址]，[code]，huggingface]\r\n【博客】【深入浅出Prompt Learning要旨及常用方法】[blog]\r\n【博客】【ChatGPT火爆，最全prompt工程指南登GitHub热榜，标星4.7k！】[code]，youtube]\r\n【博客】【ChatGPT Prompt工程：设计、实践与思考】[blog]\r\n【博客】【全面的提示工程指南】[blog]\r\n【博客】【指令学习综述｜ChatGPT背后的指令学习是什么】[blog]，[paper]\r\n【博客】【免费教你提示工程，全中文教学】[主页]，[code]\r\n【博客】【吴恩达Prompt课程笔记】[主页]\r\n【博客】【ChatGPT使用进阶，Prompt工程】[blog]\r\n【论文】【面向大型语言模型的提升提示集成】[paper]\r\n【论文】【DTG：一种简单有效的Prompt方法，激发大模型思考判断能力！】[blog]\r\nAGI开源工具&amp;博客&amp;论文\r\n【工具】【Google发布统计深度学习框架平台：OpenXLA】[blog]\r\n【博客】【AGI的火花一作Sébastien\r\nBubeck演讲万字全文】[blog]\r\n【博客】【AGI通用智能发展的思考：是否存在足够通用的处理器？】[blog]\r\n【论文】【OpenAGI:当大语言模型遇到领域专家】[paper]，[code]\r\n文本生成\r\nChatGPT\r\n从GPT3到ChatGPT模型的发展路线图\r\n\r\n\r\nChatGPT_family\r\n\r\nChatGPT 应用篇\r\n【58】【从 GPT 到 ChatGPT 的演进与应用思考】[blog]\r\n【MIT &amp; 哈佛大学 】【语言模型可以预测公众舆论\r\n】[paper]\r\n【中科院】【ChatGPT助力芯片，传统\r\nEDA如何演变成智能EDA】[blog]\r\n【微软】【《ChatGPT机器人:设计原则和模型能力》论文\r\n】[paper]\r\n【微软】【各种环境下的ChatGPT赋能长步机器人控制：\r\n一个案例的应用 】[paper]，[code]\r\n【博客】【ChatGPT获得了「Wolfram」超能力】[blog]\r\n【博客】【OpenAI开发Plugin将 ChatGPT\r\n连接到互联网】[blog]\r\n【博客】【ChatAug：利用ChatGPT进行文本数据增强】[paper]\r\n【博客】【ChatGPT 是数据隐私的另一个障碍吗】[blog]\r\n【博客】【基于ChatGPT的数据增强方法：ChatAug和AugGPT】[blog]\r\n【博客】【Character.AI\r\n在ChatGPT基础上加入个性化、UGC两大武器，有比 ChatGPT\r\n更丰富的使用场景】[blog]\r\n【博客】【让ChatGPT可以语音交互】[blog]\r\n【博客】【“ChatGPT们”的淘金时代】[blog]\r\n【博客】【70 款 ChatGPT 插件评测（含样例分析）】[blog]\r\n【论文】【人大提出WebBrain：NLP新任务，通过网络数据的挖掘生成真实文章】[paper]，[code]\r\n【医疗】【ChatGPT爆火带来思考：医学界或将迎来与AI融合的奇点？】[blog]\r\n【教育】【论ChatGPT大语言模型在教育中的机遇与挑战\r\n】[blog]\r\n【投资】【ChatGPT在投资研究领域的应用初探及原理分析】[blog]\r\n【软件】【OpenAI总裁Greg\r\nBrockman转发｜一种编译语言的调试器，利用ChatGPT旨在增强您使用GDB进行调试体验】[code]\r\n【软件】【不必排队等 OpenAI Plugins，OpenBMB\r\n开源大模型工具学习引擎】[blog]\r\n【其他】【分析了ChatGPT技术以及落地应用场景 】[blog]\r\nChatGPT 工具篇\r\n【工具】【ChatGPT 应用汇总及操作手册】[blog]\r\n【工具】【ChatGPT提示和技巧速查手册】[blog]\r\n【工具】【非常全面的ChatGPT、LLM相关资源整理分享】[code]\r\n【工具】【ChatGPT超全面课程】[blog]\r\n【工具】【BloombergGPT: A Large Language Model for\r\nFinance】[paper]\r\n【工具】【ChatPDF：一键上传PDF文件即可解读 】[blog]，[试用地址]\r\n【工具】【ChatWeb：可爬取网页正文，并根据正文回答问题\r\n】[code]\r\n【工具】【chatgpt_academic：中科院基于 ChatGPT\r\n专属定制的学术研究及日常开发工具】[blog]，[code]，[demo]\r\n【工具】【Einstein GPT：SaaS 行业巨头 Salesforce\r\n宣布与 OpenAI 合作，推出 Einstein\r\nGPT，这是全球首个用于客户关系管理（CRM）的生成式 AI 产品 】[Einstein\r\nGPT地址]，[试用地址]\r\n【工具】【HuggingGPT: Solving AI Tasks with ChatGPT\r\nand its Friends in HuggingFace 】[paper]\r\n【工具】【ImpressionGPT：\r\n利用ChatGPT对放射科报告进行总结的迭代优化框架】[paper]\r\n【工具】【OpenGpt：创建ChatGPT小应用的AI平台】[官网]，[code]\r\n【工具】【TagGPT：腾讯提出零样本多模态标签的大语言模型TagGPT】[paper]，[code]\r\n【工具】【Visual ChatGPT:\r\n在视觉模型加持下的ChatGPT，聊天生图全拿捏了。】[paper]\r\n【工具】【NetGPT：用于网络流量的生成预训练Transformer模型】[paper]\r\nChatGPT 技术篇\r\n【符尧】【深度拆解GPT-3.5能力起源】[原文blog]，[译文blog]\r\n【知乎】【ChatGPT发展历程、原理、技术架构详解和产业未来】[blog]\r\n【斯坦福】【82页PPT ！最新ChatGPT: 提示学习,\r\n指导微调和RLHF 】[blog]，[提取码:chat]\r\n【微软】【让天下没有难训练的大模型，微软亚洲研究院开源TorchScale\r\n】[code]\r\n【亚马逊 】【他们提出了包含视觉特征的\r\nMultimodal-CoT，该架构在参数量小于 10 亿的情况下，在 ScienceQA\r\n基准测试中，比 GPT-3.5 高出 16 个百分点 】[paper]，[code]\r\n【OpenBMB】【Nature ：生成式 AI 的前景与风险】[blog]\r\n【博客】【万字长文解读：从Transformer到ChatGPT，通用人工智能曙光初现】[blog]\r\n【博客】ChatGPT_Inference_Cost\r\n【博客】ChatGPT_Official_API_Learning\r\n【博客】ChatGPT_Parameter_is_not_175B\r\n【博客】ChatGPT_Road_Map_from_yao.fu\r\n【博客】Lessons_Learned_from_ChatGPT_Recurrence\r\n【博客】LLM_Pre-training_Guide（Bloom-175B）\r\n【博客】The_guide_of_training_LLM\r\n【博客】【AI芯片制造商Cerebras发布7个基于GPT的大语言模型，现已开源】[官网地址\r\n)]，[GPT地址]，[Hugging Face地址]\r\n【博客】【大模型论文周报丨GPT-4发布，谷歌开放PaLM\r\nAPI，斯坦福7B开源模型Alpaca媲美GPT-3.5】[blog]\r\n【博客】【LLaMA模型Meta版泄露，GitHub获8K星】[blog]\r\n【博客】【ChatGPT or Grammarly? Evaluating ChatGPT\r\non Grammatical Error Correction Benchmark 】[paper]\r\n【博客】【打造中国版ChatGPT，国内哪家实力最强】[blog]\r\n【博客】【复旦大学邱锡鹏教授解读ChatGPT】[blog]\r\n【博客】【万字长文:可能是全网最晚的ChatGPT技术总结\r\n】[blog]\r\n【博客】【ChatGPT作为知识库问答系统的问答能力评测\r\n】[blog]\r\n【博客】【ChatGPT作者John\r\nShulman：我们成功的秘密武器】[blog]，[blog译文]\r\n【博客】【ChatGPT 是数据隐私的另一个障碍吗】[blog]\r\n【博客】【Hugging Face 每周速递: ChatGPT API\r\n怎么用？我们帮你搭好页面了 】[blog]\r\n【博客】【复旦大学教授肖仰华：ChatGPT\r\n浪潮下，面向大模型如何做数据治理？】[blog]\r\n【博客】【腾讯在ChatGPT的布局】[blog]\r\n【博客】【浅析ChatGPT：历史沿革、应用现状及前景展望】[blog]\r\n【博客】【ChatGPT 背后的“功臣”——人类反馈强化学习RLHF\r\n技术详解】[blog]\r\n【博客】【万字长文解析！复现和使用GPT-3/ChatGPT，你所应该知道的】[blog]\r\n【博客】【想训练ChatGPT？得先弄明白Reward\r\nModel怎么训（附源码） 】[blog]\r\n【博客】【ChatGPT核心技术：强化学习PPO算法】[blog]\r\n【博客】【解读 ChatGPT\r\n背后的技术重点：RLHF、IFT、CoT、红蓝对抗】[blog]\r\n【博客】【OpenAI ChatGPT Code Interpreter入门】[blog]\r\n【伦理】【加拿大魁北克大学教授详述：我们该拿ChatGPT怎么办？】[blog]\r\n【论文】【AIGC时代的ChatGPT全面综述】[paper]\r\n【论文】【ChatGPT is a Knowledgeable but\r\nInexperienced Solver: An Investigation of Commonsense Problem in Large\r\nLanguage Models】[paper]\r\n【论文】【GPT-3 和 GPT-3.5 系列模型的全面分析】[paper]\r\n【论文】【ChatGPT Outperforms Crowd-Workers for\r\nText-Annotation Tasks】[paper]\r\n【论文】【微软&amp;佐治亚理工学院 |\r\nAdaLoRA：自适应预算分配以实现参数有效的微调】[paper]，[code]\r\n【论文】【微软 | 大型语言模型的语境忠实提示法】[paper]\r\n【论文】【KAUST |\r\nChatGPT问，BLIP-2回答模型：面向丰富的视觉描述的自动提问】[paper]，[code]\r\n【论文】【ChatGPT真的可以取代知识图谱问答吗？ 】[paper]，[paper翻译]\r\n【论文】【Meta &amp;\r\n斯坦福大学推出FlexGen：用单个GPU进行大型语言模型的高吞吐量生成性推理】[paper]，[code]\r\n【论文】【ChatGPT破圈的「秘密武器」：详解RLHF如何影响人类社会！\r\n】[paper]，[blog]\r\n【论文】【探讨ChatGPT在对抗攻击和分布外泛化下的鲁棒性】[paper]，[code]\r\n【论文】【复旦清华联合顶刊发文｜ChatGPT：潜力、前景和局限\r\n】[blog]，[paper]\r\n【论文】【引导ChatGPT不要输出有害信息】[paper]\r\n【论文】【Junnan\r\nLi大佬发表最新多模态的杰作BLIP2】[paper]，[code]，[blog]\r\n【论文】【Instruction Tuning：无/少样本学习新范式\r\n】[paper]，[code]\r\n【论文】【GPTScore：一种新的评估语言模型方法】[paper]，[code]\r\n【论文】【ChatGPT内核：InstructGPT，基于反馈指令的PPO强化学习】[blog]，[B站]\r\n【论文】【Fine-tune-CoT：小模型也能做推理，完美逆袭大模型\r\n】[paper]，[code]\r\n【论文】【ChatGPT的潜力解锁：自然语言处理中应用、优势、限制和未来方向的全面探索】[paper]\r\n【论文】【阿里巴巴&amp;清华大学|大型语言模型在算术任务中的表现如何？】[paper]，[code]\r\n【代码】【本科生60行代码教你手搓GPT大模型 】[code]\r\nGPT4\r\nGPT4 官方文档\r\n【博客】【GPT4_System_Card中文翻译】[blog]\r\n【博客】【GPT4_Technical_Report中文翻译】[blog]\r\nGPT4 博客篇\r\n【博客】【【万字长文】GPT-4秘密泄露！所有的信息都在这里！从GPT-4\r\n架构、基础设施、训练数据集、成本、视觉到MoE！】[blog]，[原blog]\r\n【纽约时报】【GPT-4 令人印象深刻但仍在 10\r\n个方面具有缺陷】[blog]\r\n【Open AI】【多模态大模型GPT-4的新突破】[blog]\r\n【OpenAI】【重磅发布GPT-4】[blog]\r\n【OpenAI】【GPT-4 创造者 Ilya Sutskever 谈 AI 幻觉和\r\nAI 民主】[blog]\r\n【OpenAI】【GPT-4创造者：第二次改变AI浪潮的方向】[blog]\r\n【OpenAI】【当GPT-4进入北京市2022高考考场能有什么表现？】[blog]\r\n【博客】GPT4技术细节\r\n【博客】GPT4技术关键点总结\r\n【博客】GPT4和ChatGPT的效果对比\r\n【博客】The\r\nUltimate GPT-4 Guide\r\nGPT4 论文篇\r\n【微软】【用GPT-4进行指令调优】[paper]，[code]\r\n【论文】【点燃通用人工智能的火花：GPT-4的早期实验】[原始paper]，[中文版paper]，[blog]\r\n【论文】【GPT4All：用GPT-3.5-Turbo的大规模数据提炼训练一个助理式聊天机器人】[paper]，[code]\r\n【论文】【美国东北大学：可以通过要求GPT4反思“你为什么错了？”来提高30%的性能】[paper]，[code]\r\n【论文】【对ChatGPT/GPT-4研究的总结以及对大型语言模型未来的展望】[paper]\r\n【论文】【评估日本医疗执照考试的GPT-4和ChatGPT】[paper]\r\n【论文】【Amazon |\r\n深入研究LLMs与AutoGPT的结合：揭示出GPT-4惊人的人类决策能力！】[blog]，[paper]，[code]\r\nAnima\r\n【33B QLoRA大语言模型Anima的性能超越了对比的所有的中文开源模型。】[blog]，[code]，[model]\r\nBard\r\n【谷歌再次开放Bard访问权，向着ChatGPT发起再一次攻击】[报名地址\r\n)]，[blog]，[theverge]\r\nBaize\r\n【用ChatGPT训练羊驼：「Baize」开源，轻松构建专属模型】[blog]，[paper]，[code]，[demo]\r\nbaichuan以及扩展\r\n【baichuan-7b】【王小川大模型首亮相！70亿参数霸榜，清北抢先用｜独家专访】[blog]，[Hugging\r\nFace]，[code]，[Model\r\nScope]，[C-EVAL]\r\n【firefly-baichuan-7b-qlora-sft】[使用Firefly项目中的QLoRA训练流程，在moss-003-sft-data百万多轮指令数据上进行了指令微调baichuan-7b模型]，[blog]，[Hugging\r\nFace model]，[code]，[Model\r\nScope]，[C-EVAL]\r\nBLOOM\r\n【【LLM系列之BLOOM】BLOOM: 多语言大模型】[blog]，[paper]，[code]，[huggingface]\r\nBiomedGPT\r\n【BiomedGPT: 统一通用的生物医学生成式预训练Transformer】[paper]\r\nClaude\r\n【ChatGPT最强竞品Claude今日开放API】[产品地址]，[申请地址]，[API说明]，[blog]，[Claude支持100k上下文]，[Claude2发布]\r\nClaude 2\r\n【ChatGPT最强竞品Claude2来了】[blog]\r\nChatGLM-6B以及扩展\r\n【ChatGLM：千亿基座的对话模型开启内测 ⸺对应单卡版本开源】[blog]，[code]\r\n【chatglm+langchain+互联网，你可以将大模型接入网络了】[blog]，[code]\r\n【Chinese-LangChain】【基于ChatGLM-6b+langchain实现本地化知识库检索与智能答案生成】[code]，[blog]\r\n【ChatGLM_multi_gpu_zero_Tuning：简单高效实现多卡微调大模型】[code]\r\n【浅尝prompt咒语设计：one-shot微调chatglm-6b实践信息抽取】[blog]\r\n【ChatGLM-6B模型结构组件源码阅读】[blog]\r\n【基于1万亿token开源大模型Falcon，超越650亿的LLaMA，可商用】[blog1]，[blog2]\r\nChatYuan\r\n【ChatYuan：基于PromptCLUE-large的中文对话开源大模型】[blog]\r\nCopilot X\r\n【GitHub Copilot X编辑器发布，大大提升编码速度】[blog]\r\nColossalAI\r\n【穷孩子如何体验ColossalAI SFT（Colab篇）】[blog]\r\nCPM-Bee\r\n【中文基座模型CPM-Bee开源了】[blog]，[code]，[HuggingFace]\r\nChatDB\r\n【清华大学和北京智源人工智能研究院的研究者们提出了ChatDB：用数据库作为符号性记忆模块来增强大语言模型】[blog]，[paper]，[主页]，[code]\r\nDolly\r\n【声称它\r\n\"像ChatGPT一样神奇\"，但只需要使用一台机器在不到三个小时的时间里训练的数据少得多。】[blog]，[Databricks Inc地址]\r\nDolly2.0\r\n【Databricks的dolly-v2-12b，是一个在Databricks机器学习平台上训练的指令跟随型大型语言模型】[blog_en]，[blog_zh]\r\nDeepSpeed-Chat\r\n【DeepSpeed对话：易于使用、快速而实惠的RLHF训练，在各种规模下训练ChatGPT模型】[code]，[blog]\r\nFrugalGPT\r\n【斯坦福提出FrugalGPT｜性能媲美GPT4，成本降低98%】[paper]，[blog]\r\nGPT3.5\r\n【GPT3.5试用地址 】[试用地址]\r\nJittorLLMs\r\n【笔记本没有显卡也能跑大模型，具有高性能、配置要求低、中文支持好、可移植等特点】[code]\r\nLLM as Controller\r\n【LLM as Controller—无限拓展LLM的能力边界】[blog]\r\nMetaGPT\r\n【MetaGPT：多角色元编程框架】[code]\r\nMiniGPT-4\r\n【类似GPT-4图像理解与对话能力的AI大模型，已开源】[主页]，[paper]，[code]，[video]，[dataset]，[Demo]，[Demo1]，[Demo2]，[Demo3]，[Demo4]\r\nMOSS\r\n【FudanNLP团队最新成果，借助RLHF实现人类对齐的MOSS-RLHF来了】[blog]，[code]，[测试链接]，[模型权重]，[数据集]\r\nOpenChatKit\r\n【ChatGPT开源平替OpenChatKit：参数量200亿，在4300万条指令上微调而成】[blog]，[code]，[技术报告]\r\nOpenAssistant\r\n【ChatGPT全球最大开源平替OpenAssistant，基于Pythia和LLaMA微调而来，主要用于训练人类标注的数据，支持35种语言，免费可用RLHF数据】[官网]，[paper]，[code]，[dataset]，[youtube]\r\nWebCPM\r\n【首个联网支持中文问答开源模型WebCPM】[paper]，[code]，[blog]\r\nLLaMA以及扩展\r\n【LLaMA】【Meta开放小模型LLaMA，性能超过GPT-3】[paper]，[code]，[blog1]，[blog2]，[详聊LLaMA大模型的技术细节]\r\n【LLaMA 2】【LLaMA 2技术细节详细介绍！】[blog]，[在 Hugging Face\r\n上玩转LLaMA 2]，[伯克利AI博士详解Llama\r\n2的技术细节]，[Chinese-LlaMA2]\r\n【llama2.c】【OpenAI联创Karpathy爱上羊驼：纯C代码实现婴儿Llama2，MacBook可运行，已揽1.6k星】[blog]，[code]\r\n【LLaMA评测】[blog]\r\n【Alpaca】【斯坦福发布了一个由LLaMA\r\n7B微调的模型Alpaca（羊驼），训练3小时，性能比肩GPT-3.5】[blog]，[官网]，[model]，[code]\r\n【Alpaca-CoT】【Alpaca-CoT：多接口统一的轻量级LLM指令微调平台】[code]，[官网]\r\n【BiLLa】【BiLLa 是开源的推理能力增强的中英双语\r\nLLaMA 模型】[blog]，[code]\r\n【CaMA】【一种支持中英语言的LLaMA模型】[code]\r\n【ChatLLaMA】【初创公司 Nebuly\r\nAI在LLaMA基础上加入RLHF 开源 ChatLLaMA 训练方法】[code]\r\n【ColossalAI】【完整复现ChatGPT全流程】[code]\r\n【ColossalChat】【用于克隆 ChatGPT 和完整 RLHF\r\n管道的开源解决方案】[code]，[blog]\r\n【CAMEL】【从LLaMA衍生并适应临床的模型】[code]，[blog]\r\n【草本（原华驼）】【让LLaMA模型成为中医专家】[paper]，[code]，[blog1]，[blog2]\r\n【DB-GPT】【基于vicuna-13b和FastChat的开源实验项目】[code]\r\n【DeepSpeed-Chat】【最强ChatGPT训练框架，一键完成RLHF训练！\r\n】[code]，[blog]\r\n【ExpertLLaMA】【一个使用ExpertPrompting构建的开源聊天机器人，其能力达到ChatGPT的96%。】[code]\r\n【FreedomGPT】【FreedomGPT使用Electron 和\r\nReact构建，它是一个桌面应用程序，允许用户在他们的本地机器上运行LLaMA。】[官网地址]\r\n【FLAN】【【LLM系列之FLAN】Scaling\r\nInstruction-Finetuned Language Models】[blog]\r\n【GoGPT/GoGPT2】【基于Llama/Llama\r\n2训练的底座大模型,再扩充词表+继续预训练】[GoGPT code]，[GoGPT2 code]\r\n【Koala】【加州大学BAIR团队提出Koala：学术研究的对话模型】[blog_zh]，[blog_en]\r\n【LLaMA-Adapter】【LLaMA-Adapter，一种用于微调指令遵循LLaMA模型的轻量级自适应方法，使用Stanford\r\nAlpaca提供的 52K 数据。】[paper]，[code]\r\n【LaVIN】【MMA方案让羊驼模型实现多模态：训练时间减少71.4%，成本节省99.9%】[paper]，[code]，[blog]\r\n【lit-llama】【基于nanoGPT的LLaMA语言模型，支持量化、LoRA微调和预训练】[code]\r\n【LlamaIndex】【面向QA 系统的全新文档摘要索引】[blog]\r\n【llama.cpp】【量化130亿参数LLaMA模型的llama.cpp，推理仅需4GB内存】[blog]\r\n【llama.cpp优化版】【Edge AI 变得更快|在 C/C++\r\n中移植 Facebook 的 LLaMA 模型】[blog]\r\n【LIMA】【使用 LoRA 技术对 LLaMA 65B\r\n大模型进行微调及推理】[blog]\r\n【PaLM】【【LLM系列之PaLM】PaLM: Scaling Language\r\nModeling with Pathways】[blog]\r\n【StackLLaMA】【使用 RLHF 训练 LLaMA 的实践指南】[blog_zh]，[blog_en]\r\n【Vicuna】【通过对从ShareGPT收集的用户共享对话进行微调的LLaMA训练，Vicuna-13B达到了OpenAI\r\nChatGPT和Google Bard 90%*以上的质量 】[Vicuna官网地址]，[blog]\r\n图像、视频生成\r\n【博客】【Genmo\r\nChat】【这是一款创造性的copilot，使用GPT-4和一大套生成人工智能工具创建并编辑您需要的任何视频或图像。\r\n】[blog]\r\n【博客】【BlenderGPT】【一款基于GPT-4的扩展程序BlenderGPT开源，这是一个由GPT3/4驱动的全能AI编辑助手，为Blender提供支持\r\n】[code]\r\n【博客】【Firefly】【Adobe制造了一个人工智能图像生成器--并表示它没有窃取艺术家的作品来做这件事\r\n】[blog]\r\n【博客】【Bing Image Creator】【微软推出Bing Image\r\nCreator，用户可根据文本提示创建图片】[blog]\r\n【博客】【Hugging Face\r\n现已支持使用达摩院text-to-video模型从文本生成视频】[模型地址]\r\n【论文】【最新女娲大模型，中科院提出NUWA-XL：扩散模型中的扩散，生成超长视频】[paper]，[blog]\r\n【论文】【艾伦AI研究院 &amp; 华盛顿大学 |\r\nCHAMPAGNE：从大规模的网络视频中学习真实世界的对话】[paper]，[code]\r\n【论文】【用AI直接复现你在想什么，Stable\r\nDiffusion逼真复现图像】[paper]，[blog]\r\n【论文】【Stable\r\nDiffusion公司新作Gen-1：基于扩散模型的视频合成新模型，加特效杠杠的！】[paper]，[site]\r\n【论文】【使用Diffusers 实现 ControlNet\r\n高速推理】[blog]\r\n【论文】【文生图引入ControlNet，深度、边缘信息全能复用\r\n】[paper]，[code]\r\n【论文】【ChatGPT｜可用于AI绘画，效果飞升47% 】[paper]\r\n【论文】【智源研究院提出SegGPT：\r\n一个用于分割上下文中所有事物的通用模型】[paper]\r\n【论文】【OpenAI开源新模型代码库Consistency\r\nModels，无需对抗训练即可快速获得高质量样本】[paper]，[code]，[blog]\r\n【可控图文大模型】【伯克利&amp;微软｜用GPT-4进行可控的文本-图像生成】[paper]\r\n代码生成\r\n【综述】【代码大模型综述：中科院和MSRA调研27个LLMs，并给出5个有趣挑战】[blog]，[paper]，[项目主页]\r\n【博客】【GPT-Engineer｜提需求即可生成整个代码库，已20K星】[blog]，[code]\r\n【博客】【StarCoder: 最先进的代码大模型】[blog]\r\n【论文】【北京大学：具有大语言模型的自我规划代码生成】[paper]\r\n【论文】【谷歌提出Self-Debugging:教导大型语言模型进行自我调试】[paper]\r\n【论文】【通过自我改进实现更好的代码语言模型，显著提高模型生成任务的性能】[paper]\r\n【论文】【Baldur:\r\n基于大型语言模型的完全证明生成与修复】[paper]\r\n【论文】【CodeGeeX: A Pre-Trained Model for Code\r\nGeneration with Multilingual Evaluations on HumanEval-X 】[paper]，[code]\r\n【论文】【代码模型 CodeGeeX2-6B\r\n开源，最低6GB显存，性能优于StarCoder】[blog]，[code]\r\n【论文】【CodeT5+：非常灵活的、面向代码理解和生成的开放大型代码语言模型】[paper]\r\n【工具】【Cursor：一个集成了 GPT-4\r\n的国内直接可以访问的，优秀而强大的免费代码生成器，可以帮助你快速编写、编辑和讨论代码。】[官网地址]\r\n【论文】【MIT最新研究：利用大预言模型生成Code】[paper]，[code]，[项目网址]\r\n【论文】【MathPrompter:\r\n基于大型语言模型的数学推理】[paper]\r\n【论文】【MIT最新研究：利用大语言模型生成Code】[paper]，[code]，[官网地址]\r\n语音生成\r\n【论文】【Meta AI研究者推出MUSICGEN】[paper]，[blog]，[demo]\r\n【论文】【文字、图片一键生成逼真音效，音频界AIGC来了】[paper]，[code]\r\n【论文】【音乐可视化｜利用大型语言模型和文本到图像模型帮助生成「音乐迪斯科」】[paper]，[blog]\r\n【论文】【MetaAI发布第一个生成的人工智能语音模型Voicebox】[blog]，[paper]\r\n多模态生成\r\n【BLIP-2】【高效训练多模态大模型（BLIP-2）】[paper]，[code]，[demo]，[doc]，[fine-tuing]，[hugging face\r\nspaces]\r\n【VisCPM】【SOTA 开源中文多模态大模型】[blog]，[code]\r\n【HuggingFace Transformers\r\nAgents】【一键控制10万多个AI模型，HuggingFace给类ChatGPT模型们做了个「APP\r\nStore」】[demo]，[blog]\r\n【LLaVA】【熔岩羊驼LLaVA来了：像GPT-4一样可以看图聊天，无需邀请码，在线可玩】[paper]，[introduce]\r\n【UniDiffuser】【清华朱军团队开源UniDiffuser：首个基于Transformer的多模态扩散大模型！文图互生、改写全拿下！】[paper]，[code]\r\n【Video-LLaMA】【人机视频对话｜Video-LLaMA多模态框架，使大型语言模型具备了理解视频内容的能力】[paper]\r\n【X-LLM】【多模态语言训练大模型】[项目地址]，[paper]\r\n四、构筑大语言模型应用：应用开发与架构设计\r\n一系列的流行的或者不流行的开源项目，涉及如下：\r\n\r\nLLM 能力的充分运用\r\n\r\nPrompt 编写：Prompt 学习与编写模式\r\nPrompt 管理：Prompt 即代码\r\n\r\nLLM 下的软件开发工序及应用架构设计\r\n\r\n新的交互设计：Chat 模式\r\n大模型友好的工序：基于 AI 2.0 （ChatGPT +\r\nCopilot）如何去设计软件开发流程\r\nLLM 应用架构的设计与落地：Unit Mesh\r\n\r\n面向特定场景的 LLM 应用\r\n\r\n基于开源模型构建自己的模型：特定场景的模型微调 + LLMOps\r\n上下文工程（prompt 工程）：LLM 应用的核心\r\n\r\n\r\n围绕于上述的一系列内容，我们也在思考软件开发能给我们带来了什么。所以，我重新整理了过去半年的一些思考、文章，重新编写了这本开源电子书，希望能够帮助到大家。\r\n相关开源项目如下（包括但是不限于）：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n名称\r\n描述\r\n类型\r\nStars\r\n\r\n\r\n\r\n\r\n理解\r\nPrompt\r\n基于编程、绘画、写作的 AI 探索与总结。\r\n文档\r\n\r\n\r\n\r\nPrompt\r\n编写模式\r\n如何将思维框架赋予机器，以设计模式的形式来思考 prompt。\r\n文档\r\n\r\n\r\n\r\nClickPrompt\r\n用于一键轻松查看、分享和执行您的 Prompt。\r\n应用\r\n\r\n\r\n\r\nChatVisualNovel\r\n基于 ChatGPT 的定制化视觉小说引擎\r\n应用\r\n\r\n\r\n\r\nChatFlow\r\n打造个性化 ChatGPT 流程，构建自动化之路。\r\n框架\r\n\r\n\r\n\r\nUnit Mesh\r\n基于 AI 为核心的软件 2.0 思想的软件架构。\r\n架构\r\n\r\n\r\n\r\nUnit\r\nMinions\r\nAI 研发提效研究：自己动手训练 LoRA\r\n微调教程、指南、数据集\r\n\r\n\r\n\r\nUnit\r\nRuntime\r\n一个 ChatGPT 等 AI\r\n代码的运行环境，可一键启动并实时交互，帮助您快速构建和测试 AI\r\n代码。\r\n基础设施\r\n\r\n\r\n\r\nDevTi\r\n基于 LLM\r\n的微调来提供全面智能化解决方案，助力开发人员高效完成开发任务，以实现自动化用户任务拆解、用户故事生成、自动化代码生成、自动化测试生成等等。\r\n微调代码\r\n\r\n\r\n\r\nAutoDev\r\n一款 Intellij IDEA 的 LLM/AI 辅助编程插件。AutoDev\r\n能够与您的需求管理系统（例如 Jira、Trello、Github Issue\r\n等）直接对接。\r\nIDEA 插件\r\n\r\n\r\n\r\nArchGuard\r\nCo-mate\r\n基于人工智能技术的架构副驾驶、设计和治理工具\r\n架构协同应用\r\n\r\n\r\n\r\n\r\n五、Prompt&amp;LLM论文、开源数据&amp;模型、AIGC应用\r\n目录顺序如下\r\n\r\n国内外，垂直领域大模型\r\nAgent和指令微调等训练框架\r\n开源指令，预训练，rlhf，对话，agent训练数据梳理\r\nAIGC相关应用\r\nprompt写作指南和5星博客等资源梳理\r\nPrompt和LLM论文细分方向梳理\r\n\r\n参考来源：https://github.com/DSXiangLi/DecryptPrompt/tree/main\r\nDecryptPrompt\r\n\r\n如果LLM的突然到来让你感到沮丧，不妨读下主目录的Choose Your Weapon\r\nSurvival Strategies for Depressed AI Academics 持续更新以下内容，Star to\r\nkeep updated~\r\n\r\n目录顺序如下\r\n\r\n国内外，垂直领域大模型\r\nAgent和指令微调等训练框架\r\n开源指令，预训练，rlhf，对话，agent训练数据梳理\r\nAIGC相关应用\r\nprompt写作指南和5星博客等资源梳理\r\nPrompt和LLM论文细分方向梳理\r\n\r\nMy blogs &amp; ChatGPT应用\r\n\r\n解密Prompt系列1.\r\nTunning-Free Prompt：GPT2 &amp; GPT3 &amp; LAMA &amp;\r\nAutoPrompt\r\n解密Prompt系列2.\r\n冻结Prompt微调LM： T5 &amp; PET &amp; LM-BFF\r\n解密Prompt系列3.\r\n冻结LM微调Prompt: Prefix-tuning &amp; Prompt-tuning &amp;\r\nP-tuning\r\n解密Prompt系列4.\r\n升级Instruction Tuning：Flan/T0/InstructGPT/TKInstruct\r\n解密prompt系列5.\r\nAPE+SELF=自动化指令集构建代码实现\r\n解密Prompt系列6.\r\nlora指令微调扣细节-请冷静,1个小时真不够~\r\n解密Prompt系列7.\r\n偏好对齐RLHF-OpenAI·DeepMind·Anthropic对比分析\r\n解密Prompt系列8.\r\n无需训练让LLM支持超长输入:知识库 &amp; Unlimiformer &amp; PCW &amp;\r\nNBCE\r\n解密Prompt系列9.\r\n模型复杂推理-思维链基础和进阶玩法\r\n解密Prompt系列10.\r\n思维链COT原理探究\r\n解密Prompt系列11.\r\n小模型也能COT，先天不足后天补\r\n解密Prompt系列12.\r\nLLM Agent零微调范式 ReAct &amp; Self Ask\r\n解密Prompt系列13.\r\nLLM Agent指令微调方案: Toolformer &amp; Gorilla\r\n解密Prompt系列14.\r\nLLM Agent之搜索应用设计：WebGPT &amp; WebGLM &amp; WebCPM\r\n解密Prompt系列15.\r\nLLM Agent之数据库应用设计：DIN &amp; C3 &amp; SQL-Palm &amp;\r\nBIRD\r\n解密Prompt系列16.\r\nLLM对齐经验之数据越少越好？LTD &amp; LIMA &amp; AlpaGasus\r\n解密Prompt系列17.\r\nLLM对齐方案再升级 WizardLM &amp; BackTranslation &amp;\r\nSELF-ALIGN\r\n\r\nLLMS\r\n模型评测\r\n\r\n大模型评估尚未出现北极星指标，榜单排名往往和实际使用能力存在较大差异，几天没看感觉有的榜单快被玩坏了......\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n榜单\r\n结果\r\n\r\n\r\n\r\n\r\nAlpacaEval：LLM-based\r\nautomatic evaluation\r\n开源模型王者vicuna,openchat, wizardlm\r\n\r\n\r\nHuggingface\r\nOpen LLM Leaderboard\r\nMMLU只评估开源模型，Falcon夺冠，在Eleuther\r\nAI4个评估集上评估的LLM模型榜单,vicuna夺冠\r\n\r\n\r\nBerkley出品大模型排位赛榜有准中文榜单\r\nElo评分机制，GPT4自然是稳居第一，GPT4&gt;Claude&gt;GPT3.5&gt;Vicuna&gt;others\r\n\r\n\r\nCMU开源聊天机器人评测应用\r\nChatGPT&gt;Vicuna&gt;others；在对话场景中训练可能很重要\r\n\r\n\r\nZ-Bench中文真格基金评测\r\n国产中文模型的编程可用性还相对较低，大家水平差不太多，两版ChatGLM提升明显\r\n\r\n\r\nChain-of-thought评估\r\nGSM8k, MATH等复杂问题排行榜\r\n\r\n\r\nInfoQ\r\n大模型综合能力评估\r\n面向中文，ChatGPT&gt;文心一言&gt; Claude&gt;星火\r\n\r\n\r\nToolBench:\r\n工具调用评估榜单\r\n工具微调模型和ChatGPT进行对比，提供评测脚本\r\n\r\n\r\nAgentBench:\r\n推理决策评估榜单\r\n清华联合多高校推出不同任务环境，例如购物，家居，操作系统等场景下模型推理决策能力\r\n\r\n\r\nFlagEval\r\n智源出品主观+客观LLM评分榜单\r\n\r\n\r\nBird-Bench\r\n更贴合真实世界应用的超大数据库，需要领域知识的NL2SQL榜单，模型追赶人类尚有时日\r\n\r\n\r\nkola\r\n以世界知识为核心的评价基准，包括已知的百科知识和未知的近90天网络发布内容，评价知识记忆，理解，应用和创造能力\r\n\r\n\r\nCEVAL\r\n中文知识评估，覆盖52个学科，机器评价主要为多项选择\r\n\r\n\r\nCMMLU\r\n67个主题中文知识和推理能力评估，多项选择机器评估\r\n\r\n\r\n\r\n国外开源模型\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n模型链接\r\n模型描述\r\n\r\n\r\n\r\n\r\nLLama2\r\nOpen Meta带着可商用开源的羊驼2模型来了~\r\n\r\n\r\nVicuna\r\nAlpaca前成员等开源以LLama13B为基础使用ShareGPT指令微调的模型，提出了用GPT4来评测模型效果\r\n\r\n\r\nWizardLM\r\n微软新发布13B，登顶AlpacaEval开源模型Top3，使用ChatGPT对指令进行复杂度进化微调LLama2\r\n\r\n\r\nMistral7B\r\n法国“openai”开源，超过llama2当前最好7B模型\r\n\r\n\r\nOpenChat\r\n80k ShareGPT对话微调LLama-2 13B开源模型中的战斗机\r\n\r\n\r\nGuanaco\r\nLLama 7B基座，在alpaca52K数据上加入534K多语言指令数据微调\r\n\r\n\r\nLLaMA\r\nMeta开源指令微调LLM，规模70 亿到 650 亿不等\r\n\r\n\r\nMPT\r\nMosaicML开源的预训练+指令微调的新模型，可商用，支持84k\r\ntokens超长输入\r\n\r\n\r\nFalcon\r\nFalcon由阿联酋技术研究所在超高质量1万亿Token上训练得到1B，7B，40B开源，免费商用！土豪们表示钱什么的格局小了\r\n\r\n\r\nRedPajama\r\nRedPajama项目既开源预训练数据后开源3B，7B的预训练+指令微调模型\r\n\r\n\r\nkoala\r\n使用alpaca，HC3等开源指令集+\r\nShareGPT等ChatGPT数据微调llama，在榜单上排名较高\r\n\r\n\r\nChatLLaMA\r\n基于RLHF微调了LLaMA\r\n\r\n\r\nAlpaca\r\n斯坦福开源的使用52k数据在7B的LLaMA上微调得到，\r\n\r\n\r\nAlpaca-lora\r\nLORA微调的LLaMA\r\n\r\n\r\nDromedary\r\nIBM self-aligned model with the LLaMA base\r\n\r\n\r\nColossalChat\r\nHPC-AI Tech开源的Llama+RLHF微调\r\n\r\n\r\nMiniGPT4\r\nVicuna+BLIP2 文本视觉融合\r\n\r\n\r\nStackLLama\r\nLLama使用Stackexchange数据+SFT+RL\r\n\r\n\r\nCerebras\r\nCerebras开源了1亿到130亿的7个模型，从预训练数据到参数全开源\r\n\r\n\r\nPaLM-E\r\n谷歌多模态大模型，540B的PaLM语言模型和22B的ViT视觉模型相结合，得到562B的PaLM-E模型，在机器人应用场景有了新的突破\r\n\r\n\r\nDolly-v2\r\n可商用 7b指令微调开源模型在GPT-J-6B上微调\r\n\r\n\r\nOpenChatKit\r\nopenai研究员打造GPT-NoX-20B微调+6B审核模型过滤\r\n\r\n\r\nMetaLM\r\n微软开源的大规模自监督预训练模型\r\n\r\n\r\nAmazon\r\nTitan\r\n亚马逊在aws上增加自家大模型\r\n\r\n\r\nOPT-IML\r\nMeta复刻GPT3，up to 175B, 不过效果并不及GPT3\r\n\r\n\r\nBloom\r\nBigScience出品，规模最大176B\r\n\r\n\r\nBloomZ\r\nBigScience出品, 基于Bloom微调\r\n\r\n\r\nGalacia\r\n和Bloom相似，更针对科研领域训练的模型\r\n\r\n\r\nT0\r\nBigScience出品，3B~11B的在T5进行指令微调的模型\r\n\r\n\r\nEXLLama\r\nPython/C++/CUDA implementation of Llama for use with 4-bit GPTQ\r\nweight\r\n\r\n\r\nLongChat\r\nllama-13b使用condensing rotary embedding\r\ntechnique微调的长文本模型\r\n\r\n\r\nMPT-30B\r\nMosaicML开源的在8Ktoken上训练的大模型\r\n\r\n\r\n\r\n国内开源模型\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n模型链接\r\n模型描述\r\n\r\n\r\n\r\n\r\nChatGLM2\r\n32K长文本，FlashAttention+Multi-Query\r\nAttenion的显存优化，更强推理能力，哈哈不过很多简单问题也硬要COT，中英平行能力似乎略有下降的ChatGLM2，但是免费商用！\r\n\r\n\r\nChatGLM\r\n清华开源的、支持中英双语的对话语言模型，使用了代码训练，指令微调和RLHF。chatglm2支持超长文本，可免费商用啦！\r\n\r\n\r\nLLama2-chinese\r\n没等太久中文预训练微调后的llama2它来了~\r\n\r\n\r\nYuLan-chat2\r\n高瓴人工智能基于Llama-2中英双语继续预训练+指令微调/对话微调\r\n\r\n\r\nziya\r\nIDEA研究院在7B/13B llama上继续预训练+SFT+RM+PPO+HFTT+COHFT+RBRS\r\n\r\n\r\nBaichuan\r\n百川智能开源7B大模型可商用免费\r\n\r\n\r\nBaichuan2\r\n百川第二代，提供了7B/13B Base和chat的版本\r\n\r\n\r\nzephyr-7B\r\nHuggingFace 团队基于 UltraChat 和 UltraFeedback 训练了 Zephyr-7B\r\n模型\r\n\r\n\r\nXWin-LM\r\nllama2 + SFT + RLHF\r\n\r\n\r\nChinese-LLaMA-Alpaca\r\n哈工大中文指令微调的LLaMA\r\n\r\n\r\nMoss\r\n为复旦正名！开源了预训练，指令微调的全部数据和模型。可商用\r\n\r\n\r\nQwen-7B\r\n阿里开源，可商用，通义千文7B模型\r\n\r\n\r\nIntenrLM\r\n书生浦语在过万亿 token 数据上训练的多语千亿参数基座模型\r\n\r\n\r\nAquila2\r\n智源更新Aquila2模型系列包括全新34B\r\n\r\n\r\nAquila\r\n智源开源7B大模型可商用免费\r\n\r\n\r\nUltraLM系列\r\n面壁智能开源UltraLM13B，奖励模型UltraRM，和批评模型UltraCM\r\n\r\n\r\nkimi Chat\r\nMoonshot上线超长文本LLM 可输入20W上文需要申请试用\r\n\r\n\r\nPandaLLM\r\nLLAMA2上中文wiki继续预训练+COIG指令微调\r\n\r\n\r\nXVERSE\r\n据说中文超越llama2的元象开源模型13B模型\r\n\r\n\r\nBiLLa\r\nLLama词表扩充预训练+预训练和任务1比1混合SFT+指令样本SFT三阶段训练\r\n\r\n\r\nPhoenix\r\n港中文开源凤凰和奇美拉LLM，Bloom基座，40+语言支持\r\n\r\n\r\nWombat-7B\r\n达摩院开源无需强化学习使用RRHF对齐的语言模型, alpaca基座\r\n\r\n\r\nTigerBot\r\n虎博开源了7B 180B的模型以及预训练和微调语料\r\n\r\n\r\nLuotuo\r\n中文指令微调的LLaMA，和ChatGLM\r\n\r\n\r\nOpenBuddy\r\nLlama 多语言对话微调模型\r\n\r\n\r\nChinese\r\nVincuna\r\nLLama 7B基座，使用Belle+Guanaco数据训练\r\n\r\n\r\nLinly\r\nLlama\r\n7B基座，使用belle+guanaco+pclue+firefly+CSL+newscommentary等7个指令微调数据集训练\r\n\r\n\r\nFirefly\r\n中文2.6B模型，提升模型中文写作，古文能力，待开源全部训练代码，当前只有模型\r\n\r\n\r\nBaize\r\n使用100k self-chat对话数据微调的LLama\r\n\r\n\r\nBELLE\r\n使用ChatGPT生成数据对开源模型进行中文优化\r\n\r\n\r\nChatyuan\r\nchatgpt出来后最早的国内开源对话模型，T5架构是下面PromptCLUE的衍生模型\r\n\r\n\r\nPromptCLUE\r\n多任务Prompt语言模型\r\n\r\n\r\nPLUG\r\n阿里达摩院发布的大模型，提交申请会给下载链接\r\n\r\n\r\nCPM2.0\r\n智源发布CPM2.0\r\n\r\n\r\nGLM\r\n清华发布的中英双语130B预训练模型\r\n\r\n\r\nBayLing\r\n基于LLama7B/13B，增强的语言对齐的英语/中文大语言模型\r\n\r\n\r\n\r\n垂直领域模型&amp;进展\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n领域\r\n模型链接\r\n模型描述\r\n\r\n\r\n\r\n\r\n医疗\r\nMedGPT\r\n医联发布的\r\n\r\n\r\n医疗\r\nMedPalm\r\nGoogle在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到，同时构建了MultiMedQA\r\n\r\n\r\n医疗\r\nChatDoctor\r\n110K真实医患对话样本+5KChatGPT生成数据进行指令微调\r\n\r\n\r\n医疗\r\nHuatuo Med-ChatGLM\r\n医学知识图谱和chatgpt构建中文医学指令数据集+医学文献和chatgpt构建多轮问答数据\r\n\r\n\r\n医疗\r\nChinese-vicuna-med\r\nChinese-vicuna在cMedQA2数据上微调\r\n\r\n\r\n医疗\r\nOpenBioMed\r\n清华AIR开源轻量版BioMedGPT,\r\n知识图谱&amp;20+生物研究领域多模态预训练模型\r\n\r\n\r\n医疗\r\nDoctorGLM\r\nChatDoctor+MedDialog+CMD 多轮对话+单轮指令样本微调GLM\r\n\r\n\r\n医疗\r\nMedicalGPT-zh\r\n自建的医学数据库ChatGPT生成QA+16个情境下SELF构建情景对话\r\n\r\n\r\n医疗\r\nPMC-LLaMA\r\n医疗论文微调Llama\r\n\r\n\r\n医疗\r\nPULSE\r\nBloom微调+继续预训练\r\n\r\n\r\n医疗\r\nNHS-LLM\r\nChatgpt生成的医疗问答，对话，微调模型\r\n\r\n\r\n医疗\r\n神农医疗大模型\r\n以中医知识图谱的实体为中心生成的中医知识指令数据集11w+，微调LLama-7B\r\n\r\n\r\n医疗\r\n岐黄问道大模型\r\n3个子模型构成，已确诊疾病的临床治疗模型+基于症状的临床诊疗模型+中医养生条理模型，看起来是要ToB落地\r\n\r\n\r\n医疗\r\nZhongjing\r\n基于Ziya-LLama+医疗预训练+SFT+RLHF的中文医学大模型\r\n\r\n\r\n医疗\r\nMeChat\r\n心理咨询领域，通过chatgpt改写多轮对话56k\r\n\r\n\r\n医疗\r\nSoulChat\r\n心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调\r\nChatGLM-6B\r\n\r\n\r\n医疗\r\nMindChat\r\nMindChat-Baichuan-13B,Qwen-7B,MindChat-InternLM-7B使用不同基座在模型安全，共情，人类价值观对其上进行了强化\r\n\r\n\r\n医疗\r\nDISC-MedLLM\r\n疾病知识图谱构建QA对+QA对转化成单论对话+真实世界数据重构+人类偏好数据筛选，SFT微调baichuan\r\n\r\n\r\n法律\r\nLawGPT-zh\r\n利用ChatGPT清洗CrimeKgAssitant数据集得到52k单轮问答+我们根据中华人民共和国法律手册上最核心的9k法律条文，利用ChatGPT联想生成具体的情景问答+知识问答使用ChatGPT基于文本构建QA对\r\n\r\n\r\n法律\r\nLawGPT\r\n基于llama+扩充词表二次预训练+基于法律条款构建QA指令微调\r\n\r\n\r\n法律\r\nLawyer\r\nLlama\r\n法律指令微调数据集：咨询+法律考试+对话进行指令微调\r\n\r\n\r\n法律\r\nLexiLaw\r\n法律指令微调数据集：问答+书籍概念解释，法条内容进行指令微调\r\n\r\n\r\n法律\r\nChatLaw\r\n北大推出的法律大模型，应用形式很新颖类似频道内流一切功能皆融合在对话形式内\r\n\r\n\r\n法律\r\n录问模型\r\n在baichuan基础上40G二次预训练+100K指令微调，在知识库构建上采用了Emb+意图+关键词联想结合的方案\r\n\r\n\r\n金融\r\nFinChat.io\r\n使用最新的财务数据，电话会议记录，季度和年度报告，投资书籍等进行训练\r\n\r\n\r\n金融\r\nOpenGPT\r\n领域LLM指令样本生成+微调框架\r\n\r\n\r\n金融\r\n乾元BigBang金融2亿模型\r\n金融领域预训练+任务微调\r\n\r\n\r\n金融\r\n度小满千亿金融大模型\r\n在Bloom-176B的基础上进行金融+中文预训练和微调\r\n\r\n\r\n金融\r\nbondGPT\r\nGPT4在细分债券市场的应用开放申请中\r\n\r\n\r\n金融\r\nIndexGPT\r\nJPMorgan在研的生成式投资顾问\r\n\r\n\r\n金融\r\n恒生LightGPT\r\n金融领域继续预训练+插件化设计\r\n\r\n\r\n金融\r\n知彼阿尔法\r\n企查查商查大模型\r\n\r\n\r\n金融\r\nAlphaBox\r\n熵简科技发布大模型金融应用，多文档问答+会议转录+文档编辑\r\n\r\n\r\n金融\r\n曹植\r\n达观发布金融大模型融合data2text等金融任务，赋能报告写作\r\n\r\n\r\n金融\r\n聚宝盆\r\n基于 LLaMA\r\n系基模型经过中文金融知识指令精调/指令微调(Instruct-tuning)\r\n的微调模型\r\n\r\n\r\n金融\r\nPIXIU\r\n整理了多个金融任务数据集加入了时间序列数据进行指令微调\r\n\r\n\r\n金融\r\nChatFund\r\n韭圈儿发布的第一个基金大模型，看起来是做了多任务指令微调，和APP已有的数据功能进行了全方位的打通，从选基，到持仓分析等等\r\n\r\n\r\n金融\r\nFinGPT\r\n金融传统任务微调 or chatgpt生成金融工具调用\r\n\r\n\r\n金融\r\nCFGPT\r\n金融预训练+指令微调+RAG等检索任务增强\r\n\r\n\r\n编程\r\nStarcoder\r\n80种编程语言+Issue+Commit训练得到的编程大模型\r\n\r\n\r\n编程\r\nChatSQL\r\n基于ChatGLM实现NL2sql\r\n\r\n\r\n编程\r\ncodegeex\r\n13B预训练+微调多语言变成大模型\r\n\r\n\r\n编程\r\ncodegeex2\r\nChatglm2的基础上CodeGeeX2-6B 进一步经过了 600B 代码数据预训练\r\n\r\n\r\n编程\r\nstabelcode\r\n560B token多语言预训练+ 120,000 个 Alpaca指令对齐\r\n\r\n\r\n编程\r\nSQLCoder\r\n在StarCoder的基础上微调15B超越gpt3.5\r\n\r\n\r\n数学\r\nMathGPT\r\n是好未来自主研发的，面向全球数学爱好者和科研机构，以解题和讲题算法为核心的大模型。\r\n\r\n\r\n数学\r\nMammoTH\r\n通过COT+POT构建了MathInstruct数据集微调llama在OOD数据集上超越了WizardLM\r\n\r\n\r\n数学\r\nMetaMath\r\n模型逆向思维解决数学问题，构建了新的MetaMathQA微调llama2\r\n\r\n\r\n交通\r\nTransGPT\r\nLLama-7B+34.6万领域预训练+5.8万条领域指令对话微调（来自文档问答）\r\n\r\n\r\n交通\r\nTrafficGPT\r\nChatGPT+Prompt实现规划，调用交通流量领域专业TFM模型，TFM负责数据分析，任务执行，可视化等操作\r\n\r\n\r\n科技\r\nMozi\r\n红睡衣预训练+论文QA数据集 + ChatGPT扩充科研对话数据\r\n\r\n\r\n天文\r\nStarGLM\r\n天文知识指令微调，项目进行中后期考虑天文二次预训练+KG\r\n\r\n\r\n写作\r\n阅文-网文大模型介绍\r\n签约作者内测中，主打的内容为打斗场景，剧情切换，环境描写，人设，世界观等辅助片段的生成\r\n\r\n\r\n写作\r\nMediaGPT\r\nLLama-7B扩充词表+指令微调，指令来自国内媒体专家给出的在新闻创作上的80个子任务\r\n\r\n\r\n电商\r\nEcomGPT\r\n电商领域任务指令微调大模型，指令样本250万，基座模型是Bloomz\r\n\r\n\r\n\r\nTool and Library\r\n推理框架\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n工具描述\r\n链接\r\n\r\n\r\n\r\n\r\nFlexFlow：模型部署推理框架\r\nhttps://github.com/flexflow/FlexFlow\r\n\r\n\r\nMedusa：针对采样解码的推理加速框架，可以和其他策略结合\r\nhttps://github.com/FasterDecoding/Medusa\r\n\r\n\r\nFlexGen: LLM推理 CPU Offload计算架构\r\nhttps://github.com/FMInference/FlexGen\r\n\r\n\r\nVLLM：超高速推理框架Vicuna，Arena背后的无名英雄，比HF快24倍，支持很多基座模型\r\nhttps://github.com/vllm-project/vllm\r\n\r\n\r\nStreamingllm:\r\n新注意力池Attention方案，无需微调拓展模型推理长度，同时为推理提速\r\nhttps://github.com/mit-han-lab/streaming-llm\r\n\r\n\r\n\r\n指令微调，预训练，rlhf框架\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n工具描述\r\n链接\r\n\r\n\r\n\r\n\r\nLoRA：Low-Rank指令微调方案\r\nhttps://github.com/tloen/alpaca-lora\r\n\r\n\r\npeft：parameter-efficient prompt tunnging工具集\r\nhttps://github.com/huggingface/peft\r\n\r\n\r\nRL4LMs：AllenAI的RL工具\r\nhttps://github.com/allenai/RL4LMs\r\n\r\n\r\nRLLTE：港大，大疆等联合开源RLLTE开源学习框架\r\nhttps://github.com/RLE-Foundation/rllte\r\n\r\n\r\ntrl：基于Transformer的强化训练框架\r\nhttps://github.com/lvwerra/trl\r\n\r\n\r\ntrlx：分布式训练trl\r\nhttps://github.com/CarperAI/trlx\r\n\r\n\r\n北大开源河狸项目可复现RLHF，支持多数LLM，提供RLHF数据\r\nhttps://github.com/PKU-Alignment/safe-rlhf\r\n\r\n\r\nRL4LMs：AllenAI的RL工具\r\nhttps://github.com/allenai/RL4LMs\r\n\r\n\r\nLMFlow：港科大实验室开源的大模型微调框架，支持以上多数开源模型的指令微调和RLHF\r\nhttps://github.com/OptimalScale/LMFlow\r\n\r\n\r\nhugNLP:基于Huggingface开发继承Prompt技术，预训练和是指输入等多种方案\r\nhttps://github.com/wjn1996/HugNLP\r\n\r\n\r\nDeepspeed：针对RL训练和推理的整合优化\r\nhttps://github.com/microsoft/DeepSpeed\r\n\r\n\r\nUerpy:预训练框架支持lm,mlm,unilm等\r\nhttps://github.com/dbiir/UER-py\r\n\r\n\r\nTecentPretrain: Uerpy的重构版本支持llama预训练\r\nhttps://github.com/Tencent/TencentPretrain/tree/main\r\n\r\n\r\nlamini: 整合指令数据生成，SFT，RLHF的工具库\r\nhttps://github.com/lamini-ai/lamini/\r\n\r\n\r\nChain-of-thought-hub：模型推理能力评估平台\r\nhttps://github.com/FranxYao/chain-of-thought-hub\r\n\r\n\r\nEasyEdit：浙大开源支持多种模型，多种方案的模型知识精准编辑器\r\nhttps://github.com/zjunlp/EasyEdit\r\n\r\n\r\nOpenDelta：集成了各种增量微调方案的开源实现\r\nhttps://github.com/thunlp/OpenDelta\r\n\r\n\r\n\r\nAuto/Multi Agent\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n工具描述\r\n链接\r\n\r\n\r\n\r\n\r\nChatDev: 面壁智能开源多智能体协作的虚拟软件公司\r\nhttps://github.com/OpenBMB/ChatDev\r\n\r\n\r\nGenerative Agents:斯坦福AI小镇的开源代码\r\nhttps://github.com/joonspk-research/generative_agents\r\n\r\n\r\nBabyAGI：自执行LLM Agent\r\nhttps://github.com/yoheinakajima/babyagi\r\n\r\n\r\nAutoGPT：自执行LLM Agent\r\nhttps://github.com/Torantulino/Auto-GPT\r\n\r\n\r\nMetaGPT:\r\n覆盖软件公司全生命流程，例如产品经理等各个职业的AutoGPT\r\nhttps://github.com/geekan/MetaGPT\r\n\r\n\r\nResearchGPT: 论文写作领域的AutoGPT，融合论文拆解+网络爬虫\r\nhttps://github.com/assafelovic/gpt-researcher\r\n\r\n\r\nMiniAGI：自执行LLM Agent\r\nhttps://github.com/muellerberndt/mini-agi\r\n\r\n\r\nAL Legion： 自执行LLM Agent\r\nhttps://github.com/eumemic/ai-legion\r\n\r\n\r\nAgentVerse：多模型交互环境\r\nhttps://github.com/OpenBMB/AgentVerse\r\n\r\n\r\nAgentSims:\r\n给定一个社会环境，评估LLM作为智能体的预定任务目标完成能力的沙盒环境\r\nhttps://github.com/py499372727/AgentSims/\r\n\r\n\r\nGPTRPG：RPG环境 AI Agent游戏化\r\nhttps://github.com/dzoba/gptrpg\r\n\r\n\r\nGPTeam：多智能体交互\r\nhttps://github.com/101dotxyz/GPTeam\r\n\r\n\r\nGPTEngineer：自动工具构建和代码生成\r\nhttps://github.com/AntonOsika/gpt-engineer\r\n\r\n\r\nWorkGPT：类似AutoGPT\r\nhttps://github.com/team-openpm/workgpt\r\n\r\n\r\n\r\nAgent工具框架类\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n工具描述\r\n链接\r\n\r\n\r\n\r\n\r\nlangchain：LLM Agent框架\r\nhttps://github.com/hwchase17/langchain\r\n\r\n\r\nllama index：LLM Agent框架\r\nhttps://github.com/jerryjliu/llama_index\r\n\r\n\r\nRagas:\r\n评估检索增强LLM效果的框架，基于大模型prompt评估事实性，召回相关性，召回内容质量，回答相关性等\r\nhttps://github.com/explodinggradients/ragas#fire-quickstart\r\n\r\n\r\nlangflow：把langchain等agent组件做成了可拖拽式的UI\r\nhttps://github.com/logspace-ai/langflow\r\n\r\n\r\nHaystack: LLM Agent\r\n框架，pipeline的设计模式个人感觉比langchain更灵活更简洁\r\nhttps://github.com/deepset-ai/haystack\r\n\r\n\r\nEdgeChain: 通过Jsonnet配置文件实现LLM Agent\r\nhttps://github.com/arakoodev/EdgeChains/tree/main\r\n\r\n\r\nsemantic-kernel：整合大模型和编程语言的SDK\r\nhttps://github.com/microsoft/semantic-kernel\r\n\r\n\r\nBMTTools: 清华出品多工具调用开源库，提供微调数据和评估ToolBench\r\nhttps://github.com/OpenBMB/BMTools\r\n\r\n\r\nJarvis: 大模型调用小模型框架，给小模型一个未来！\r\nhttps://github.com/search?q=jarvis\r\n\r\n\r\nLLM-ToolMaker:让LLM自己制造Agent\r\nhttps://github.com/FMInference/FlexGen\r\n\r\n\r\nGorilla: LLM调用大量API\r\nhttps://github.com/ShishirPatil/gorilla\r\n\r\n\r\nwenda:闻达小模型整合搜索用于知识融入\r\nhttps://github.com/l15y/wenda\r\n\r\n\r\nAlexandria:\r\n从Arix论文开始把整个互联网变成向量索引，可以免费下载\r\nhttps://alex.macrocosm.so/download\r\n\r\n\r\nRapidAPI: 统一这个世界的所有API，最大API\r\nHub，有调用成功率，latency等，是真爱！\r\nhttps://rapidapi.com/hub\r\n\r\n\r\n\r\n其他垂直领域Agent\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n工具描述\r\n链接\r\n\r\n\r\n\r\n\r\nDeep-KE：基于LLM对数据进行智能解析实现知识抽取\r\nhttps://github.com/zjunlp/DeepKE\r\n\r\n\r\nIncarnaMind：多文档RAG方案，动态chunking的方案可以借鉴\r\nhttps://github.com/junruxiong/IncarnaMind\r\n\r\n\r\nVectra：平台化的LLM\r\nAgent搭建方案，从索引构建，内容召回排序，到事实检查的LLM生成\r\nhttps://vectara.com/tour-vectara/\r\n\r\n\r\nData-Copilot：时间序列等结构化数据分析领域的Agent解决方案\r\nhttps://github.com/zwq2018/Data-Copilot\r\n\r\n\r\nDB-GPT:\r\n以数据库为基础的GPT实验项目，使用本地化的GPT大模型与您的数据和环境进行交互\r\nhttps://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-cn/zh_CN/latest/index.html\r\n\r\n\r\nguardrails：降低模型幻觉的python框架，promp模板+validation+修正\r\nhttps://github.com/shreyar/guardrails\r\n\r\n\r\nguidance：微软新开源框架，同样是降低模型幻觉的框架，prompt+chain的升级版加入逐步生成和思维链路\r\nhttps://github.com/guidance-ai/guidance\r\n\r\n\r\nSolidGPT: 上传个人数据，通过命令交互创建项目PRD等\r\nhttps://github.com/AI-Citizen/SolidGPT\r\n\r\n\r\n\r\nTraining Data\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n数据类型\r\n数据描述\r\n数据链接\r\n\r\n\r\n\r\n\r\n指令微调\r\nself-instruct，GPT3自动生成&amp;过滤得到指令集\r\nhttps://github.com/yizhongw/self-instruct\r\n\r\n\r\n指令微调\r\nStandford Alpaca：52K\r\ntext-davinci-003生成的self-instruct指令数据集\r\nhttps://github.com/tatsu-lab/stanford_alpaca\r\n\r\n\r\n指令微调\r\nGPT4-for-LLM 中文+英文+对比指令\r\nhttps://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM\r\n\r\n\r\n指令微调\r\nGPTTeacher更多样的通用指令，角色扮演和代码指令\r\nhttps://github.com/teknium1/GPTeacher/tree/main\r\n\r\n\r\n指令微调\r\n中文翻译Alpaca还有一些其他指令数据集\r\nhttps://github.com/hikariming/alpaca_chinese_dataset\r\nhttps://github.com/carbonz0/alpaca-chinese-dataset\r\n\r\n\r\n指令微调\r\nalpaca指令GPT4生成，和以上几版对比显著质量更高，回复更长\r\nhttps://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/tree/main\r\n\r\n\r\n指令微调\r\nGuanaco数据：对Alphca指令重写后以不同语言生成总共534K，有对话和非对话类型，还有补充的QA生成样本\r\nhttps://huggingface.co/datasets/JosephusCheung/GuanacoDataset\r\n\r\n\r\n指令微调\r\nOIG中文指令包括翻译alpaca+natural+unnatural，多轮对话，考试，leetcode指令\r\nhttps://github.com/BAAI-Zlab/COIG\r\n\r\n\r\n指令微调\r\nVicuna训练使用的样本，用API获取了sharegpt上用户和chatgpt对话历史，部分网友整理到了HF\r\nhttps://github.com/domeccleston/sharegpt\r\nhttps://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main\r\n\r\n\r\n指令微调\r\nHC3指令数据中英文，包括金融，开放QA，百科，DBQA，医学等包含人工回复\r\nhttps://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main\r\n\r\n\r\n指令微调\r\nMOSS开源的SFT数据包含使用plugin的对话数据\r\nhttps://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main\r\n\r\n\r\n指令微调\r\nInstructWild数据：用四处爬取的chatgpt指令作为种子self-instruct扩充生成，中英双语\r\nhttps://github.com/XueFuzhao/InstructionWild/tree/main/data\r\n\r\n\r\n指令微调\r\nBELLE100万指令数据，参考Alpaca用ChatGPT生成，有数学，多轮对话，校色对话等等\r\nhttps://github.com/LianjiaTech/BELLE\r\n\r\n\r\n指令微调\r\nPromptCLUE多任务提示数据集：模板构建，只包含标准NLP任务\r\nhttps://github.com/CLUEbenchmark/pCLUE\r\n\r\n\r\n指令微调\r\nTK-Instruct微调用的指令数据集, 全人工标注1600+NLP任务\r\nhttps://instructions.apps.allenai.org/\r\n\r\n\r\n指令微调\r\nT0微调用的指令数据集（P3）\r\nhttps://huggingface.co/datasets/bigscience/P3\r\n\r\n\r\n指令微调\r\np3衍生的46种多语言数据集（xmtf）\r\nhttps://github.com/bigscience-workshop/xmtf\r\n\r\n\r\n指令微调\r\nUnnatural Instruction使用GPT3生成后改写得到240k\r\nhttps://github.com/orhonovich/unnatural-instructions\r\n\r\n\r\n指令微调\r\nalpaca COT对多个数据源进行了清理并统一格式放到的了HF,\r\n重点是人工整理的COT数据\r\nhttps://github.com/PhoebusSi/Alpaca-CoT\r\n\r\n\r\n指令微调\r\n人工编写包含23种常见的中文NLP任务的指令数据，中文写作方向\r\nhttps://github.com/yangjianxin1/Firefly\r\n\r\n\r\n指令微调\r\nAmazon COT指令样本包括各类QA，bigbench，math等\r\nhttps://github.com/amazon-science/auto-cot\r\n\r\n\r\n指令微调\r\nCSL包含 396,209 篇中文核心期刊论文元信息\r\n（标题、摘要、关键词、学科、门类）可做预训练可构建NLP指令任务\r\nhttps://github.com/ydli-ai/CSL\r\n\r\n\r\n指令微调\r\nalpaca code 20K代码指令数据\r\nhttps://github.com/sahil280114/codealpaca#data-release\r\n\r\n\r\n指令微调\r\nGPT4Tools 71K GPT4指令样本\r\nhttps://github.com/StevenGrove/GPT4Tools\r\n\r\n\r\n指令微调\r\nGPT4指令+角色扮演+代码指令\r\nhttps://github.com/teknium1/GPTeacher\r\n\r\n\r\n指令微调\r\nMol-Instructions 2043K\r\n分子+蛋白质+生物分子文本指令，覆盖分子设计、蛋白质功能预测、蛋白质设计等任务\r\nhttps://github.com/zjunlp/Mol-Instructions\r\n\r\n\r\n数学\r\n腾讯人工智能实验室发布网上爬取的数学问题APE210k\r\nhttps://github.com/Chenny0808/ape210k\r\n\r\n\r\n数学\r\n猿辅导 AI Lab开源小学应用题Math23K\r\nhttps://github.com/SCNU203/Math23k/tree/main\r\n\r\n\r\n数学\r\ngrade school\r\nmath把OpenAI的高中数学题有改造成指令样本有2-8步推理过程\r\nhttps://huggingface.co/datasets/qwedsacf/grade-school-math-instructions\r\n\r\n\r\n数学\r\n数学问答数据集有推理过程和多项选择\r\nhttps://huggingface.co/datasets/math_qa/viewer/default/test?row=2\r\n\r\n\r\n数学\r\nAMC竞赛数学题\r\nhttps://huggingface.co/datasets/competition_math\r\n\r\n\r\n数学\r\n线性代数等纯数学计算题\r\nhttps://huggingface.co/datasets/math_dataset\r\n\r\n\r\n代码\r\nAPPS从不同的开放访问编码网站Codeforces、Kattis 等收集的问题\r\nhttps://opendatalab.org.cn/APPS\r\n\r\n\r\n代码\r\nLyra代码由带有嵌入式 SQL 的 Python\r\n代码组成，经过仔细注释的数据库操作程序，配有中文评论和英文评论。\r\nhttps://opendatalab.org.cn/Lyra\r\n\r\n\r\n代码\r\nConala来自StackOverflow问题,手动注释3k，英文\r\nhttps://opendatalab.org.cn/CoNaLa/download\r\n\r\n\r\n代码\r\ncode-alpaca ChatGPT生成20K代码指令样本\r\nhttps://github.com/sahil280114/codealpaca.git\r\n\r\n\r\n代码\r\n32K,\r\n四种不同类型、不同难度的代码相关中文对话数据，有大模型生成，\r\nhttps://github.com/zxx000728/CodeGPT\r\n\r\n\r\n对话\r\nLAION 策划的开放指令通用数据集中手动选择的组件子集 已开源40M\r\n3万个,100M在路上\r\nhttps://github.com/LAION-AI/Open-Instruction-Generalist\r\n\r\n\r\n对话\r\nBaize基于Chat GPT构建的self-chat数据\r\nhttps://github.com/project-baize/baize-chatbot/tree/main/data\r\n\r\n\r\n对话\r\nFaceBook开源BlenderBot训练对话数据~6K\r\nhttps://huggingface.co/datasets/blended_skill_talk\r\n\r\n\r\n对话\r\nAllenAI开源38.5万个对话高质量数据集SODA\r\nhttps://realtoxicityprompts.apps.allenai.org/\r\n\r\n\r\n对话\r\nInstructDial在单一对话任务类型上进行指令微调\r\nhttps://github.com/prakharguptaz/Instructdial\r\n\r\n\r\n对话\r\nUltra Chat 两个独立的 ChatGPT Turbo API\r\n进行对话，从而生成多轮对话数据\r\nhttps://github.com/thunlp/UltraChat\r\n\r\n\r\n对话\r\nAwesome Open-domain Dialogue Models提供多个开放域对话数据\r\nhttps://github.com/cingtiye/Awesome-Open-domain-Dialogue-Models#%E4%B8%AD%E6%96%87%E5%BC%80%E6%94%BE%E5%9F%9F%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86\r\n\r\n\r\n对话\r\nSalesforce开源超全DialogStudio\r\nhttps://github.com/salesforce/DialogStudio\r\n\r\n\r\n对话\r\n基于事实Reference的多轮问答中文数据，已开源5万，之后会开源更多\r\nhttps://github.com/sufengniu/RefGPT\r\n\r\n\r\nRLFH\r\n北大河狸开源RLHF数据集10K，1M需要申请\r\nhttps://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K\r\n\r\n\r\nRLHF\r\nAnthropic hh-rlhf数据集\r\nhttps://huggingface.co/datasets/Anthropic/hh-rlhf\r\n\r\n\r\nRLHF\r\nStack-exchange上问题对应多个答案，每个答案有打分\r\nhttps://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences/tree/main\r\n\r\n\r\nRLHF\r\nFacebook Bot Adversarial Dialogues数据集5K\r\nhttps://github.com/facebookresearch/ParlAI\r\n\r\n\r\nRLHF\r\nAllenAI Real Toxicity prompts\r\nhttps://github.com/facebookresearch/ParlAI\r\n\r\n\r\nRLHF\r\nOpenAssistant Conversations 160K消息，13500人工生成, 英文为主\r\nhttps://huggingface.co/datasets/OpenAssistant/oasst1\r\n\r\n\r\nRLHF\r\n知乎问答偏好数据集\r\nhttps://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k\r\n\r\n\r\nRLHF\r\nhh-rlhf中文翻译偏好数据\r\nhttps://huggingface.co/datasets/liswei/rm-static-zhTW\r\n\r\n\r\nRLHF\r\n面壁智能开源大规模偏好数据，基于64Kprompt使用不同模型生成4个回答使用GPT-4评估\r\nhttps://github.com/OpenBMB/UltraFeedback\r\n\r\n\r\n评估集\r\nBigBench(Beyond the Imitation Game Benchmark)\r\nhttps://github.com/google/BIG-bench\r\n\r\n\r\n评估集\r\nComplex QA：用于ChatGPT的评测指令集\r\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT\r\n\r\n\r\n评估集\r\nLangchain开源评估数据集\r\nhttps://huggingface.co/LangChainDatasets\r\n\r\n\r\n评估集\r\n2010-2022年全国高考卷的题目\r\nhttps://github.com/OpenLMLab/GAOKAO-Bench\r\n\r\n\r\n评估集\r\n中文通用大模型综合性评测基准SuperCLUE\r\nhttps://github.com/CLUEbenchmark/SuperCLUE\r\n\r\n\r\n英文预训练\r\nRedPajama开源的复刻llama的预训练数据集，1.21万亿Token\r\nhttps://github.com/togethercomputer/RedPajama-Data\r\n\r\n\r\n英文预训练\r\nCerebras基于RedPajama进行清洗去重后得到的高质量数据集,\r\n6270亿Token\r\nhttps://huggingface.co/datasets/cerebras/SlimPajama-627B/tree/main/train\r\n\r\n\r\n英文预训练\r\nPile 22个高质量数据集混合的预训练数据集800G,全量开放下载\r\nhttps://pile.eleuther.ai/\r\n\r\n\r\n通用预训练\r\nUER整理CLUECorpusSmall+News Commentary中英\r\nhttps://github.com/dbiir/UER-py/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE\r\n\r\n\r\n中文预训练\r\n智源人工智能开源的wudao 200G预训练数据\r\nhttps://github.com/BAAI-WuDao/WuDaoMM\r\n\r\n\r\n中文预训练\r\n里屋社区发起开源力量收集中文互联网语料集MNBVC目标是对标ChatGPT的40T\r\nhttps://github.com/esbatmop/MNBVC\r\n\r\n\r\n中文预训练\r\n复旦开源15万中文图书下载和抽取方案\r\nhttps://github.com/FudanNLPLAB/CBook-150K\r\n\r\n\r\n中文预训练\r\n书生万卷数据集来自公开网页多模态数据集，包括文本，图文和视频，其中文本1T，图文150G\r\nhttps://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0\r\n\r\n\r\n领域预训练\r\n首个中文科学文献数据集CSL,也有多种NLP任务数据\r\nhttps://github.com/ydli-ai/CSL\r\n\r\n\r\n平行语料\r\nnews-commentary中英平行语料，用于中英间知识迁移\r\nhttps://data.statmt.org/news-commentary/v15/training/\r\n\r\n\r\n多源数据集整合\r\nopendatalab整合了预训练阶段的多个数据源\r\nhttps://opendatalab.org.cn/?industry=9821&amp;source=JUU3JTlGJUE1JUU0JUI5JThF\r\n\r\n\r\nTool-搜索增强\r\nwebCPM开源的和搜索工具进行交互问答的数据集，包括网页抽取式摘要，多事实内容回答等人工标注数据\r\nhttps://github.com/thunlp/WebCPM\r\n\r\n\r\nTool-多工具\r\nBmTools开源的多工具调用指令数据集\r\nhttps://github.com/OpenBMB/BMTools\r\n\r\n\r\nNL2SQL\r\nDB-GPT-Hub梳理了多源text-to-sql数据集\r\nhttps://github.com/eosphoros-ai/DB-GPT-Hub\r\n\r\n\r\n\r\nAIGC\r\n\r\nNexusGPT: :\r\nAutoGPT可以出来工作了，第一个全AI Freelance平台\r\ncognosys:\r\n全网最火的web端AutoGPT，不过咋说呢试用了下感觉下巴要笑掉了，不剧透去试试你就知道\r\n\r\ngodmode：可以进行人为每一步交互的的AutoGPT\r\nagentgpt:\r\n基础版AutoGPT\r\n⭐️\r\ndo Anything:\r\nAutoGPT Like的to Do List生成器 \r\nChatMind:\r\nchatgpt生成思维导图，模板很丰富，泛化性也不错，已经被XMind收购了~  ⭐️\r\nNew\r\nBing：需要连外网否则会重定向到bing中国，需要申请waitlist  ⭐️\r\nPerplexity.ai:\r\n同样需要科学上网，感觉比Bing做的更好的接入ChatGPT的神奇搜索引擎，在Bing之外还加入了相关推荐和追问\r\n ⭐️\r\nBingGPT:\r\nNewBing开源桌面客户端，可以将聊天记录导出 \r\nAutoLabel:\r\nAutoLabel标注方案 \r\nMem:\r\n笔记类产品，可以构建个人知识AI管家例如知识图谱，已获openai融资 \r\nDocsGPT:\r\n把ChatGPT开放域问答转化成封闭域问答的通用方案，试用垂类领域问答场景,可以试用定制的ChatBot\r\n ⭐️\r\nlangchain-ChatGLM:\r\n基于ChatGLM的本地知识问答，和上面的DocsGPT相似，不过可以本地部署⭐️\r\nChatPDF: 国内的ChatPDF,\r\n上传pdf后，会给出文章的Top5可能问题，然后对话式从文档中进行问答和检索，10s读3万字\r\n\r\nChatDoc:ChatPDF升级版，增加了表格类解析，和完善的索引引用加跳转加对应文章内容高亮，哈哈我准备自己整一个\r\n\r\nChatPaper:\r\n根据输入关键词，自动在arxiv上下载最新的论文，并对论文进行摘要总结，可以在huggingface上试用！\r\n\r\nOpenRead:\r\n面向论文写作，阅读场景，可以帮助生成文献综述，以及提供和NotionAI相似的智能Markdown用于写作\r\n\r\nresearchgpt:\r\n和ChatPDF类似，支持arivx论文下载，加载后对话式获取论文重点 \r\nBriefGPT:\r\n日更Arxiv论文，并对论文进行摘要，关键词抽取，帮助研究者了解最新动态,\r\nUI不错哟 \r\nChatGPT-academic:\r\n又是一个基于gradio实现的paper润色，摘要等功能打包的实现 \r\nfeishu-chatgpt:\r\n飞书chatgpt，和365copilot相似也是多组件集成, 有点全！ \r\nAI Topiah:\r\n聆心智能AI角色聊天，和路飞唠了两句，多少有点中二之魂在燃烧 \r\nchatbase:\r\n情感角色聊天，还没尝试 \r\nVana: virtual DNA,\r\n通过聊天创建虚拟自己！概念很炫 \r\nWriteSonic：AI写作，支持对话和定向创作如广告文案，商品描述,\r\n支持Web检索是亮点，支持中文 \r\ncopy.ai:\r\nWriteSonic竞品，亮点是像论文引用一样每句话都有对应网站链接，可以一键复制到右边的创作Markdown，超级好用！\r\n\r\n⭐️\r\nNotionAI：智能Markdown，适用真相！在创作中用command调用AI辅助润色，扩写，检索内容，给创意idea\r\n\r\nJasper: 同上，全是竞品哈哈 \r\ncopy.down:\r\n中文的营销文案生成，只能定向创作，支持关键词到文案的生成 \r\nChatExcel:\r\n指令控制excel计算，对熟悉excel的有些鸡肋，对不熟悉的有点用 \r\nChatPPT:\r\n使用ChatGPT进行PPT制作 \r\nBibiGPT:\r\nBilibli视频内容一键总结，多模态文档 \r\nCopilot: 要付费哟\r\n\r\nFauxpilot:\r\ncopilot本地开源替代 \r\nCodeGex: 国内替代品，还没试过\r\n\r\nCodeium:\r\nCopilot替代品，有免费版本支持各种plugin \r\nsql translate:\r\ntext2sql，利用 OpenAI 的 API\r\n实现的一个很简单的工具，sql到文字，文字到sql \r\nai2sql:\r\ntext2sql老牌公司，相比sqltranslate功能更全面，支持SQL\r\n语法检查、格式化和生成公式 \r\nchat2query:\r\ntext2sql\r\n相比以上两位支持更自然的文本指令，以及更复杂的数据分析类的sql生成  ⭐️\r\nOuterBase: text2sql\r\n设计风格很吸睛！电子表格结合mysql和dashboard，更适合数据分析宝宝 \r\nWolverine:\r\n代码自我debug的python脚本 \r\ndreamstudio.ai:\r\n开创者，Stable Difussion， 有试用quota \r\nmidjourney:\r\n开创者，艺术风格为主 \r\nDall.E:\r\n三巨头这就凑齐了 \r\nControlNet:\r\n为绘画创作加持可控性 \r\nGFPGAN:\r\n照片修复 \r\nVisual\r\nChatGPT: 微软发布图像ChatGPT，对话方式进行图像生成编辑，问答  ⭐️\r\ngemo.ai:\r\n多模态聊天机器人，包括文本，图像，视频生成\r\nstorybird:\r\n根据提示词生成故事绘本，还可以售卖\r\n\r\nResources\r\n教程类\r\n\r\nOpenAI\r\nCookbook: 提供OpenAI模型使用示例 ⭐️\r\nOpenAI\r\n接口被墙解决办法:\r\n使用腾讯云搭建代理，亲测非常好用且手残党也可以轻松上手\r\nPromptPerfect:用魔法打败魔法，输入原始提示词，模型进行定向优化，试用后我有点沉默了，可以定向支持不同使用prompt的模型如Difussion，ChatGPT，\r\nDalle等\r\nClickPrompt:\r\n为各种prompt加持的工具生成指令包括Difussion，chatgptdeng, 需要OpenAI\r\nKey\r\nChatGPT\r\nShortCut：提供各式场景下的Prompt范例，范例很全，使用后可以点赞！\r\n⭐️\r\nFull\r\nChatGPT Prompts + Resources:\r\n各种尝尽的prompt范例，和以上场景有所不同\r\nlearning Prompt: prompt\r\nengineering超全教程，和落地应用收藏，包括很多LLM调用Agent的高级场景\r\n⭐️\r\nThe\r\nart of asking chatgpt for high quality answers:\r\n如何写Prompt指令出书了，链接是中文翻译的版本，比较偏基础使用\r\nPrompt-Engineer-Guide:\r\n同learnig prompt类的集成教程，互相引用可还行？！分类索引做的更好些\r\n⭐️\r\nOpenAI\r\n应用汇总指南: 纯应用类的汇总指南\r\nAI 导航:\r\n包括但不限于ChatGPT的应用汇总网站，更新很快，发现了一些新大陆\r\nAI Alignment Forum:\r\nRLHF等对齐相关最新论文和观点的讨论论坛\r\nLangchain:\r\nChat with your data:吴恩达LLM实践课程\r\n构筑大语言模型应用：应用开发与架构设计:\r\n一本关于 LLM 在真实世界应用的开源电子书\r\nLarge\r\nLanguage Models: Application through Production:\r\n大模型应用Edx出品的课程\r\n\r\n书籍博客类\r\n\r\nOpenAI ChatGPT\r\nIntro\r\nOpenAI\r\nInstructGPT intro\r\nAllenAI ChatGPT能力解读：How\r\ndoes GPT Obtain its Ability? Tracing Emergent Abilities of Language\r\nModels to their Sources ⭐️\r\nHuggingface ChatGPT能力解读：The techniques behind\r\nChatGPT: RLHF, IFT, CoT, Red teaming, and more\r\nStephen Wolfram ChatGPT能力解读: What\r\nIs ChatGPT Doing and Why Does It Work?\r\nChatgpt相关解读汇总\r\n麻省理工科技采访OpenAI工程师\r\nAGI历史与现状\r\n张俊林\r\n通向AGI之路：大型语言模型（LLM）技术精要\r\n知乎回答\r\nOpenAI 发布 GPT-4，有哪些技术上的优化或突破?\r\n追赶ChatGPT的难点与平替\r\n压缩即泛化，泛化即智能\r\n⭐️\r\n陆奇最新演讲实录：我的大模型世界观｜第十四期\r\nLLM\r\nPowered Autonomous Agents ⭐️\r\nAll\r\nYou Need to Know to Build Your First LLM App ⭐️\r\nGPT-4\r\nArchitecture, Infrastructure, Training Dataset, Costs, Vision,\r\nMoE\r\n为什么伟大不能被计划:\r\nOpenAI研究员出书\r\n拾象投研机构对LLM的调研报告（文中有两次PPT的申请链接）:\r\n对大模型应用给出了很全面的总结梳理\r\n启明创投State of\r\nGenerative AI 2023:\r\n最近发现应用落地才是LLM真正产生价值的核心，开始更多关注一些投研的分析报告\r\nHow\r\nto Use AI to Do Stuff: An Opinionated Guide\r\nLlama 2:\r\nan incredible open LLM\r\nWolfram语言之父新书：这就是ChatGPT\r\n谷歌出品：对大模型领悟能力的一些探索很有意思\r\nDo Machine Learning Models Memorize or Generalize? ⭐️\r\nOpenAI首席科学家最新讲座解读LM无监督预训练学了啥\r\nAn observation on Generalization ⭐️\r\nThe\r\nComplete Beginners Guide To Autonomous Agents: Octane AI创始人 Matt\r\nSchlicht发表的关于人工智能代理的一些思考\r\nAn\r\nInitial Exploration of Theoretical Support for Language Model Data\r\nEngineering. Part 1: Pretraining:\r\n符尧大佬系列新作，通过了解大模型背后的数据工程来了解模型本质，第一篇预训练数据\r\nLarge\r\nLanguage Models (in 2023) OpenAI科学家最新大模型演讲\r\n\r\nPapers\r\npaper List\r\n\r\nhttps://github.com/dongguanting/In-Context-Learning_PaperList\r\nhttps://github.com/thunlp/PromptPapers\r\nhttps://github.com/Timothyxxx/Chain-of-ThoughtsPapers\r\nhttps://github.com/thunlp/ToolLearningPapers\r\nhttps://github.com/MLGroupJLU/LLM-eval-survey\r\n\r\n综述\r\n\r\nA Survey of Large Language Models\r\nPre-train, Prompt, and Predict: A Systematic Survey of Prompting\r\nMethods in Natural Language Processing ⭐️\r\nParadigm Shift in Natural Language Processing\r\nPre-Trained Models: Past, Present and Future\r\nWhat Language Model Architecture and Pretraining objects work best\r\nfor zero shot generalization ⭐️\r\nTowards Reasoning in Large Language Models: A Survey\r\nReasoning with Language Model Prompting: A Survey ⭐️\r\nAn Overview on Language Models: Recent Developments and Outlook\r\n⭐️\r\nA Survey of Large Language Models[6.29更新版]\r\nUnifying Large Language Models and Knowledge Graphs: A Roadmap\r\nAugmented Language Models: a Survey ⭐️\r\nDomain Specialization as the Key to Make Large Language Models\r\nDisruptive: A Comprehensive Survey\r\nChallenges and Applications of Large Language Models\r\nThe Rise and Potential of Large Language Model Based Agents: A\r\nSurvey\r\n\r\n大模型能力探究\r\n\r\nIn Context Learning\r\n\r\nLARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY\r\nHow does in-context learning work? A framework for understanding the\r\ndifferences from traditional supervised learning\r\nWhy can GPT learn in-context? Language Model Secretly Perform\r\nGradient Descent as Meta-Optimizers ⭐️\r\nRethinking the Role of Demonstrations What Makes incontext learning\r\nwork? ⭐️\r\nTrained Transformers Learn Linear Models In-Context\r\n\r\n涌现能力\r\n\r\nSparks of Artificial General Intelligence: Early experiments with\r\nGPT-4\r\nEmerging Ability of Large Language Models ⭐️\r\nLANGUAGE MODELS REPRESENT SPACE AND TIME\r\n\r\n能力评估\r\n\r\nIS CHATGPT A GENERAL-PURPOSE NATURAL LANGUAGE PROCESSING TASK\r\nSOLVER?\r\nCan Large Language Models Infer Causation from Correlation?\r\nHolistic Evaluation of Language Model\r\nHarnessing the Power of LLMs in Practice: A Survey on ChatGPT and\r\nBeyond\r\nTheory of Mind May Have Spontaneously Emerged in Large Language\r\nModels\r\nBeyond The Imitation Game: Quantifying And Extrapolating The\r\nCapabilities Of Language Models\r\nDo Models Explain Themselves? Counterfactual Simulatability of\r\nNatural Language Explanations\r\nDemystifying GPT Self-Repair for Code Generation\r\nEvidence of Meaning in Language Models Trained on Programs\r\nCan Explanations Be Useful for Calibrating Black Box Models\r\nOn the Robustness of ChatGPT: An Adversarial and Out-of-distribution\r\nPerspective\r\nLanguage acquisition: do children and language models follow similar\r\nlearning stages?\r\n\r\n\r\nPrompt Tunning范式\r\n\r\nTunning Free Prompt\r\n\r\nGPT2: Language Models are Unsupervised Multitask Learners\r\nGPT3: Language Models are Few-Shot Learners ⭐️\r\nLAMA: Language Models as Knowledge Bases?\r\nAutoPrompt: Eliciting Knowledge from Language Models\r\n\r\nFix-Prompt LM Tunning\r\n\r\nT5: Exploring the Limits of Transfer Learning with a Unified\r\nText-to-Text Transformer\r\nPET-TC(a): Exploiting Cloze Questions for Few Shot Text\r\nClassification and Natural Language Inference ⭐️\r\nPET-TC(b): PETSGLUE It’s Not Just Size That Matters Small Language\r\nModels are also few-shot learners\r\nGenPET: Few-Shot Text Generation with Natural Language\r\nInstructions\r\nLM-BFF: Making Pre-trained Language Models Better Few-shot Learners\r\n⭐️\r\nADEPT: Improving and Simplifying Pattern Exploiting Training\r\n\r\nFix-LM Prompt Tunning\r\n\r\nPrefix-tuning: Optimizing continuous prompts for generation\r\nPrompt-tunning: The power of scale for parameter-efficient prompt\r\ntuning ⭐️\r\nP-tunning: GPT Understands Too ⭐️\r\nWARP: Word-level Adversarial ReProgramming\r\n\r\nLM + Prompt Tunning\r\n\r\nP-tunning v2: Prompt Tuning Can Be Comparable to Fine-tunning\r\nUniversally Across Scales and Tasks\r\nPTR: Prompt Tuning with Rules for Text Classification\r\nPADA: Example-based Prompt Learning for on-the-fly Adaptation to\r\nUnseen Domains\r\n\r\nFix-LM Adapter Tunning\r\n\r\nLORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS ⭐️\r\nLST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer\r\nLearning\r\nParameter-Efficient Transfer Learning for NLP\r\nINTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE\r\nMODEL FINE-TUNING\r\n\r\n\r\n主流LLMS\r\n\r\nGLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL\r\nLLaMA: Open and Efficient Foundation Language Models\r\nPaLM: Scaling Language Modeling with Pathways\r\nPaLM 2 Technical Report\r\nGPT-4 Technical Report\r\nBackpack Language Models\r\nLlama 2: Open Foundation and Fine-Tuned Chat Models\r\nOpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model\r\nPre-trained from Scratch\r\nSheared LLaMA: Accelerating Language Model Pre-training via\r\nStructured Pruning\r\nMistral 7B\r\n\r\n指令微调&amp;对齐\r\n(instruction_tunning)\r\n\r\n经典方案\r\n\r\nFlan: FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS ⭐️\r\nFlan-T5: Scaling Instruction-Finetuned Language Models\r\nExT5: Towards Extreme Multi-Task Scaling for Transfer Learning\r\nInstruct-GPT: Training language models to follow instructions with\r\nhuman feedback ⭐️\r\nT0: MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK\r\nGENERALIZATION\r\nNatural Instructions: Cross-Task Generalization via Natural Language\r\nCrowdsourcing Instructions\r\nTk-INSTRUCT: SUPER-NATURALINSTRUCTIONS: Generalization via\r\nDeclarative Instructions on 1600+ NLP Tasks\r\nZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves\r\nZero-shot Generalization\r\nUnnatural Instructions: Tuning Language Models with (Almost) No\r\nHuman Labor\r\nINSTRUCTEVAL Towards Holistic Evaluation of Instrucion-Tuned Large\r\nLanguage Models\r\n\r\n更少，质量更高、更多样的指令数据带来质变\r\n\r\nLIMA: Less Is More for Alignment ⭐️\r\nMaybe Only 0.5% Data is Needed: A Preliminary Exploration of Low\r\nTraining Data Instruction Tuning\r\nTextbooks Are All You Need ⭐️\r\nAlpaGasus: Training A Better Alpaca with Fewer Data\r\nInstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning\r\nMiniGPT-4\r\nInstruction Mining: High-Quality Instruction Data Selection for\r\nLarge Language Models\r\nVisual Instruction Tuning with Polite Flamingo\r\n\r\n新对齐/微调方案\r\n\r\nWizardLM: Empowering Large Language Models to Follow Complex\r\nInstructions\r\nBecoming self-instruct: introducing early stopping criteria for\r\nminimal instruct tuning\r\nSelf-Alignment with Instruction Backtranslation ⭐️\r\nMixture-of-Experts Meets Instruction Tuning:A Winning Combination\r\nfor Large Language Models\r\nGoat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks\r\nPROMPT2MODEL: Generating Deployable Models from Natural Language\r\nInstructions\r\nOpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs\r\nPrinciple-Driven Self-Alignment of Language Models from Scratch with\r\nMinimal Human Supervision\r\nImproving Language Model Negotiation with Self-Play and In-Context\r\nLearning from AI Feedback\r\n\r\n微调经验/实验报告\r\n\r\nBELLE: Exploring the Impact of Instruction Data Scaling on Large\r\nLanguage Models: An Empirical Study on Real-World Use Cases\r\nBaize: Baize: An Open-Source Chat Model with Parameter-Efficient\r\nTuning on Self-Chat Data\r\nA Comparative Study between Full-Parameter and LoRA-based\r\nFine-Tuning on Chinese Instruction Data for Large LM\r\nExploring ChatGPT’s Ability to Rank Content: A Preliminary Study on\r\nConsistency with Human Preferences\r\nTowards Better Instruction Following Language Models for Chinese:\r\nInvestigating the Impact of Training Data and Evaluation\r\n\r\n\r\n对话模型\r\n\r\nLaMDA: Language Models for Dialog Applications\r\nSparrow: Improving alignment of dialogue agents via targeted human\r\njudgements ⭐️\r\nBlenderBot 3: a deployed conversational agent that continually\r\nlearns to responsibly engage\r\nHow NOT To Evaluate Your Dialogue System: An Empirical Study of\r\nUnsupervised Evaluation Metrics for Dialogue Response Generation\r\nDialogStudio: Towards Richest and Most Diverse Unified Dataset\r\nCollection for Conversational AI\r\nEnhancing Chat Language Models by Scaling High-quality Instructional\r\nConversations\r\nDiagGPT: An LLM-based Chatbot with Automatic Topic Management for\r\nTask-Oriented Dialogue\r\n\r\n思维链\r\n(prompt_chain_of_thought)\r\n\r\n基础&amp;进阶用法\r\n\r\n[zero-shot-COT] Large Language Models are Zero-Shot Reasoners\r\n⭐️\r\n[few-shot COT] Chain of Thought Prompting Elicits Reasoning in Large\r\nLanguage Models ⭐️\r\nSELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE\r\nMODELS\r\nLEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE\r\nMODELS ⭐️\r\nTree of Thoughts: Deliberate Problem Solving with Large Language\r\nModels\r\nPlan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought\r\nReasoning by Large Language Models\r\nDecomposed Prompting A MODULAR APPROACH FOR Solving Complex\r\nTasks\r\nSuccessive Prompting for Decomposing Complex Questions\r\nVerify-and-Edit: A Knowledge-Enhanced Chain-of-Thought\r\nFramework\r\nBeyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\r\nLarge Language Models\r\nTree-of-Mixed-Thought: Combining Fast and Slow Thinking for\r\nMulti-hop Visual Reasoning\r\nLAMBADA: Backward Chaining for Automated Reasoning in Natural\r\nLanguage\r\nAlgorithm of Thoughts: Enhancing Exploration of Ideas in Large\r\nLanguage Models\r\nGraph of Thoughts: Solving Elaborate Problems with Large Language\r\nModels\r\n\r\n分领域COT [Math, Code, Tabular, QA]\r\n\r\nSolving Quantitative Reasoning Problems with Language Models\r\nSHOW YOUR WORK: SCRATCHPADS FOR INTERMEDIATE COMPUTATION WITH\r\nLANGUAGE MODELS\r\nSolving math word problems with processand outcome-based\r\nfeedback\r\nCodeRL: Mastering Code Generation through Pretrained Models and Deep\r\nReinforcement Learning\r\nT-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large\r\nLanguage Model Signals for Science Question Answering\r\nLEARNING PERFORMANCE-IMPROVING CODE EDITS\r\nLarge Language Models are Versatile Decomposers: Decompose Evidence\r\nand Questions for Table-based Reasoning\r\nTab-CoT: Zero-shot Tabular Chain of Thought\r\n\r\n原理分析\r\n\r\nTowards Understanding Chain-of-Thought Prompting: An Empirical Study\r\nof What Matters ⭐️\r\nTEXT AND PATTERNS: FOR EFFECTIVE CHAIN OF THOUGHT IT TAKES TWO TO\r\nTANGO\r\nTowards Revealing the Mystery behind Chain of Thought: a Theoretical\r\nPerspective\r\nLarge Language Models Can Be Easily Distracted by Irrelevant\r\nContext\r\n\r\n小模型COT蒸馏\r\n\r\nSpecializing Smaller Language Models towards Multi-Step Reasoning\r\n⭐️\r\nTeaching Small Language Models to Reason\r\nLarge Language Models are Reasoning Teachers\r\nDistilling Reasoning Capabilities into Smaller Language Models\r\nThe CoT Collection: Improving Zero-shot and Few-shot Learning of\r\nLanguage Models via Chain-of-Thought Fine-Tuning\r\n\r\nCOT样本自动构建/选择\r\n\r\nSTaR: Self-Taught Reasoner Bootstrapping ReasoningWith\r\nReasoning\r\nAutoCOT：AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE\r\nMODELS\r\nLarge Language Models Can Self-Improve\r\nActive Prompting with Chain-of-Thought for Large Language\r\nModels\r\nCOMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING\r\n\r\nothers\r\n\r\nOlaGPT Empowering LLMs With Human-like Problem-Solving\r\nabilities\r\nChallenging BIG-Bench tasks and whether chain-of-thought can solve\r\nthem\r\nLarge Language Models are Better Reasoners with\r\nSelf-Verification\r\nThoughtSource A central hub for large language model reasoning\r\ndata\r\nTwo Failures of Self-Consistency in the Multi-Step Reasoning of\r\nLLMs\r\n\r\n\r\nRLHF\r\n\r\nDeepmind\r\n\r\nTeaching language models to support answers with verified\r\nquotes\r\nsparrow, Improving alignment of dialogue agents via targetd human\r\njudgements ⭐️\r\n\r\nopenai\r\n\r\nPPO: Proximal Policy Optimization Algorithms ⭐️\r\nDeep Reinforcement Learning for Human Preference\r\nFine-Tuning Language Models from Human Preferences\r\nlearning to summarize from human feedback\r\nInstructGPT: Training language models to follow instructions with\r\nhuman feedback ⭐️\r\nScaling Laws for Reward Model Over optimization ⭐️\r\n\r\nAnthropic\r\n\r\nA General Language Assistant as a Laboratory for Alignmen\r\nRed Teaming Language Models to Reduce Harms Methods,Scaling\r\nBehaviors and Lessons Learned\r\nTraining a Helpful and Harmless Assistant with Reinforcement\r\nLearning from Human Feedback ⭐️\r\nConstitutional AI Harmlessness from AI Feedback ⭐️\r\nPretraining Language Models with Human Preferences\r\nThe Capacity for Moral Self-Correction in Large Language Models\r\n\r\nAllenAI, RL4LM：IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE\r\nPROCESSING BENCHMARKS\r\n改良方案\r\n\r\nRRHF: Rank Responses to Align Language Models with Human Feedback\r\nwithout tears\r\nPRM：Let's verify step by step\r\nChain of Hindsight Aligns Language Models with Feedback\r\nAlpacaFarm: A Simulation Framework for Methods that Learn from Human\r\nFeedback\r\nOpen Problems and Fundamental Limitations of Reinforcement Learning\r\nfrom Human Feedback\r\nRAFT: Reward rAnked FineTuning for Generative Foundation Model\r\nAlignment\r\n\r\nTraining Socially Aligned Language Models in Simulated Human\r\nSociety\r\n\r\nLLM Agent 让模型使用工具\r\n(llm_agent)\r\n\r\n基于prompt通用方案\r\n\r\nReAct: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS ⭐️\r\nSelf-ask: MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN\r\nLANGUAGE MODELS ⭐️\r\nMRKL SystemsA modular, neuro-symbolic architecture that combines\r\nlarge language models, external knowledge sources and discrete\r\nreasoning\r\nPAL: Program-aided Language Models\r\nART: Automatic multi-step reasoning and tool-use for large language\r\nmodels\r\nReWOO: Decoupling Reasoning from Observations for Efficient\r\nAugmented Language Models ⭐️\r\nInterleaving Retrieval with Chain-of-Thought Reasoning for\r\nKnowledge-Intensive Multi-Step Questions\r\nChameleon: Plug-and-Play Compositional Reasoning with Large Language\r\nModels ⭐️\r\nFaithful Chain-of-Thought Reasoning\r\nReflexion: Language Agents with Verbal Reinforcement Learning\r\n⭐️\r\nSearch-in-the-Chain: Towards Accurate, Credible and Traceable Large\r\nLanguage Models for Knowledge-intensive Tasks\r\nVerify-and-Edit: A Knowledge-Enhanced Chain-of-Thought\r\nFramework\r\nRestGPT: Connecting Large Language Models with Real-World RESTful\r\nAPIs\r\nChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based\r\nLarge Language Models\r\nInstructTODS: Large Language Models for End-to-End Task-Oriented\r\nDialogue Systems\r\n\r\n基于微调通用方案\r\n\r\nTALM: Tool Augmented Language Models\r\nToolformer: Language Models Can Teach Themselves to Use Tools\r\n⭐️\r\nTool Learning with Foundation Models\r\nTool Maker：Large Language Models as Tool Maker\r\nTaskMatrix.AI: Completing Tasks by Connecting Foundation Models with\r\nMillions of APIs\r\n\r\n检索增强方案\r\n\r\nWebGPT：Browser-assisted question-answering with human feedback\r\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System\r\nwith Human Preferences\r\nWebCPM: Interactive Web Search for Chinese Long-form Question\r\nAnswering ⭐️\r\nREPLUG: Retrieval-Augmented Black-Box Language Models\r\nQuery Rewriting for Retrieval-Augmented Large Language Models\r\nRETA-LLM: A Retrieval-Augmented Large Language Model Toolkit\r\nAtlas: Few-shot Learning with Retrieval Augmented Language\r\nModels\r\nRRAML: Reinforced Retrieval Augmented Machine Learning\r\nInvestigating the Factual Knowledge Boundary of Large Language\r\nModels with Retrieval Augmentation\r\nPDFTriage: Question Answering over Long, Structured Documents\r\n\r\n调用模型方案\r\n\r\nHuggingGPT: Solving AI Tasks with ChatGPT and its Friends in\r\nHuggingFace\r\nGorilla：Large Language Model Connected with Massive APIs ⭐️\r\nOpenAGI: When LLM Meets Domain Experts\r\n\r\n垂直领域\r\n\r\nWebShop: Towards Scalable Real-World Web Interaction with Grounded\r\nLanguage Agents\r\nToolkenGPT: Augmenting Frozen Language Models with Massive Tools via\r\nTool Embeddings\r\nChemCrow Augmenting large language models with chemistry tools\r\nData-Copilot: Bridging Billions of Data and Humans with Autonomous\r\nWorkflow\r\nGeneGPT: Augmenting Large Language Models with Domain Tools for\r\nImproved Access to Biomedical Information\r\nPointLLM: Empowering Large Language Models to Understand Point\r\nClouds\r\nInterpretable Long-Form Legal Question Answering with\r\nRetrieval-Augmented Large Language Models\r\nGenerating Explanations in Medical Question-Answering by Expectation\r\nMaximization Inference over Evidence\r\nCarExpert: Leveraging Large Language Models for In-Car\r\nConversational Question Answering\r\n\r\n评估\r\n\r\nEvaluating Verifiability in Generative Search Engines\r\nMind2Web: Towards a Generalist Agent for the Web\r\nAuto-GPT for Online Decision Making: Benchmarks and Additional\r\nOpinions\r\nAPI-Bank: A Benchmark for Tool-Augmented LLMs\r\nToolLLM: Facilitating Large Language Models to Master 16000+\r\nReal-world APIs\r\n\r\nMultiAgent\r\n\r\nGenerative Agents: Interactive Simulacra of Human Behavior\r\nAgentVerse: Facilitating Multi-Agent Collaboration and Exploring\r\nEmergent Behaviors in Agents\r\nCAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale\r\nLanguage Model Society\r\nExploring Large Language Models for Communication Games: An\r\nEmpirical Study on Werewolf\r\nCommunicative Agents for Software Development\r\n\r\n其他\r\n\r\nLLM+P: Empowering Large Language Models with Optimal Planning\r\nProficiency\r\nInference with Reference: Lossless Acceleration of Large Language\r\nModels\r\nRecallM: An Architecture for Temporal Context Understanding and\r\nQuestion Answering\r\n\r\n\r\n指令数据生成\r\n(instruction_data_gen)\r\n\r\nAPE: LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS ⭐️\r\nSELF-INSTRUCT: Aligning Language Model with Self Generated\r\nInstructions ⭐️\r\niPrompt: Explaining Data Patterns in Natural Language via\r\nInterpretable Autoprompting\r\nFlipped Learning: Guess the Instruction! Flipped Learning Makes\r\nLanguage Models Stronger Zero-Shot Learners\r\nFairness-guided Few-shot Prompting for Large Language Models\r\nInstruction induction: From few examples to natural language task\r\ndescriptions.\r\nBaize An Open-Source Chat Model with Parameter-Efficient Tuning on\r\nself-Chat Data\r\nSELF-QA Unsupervised Knowledge Guided alignment.\r\nGPT Self-Supervision for a Better Data Annotator\r\nThe Flan Collection Designing Data and Methods\r\nSelf-Consuming Generative Models Go MAD\r\nInstructEval: Systematic Evaluation of Instruction Selection\r\nMethods\r\nOverwriting Pretrained Bias with Finetuning Data\r\nWizardLM: Empowering Large Language Models to Follow Complex\r\nInstructions\r\n\r\n预训练数据(pretrain_data)\r\n\r\nDoReMi: Optimizing Data Mixtures Speeds Up Language Model\r\nPretraining\r\nThe Pile: An 800GB Dataset of Diverse Text for Language\r\nModeling\r\nCCNet: Extracting High Quality Monolingual Datasets fromWeb Crawl\r\nData\r\nWanJuan: A Comprehensive Multimodal Dataset for Advancing English\r\nand Chinese Large Models\r\nCLUECorpus2020: A Large-scale Chinese Corpus for Pre-training\r\nLanguage Model\r\n\r\n领域模型 (domain_llms)\r\n\r\nMedGPT: Medical Concept Prediction from Clinical Narratives\r\nBioGPT：Generative Pre-trained Transformer for Biomedical Text\r\nGeneration and Mining\r\nGalactia：A Large Language Model for Science\r\nPubMed GPT: A Domain-specific large language model for biomedical\r\ntext ⭐️\r\nBloombergGPT： A Large Language Model for Finance\r\nChatDoctor：Medical Chat Model Fine-tuned on LLaMA Model using\r\nMedical Domain Knowledge\r\nMed-PaLM：Large Language Models Encode Clinical Knowledge[V1,V2]\r\n⭐️\r\nAugmented Large Language Models with Parametric Knowledge\r\nGuiding\r\nXuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of\r\nBillions Parameters\r\nChatLaw Open-Source Legal Large Language Model ⭐️\r\nMediaGPT : A Large Language Model For Chinese Media\r\nSMILE: Single-turn to Multi-turn Inclusive Language Expansion via\r\nChatGPT for Mental Health Support\r\nKITLM: Domain-Specific Knowledge InTegration into Language Models\r\nfor Question Answering\r\nFinVis-GPT: A Multimodal Large Language Model for Financial Chart\r\nAnalysis\r\nEcomGPT: Instruction-tuning Large Language Models with Chain-of-Task\r\nTasks for E-commerce\r\nFinGPT: Open-Source Financial Large Language Models\r\nTableGPT: Towards Unifying Tables, Nature Language and Commands into\r\nOne GPT\r\nCFGPT: Chinese Financial Assistant with Large Language Model\r\nZhongjing: Enhancing the Chinese Medical Capabilities of Large\r\nLanguage Model through Expert Feedback and Real-world Multi-turn\r\nDialogue\r\n\r\nLLM超长文本处理 (long_input)\r\n\r\nParallel Context Windows for Large Language Models\r\nStructured Prompting: Scaling In-Context Learning to 1,000\r\nExamples\r\n苏剑林,\r\nNBCE：使用朴素贝叶斯扩展LLM的Context处理长度 ⭐️\r\nVcc: Scaling Transformers to 128K Tokens or More by Prioritizing\r\nImportant Tokens\r\nUnlimiformer: Long-Range Transformers with Unlimited Length\r\nInput\r\nScaling Transformer to 1M tokens and beyond with RMT\r\nRECURRENTGPT: Interactive Generation of (Arbitrarily) Long Text\r\nTRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT\r\nLENGTH EXTRAPOLATION ⭐️\r\nFlashAttention: Fast and Memory-Efficient Exact Attention with\r\nIO-Awareness\r\nExtending Context Window of Large Language Models via Positional\r\nInterpolation\r\nLongNet: Scaling Transformers to 1,000,000,000 Tokens\r\nhttps://kaiokendev.github.io/til#extending-context-to-8k\r\n苏剑林,Transformer升级之路：10、RoPE是一种β进制编码\r\n⭐️\r\n苏剑林,Transformer升级之路：11、将β进制位置进行到底\r\n苏剑林,Transformer升级之路：12、无限外推的ReRoPE？\r\nFocused Transformer: Contrastive Training for Context Scaling\r\nLost in the Middle: How Language Models Use Long Contexts ⭐️\r\nEFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS\r\nRing Attention with Blockwise Transformers for Near-Infinite\r\nContext\r\n\r\nNL2SQL\r\n\r\n大模型方案\r\n\r\nDIN-SQL: Decomposed In-Context Learning of Text-to-SQL with\r\nSelf-Correction ⭐️\r\nC3: Zero-shot Text-to-SQL with ChatGPT ⭐️\r\nSQL-PALM: IMPROVED LARGE LANGUAGE MODEL ADAPTATION FOR\r\nTEXT-TO-SQL\r\nBIRD Can LLM Already Serve as A Database Interface? A BIg Bench for\r\nLarge-Scale Database Grounded Text-to-SQL ⭐️\r\nA Case-Based Reasoning Framework for Adaptive Prompting in\r\nCross-Domain Text-to-SQL\r\nChatDB: AUGMENTING LLMS WITH DATABASES AS THEIR SYMBOLIC MEMORY\r\nA comprehensive evaluation of ChatGPT’s zero-shot Text-to-SQL\r\ncapability\r\nFew-shot Text-to-SQL Translation using Structure and Content Prompt\r\nLearning\r\n\r\nDomain Knowledge Intensive\r\n\r\nTowards Knowledge-Intensive Text-to-SQL Semantic Parsing with\r\nFormulaic Knowledge\r\nBridging the Generalization Gap in Text-to-SQL Parsing with Schema\r\nExpansion\r\nTowards Robustness of Text-to-SQL Models against Synonym\r\nSubstitution\r\nFinQA: A Dataset of Numerical Reasoning over Financial Data\r\n\r\nothers\r\n\r\nRESDSQL: Decoupling Schema Linking and Skeleton Parsing for\r\nText-to-SQL\r\nMIGA: A Unified Multi-task Generation Framework for Conversational\r\nText-to-SQL\r\n\r\n\r\n降低模型幻觉 (reliability)\r\n\r\nSurvey of Hallucination in Natural Language Generation\r\nTrusting Your Evidence: Hallucinate Less with Context-aware Decoding\r\n⭐️\r\nSELF-REFINE:ITERATIVE REFINEMENT WITH SELF-FEEDBACK ⭐️\r\nPROMPTING GPT-3 TO BE RELIABLE\r\nEnhancing Self-Consistency and Performance of Pre-Trained Language\r\nModels through Natural Language Inference\r\nOn the Advance of Making Language Models Better Reasoners\r\nProgressive-Hint Prompting Improves Reasoning in Large Language\r\nModels\r\nASK ME ANYTHING: A SIMPLE STRATEGY FOR PROMPTING LANGUAGE MODELS\r\n⭐️\r\nInference-Time Intervention: Eliciting Truthful Answers from a\r\nLanguage Model\r\nReflexion: an autonomous agent with dynamic memory and\r\nself-reflection\r\nSelf-consistency for open-ended generations\r\nCheck Your Facts and Try Again: Improving Large Language Models with\r\nExternal Knowledge and Automated Feedback\r\nFactuality Enhanced Language Models for Open-Ended Text\r\nGeneration\r\nAdaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of\r\nLarge Language Models in Knowledge Clashes\r\nRethinking with Retrieval: Faithful Large Language Model\r\nInference\r\nRefGPT: Reference → Truthful &amp; Customized Dialogues Generation\r\nby GPTs and for GPTs\r\nEnabling Large Language Models to Generate Text with Citations\r\nLarge language models and the perils of their hallucinations\r\n\r\n大模型评估（evaluation）\r\n\r\n事实性评估\r\n\r\nTRUSTWORTHY LLMS: A SURVEY AND GUIDELINE FOR EVALUATING LARGE\r\nLANGUAGE MODELS’ ALIGNMENT\r\nTrueTeacher: Learning Factual Consistency Evaluation with Large\r\nLanguage Models\r\nTRUE: Re-evaluating Factual Consistency Evaluation\r\nSelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\r\nGenerative Large Language Models\r\nFACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in\r\nLong Form Text Generation\r\nKoLA: Carefully Benchmarking World Knowledge of Large Language\r\nModels\r\n\r\n\r\n推理优化(inference)\r\n\r\nFast Transformer Decoding: One Write-Head is All You Need\r\nFast Inference from Transformers via Speculative Decoding\r\nGQA: Training Generalized Multi-Query Transformer Models from\r\nMulti-Head Checkpoints\r\nSkeleton-of-Thought: Large Language Models Can Do Parallel\r\nDecoding\r\nSkipDecode: Autoregressive Skip Decoding with Batching and Caching\r\nfor Efficient LLM Inference\r\nBatchPrompt: Accomplish more with less\r\n\r\n模型知识编辑黑科技(model_edit)\r\n\r\nROME：Locating and Editing Factual Associations in GPT\r\nTransformer Feed-Forward Layers Are Key-Value Memories\r\nMEMIT: Mass-Editing Memory in a Transformer\r\nMEND：Fast Model Editing at Scale\r\nEditing Large Language Models: Problems, Methods, and\r\nOpportunities\r\n\r\nOther Prompt\r\nEngineer(prompt_engineer)\r\n\r\nCalibrate Before Use: Improving Few-Shot Performance of Language\r\nModels\r\nIn-Context Instruction Learning\r\nLEARNING PERFORMANCE-IMPROVING CODE EDITS\r\nBoosting Theory-of-Mind Performance in Large Language Models via\r\nPrompting\r\nGenerated Knowledge Prompting for Commonsense Reasoning\r\nRECITATION-AUGMENTED LANGUAGE MODELS\r\nkNN PROMPTING: BEYOND-CONTEXT LEARNING WITH CALIBRATION-FREE NEAREST\r\nNEIGHBOR INFERENCE\r\nEmotionPrompt: Leveraging Psychology for Large Language Models\r\nEnhancement via Emotional Stimulus\r\nCausality-aware Concept Extraction based on Knowledge-guided\r\nPrompting\r\nLARGE LANGUAGE MODELS AS OPTIMIZERS\r\n\r\nMultimodal\r\n\r\nInstructBLIP: Towards General-purpose Vision-Language Models with\r\nInstruction Tuning\r\nVisual ChatGPT: Talking, Drawing and Editing with Visual Foundation\r\nModels\r\nPaLM-E: An Embodied Multimodal Language Model\r\nLLava Visual Instruction Tuning\r\nMiniGPT-4: Enhancing Vision-Language Understanding with Advanced\r\nLarge Language Models\r\nTabLLM: Few-shot Classification of Tabular Data with Large Language\r\nModels\r\nBLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich\r\nVisual Questions\r\nmPLUG-Owl : Modularization Empowers Large Language Models with\r\nMultimodality\r\nLVLM eHub: A Comprehensive Evaluation Benchmark for Large\r\nVisionLanguage Models\r\n\r\nOthers\r\n\r\nPretraining on the Test Set Is All You Need\r\n哈哈作者你是懂讽刺文学的\r\nLearnware: Small Models Do Big\r\nThe economic potential of generative AI\r\nA PhD Student’s Perspective on Research in NLP in the Era of Very\r\nLarge Language Models\r\n\r\n🗂 参考链接来源\r\n\r\nAwesome-AIGC-Tutorials/\r\nhttps://github.com/gongminmin/awesome-aigc\r\nhttps://github.com/Moonvy/OpenPromptStudio\r\nhttps://github.com/wshzd/Awesome-AIGC/tree/main\r\nhttps://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide\r\nStable Diffusion全套教程https://github.com/ai-vip/stable-diffusion-tutorial\r\nhttps://github.com/open-mmlab/mmagic\r\nhttps://github.com/IDEA-CCNL/Fengshenbang-LM\r\n\r\n🤝 友情链接\r\n\r\nWayToAGI\r\n\r\nWaytoAGI.com\r\n是最全面的中文AIGC资源知识库，包括最新AI动态、提示词、学习指南等，长期保持活跃更新。\r\n\r\nAwesome\r\nTool Learning\r\n\r\nAwesome Tool Learning\r\n提供丰富的关于工具学习的资源，包括论文、框架和应用程序。\r\n\r\nAwesome\r\nDomain LLM\r\n\r\n这个GitHub仓库是一个汇集和整理了自ChatGPT等大语言模型出现后，各种垂直领域开源模型、数据集和评测基准的列表，同时鼓励大家为其贡献未收录的资源。\r\n\r\n\r\n🗂声明\r\n以上部分资料来自网络整理，供大家学习参考，如有侵权，麻烦联系我删除！\r\n","slug":"aigcpaper","date":"2023-10-20T12:18:45.830Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"King's OvO²"},{"id":"5c307478e6960dd7681eaa126a1f4100","title":"AI绘画之中国古建筑之美","content":"古建筑之美，在于它承载的是不同历史时期的人文特色，承载的是碧瓦朱檐间亘古流传的故事，还承载了中华千百年的历史文化。\n\n中国古建筑见证了中国千古文化底蕴，一砖一瓦一构件都是历代中国建筑设计师的心血。\n下面是使用AIGC模型生成的山水画与古建筑：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"aigc1","date":"2023-09-06T09:19:32.000Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"King's OvO²"},{"id":"711817060a93d5d34ba36cdecc22ad6f","title":"AI绘画之东方之美","content":"\n\n\n\n\n\n\n\n\n","slug":"aigc2","date":"2023-09-06T09:19:32.000Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"King's OvO²"},{"id":"dcf9040365420381634e4cbf6986bee1","title":"AI绘画之东方之罗刹国市-根据歌词描述马户和鸡的外貌绘制","content":"时隔多年刀郎终于又出新歌了。正所谓十年磨一剑，这次带来聊斋系列真是太好听了。特别是这首\r\n罗刹国向东两万六千里\n过七冲越焦海三寸的黄泥地\n只为那有一条一丘河\n河水流过苟苟营\n苟苟营当家的叉杆儿唤作马户\n十里花场有浑名\n她两耳傍肩三孔鼻\n未曾开言先转腚\n每一日蹲窝里把蛋来卧\n老粉嘴多半辈儿以为自己是只鸡\n那马户不知道他是一头驴\n那又鸟不知道他是一只鸡\n勾栏从来扮高雅\n自古公公好威名\n打西边来了一个小伙儿他叫马骥\n美丰姿 少倜傥 华夏的子弟\n只为他人海泛舟搏风打浪\n龙游险滩流落恶地\n他见这罗刹国里常颠倒\n马户爱听那又鸟的曲\n三更的草鸡打鸣当司晨\n半扇门楣上裱真情\n它红描翅那个黑画皮\n绿绣鸡冠金镶蹄\n可是那从来煤蛋儿生来就黑\n不管你咋样洗呀那也是个脏东西\n那马户不知道他是一头驴\n那又鸟不知道他是一只鸡\n岂有画堂登猪狗\n哪来鞋拔作如意\n它红描翅那个黑画皮\n绿绣鸡冠金镶蹄\n可是那从来煤蛋儿生来就黑\n不管你咋样洗呀那也是个脏东西\n爱字有心心有好歹\n百样爱也有千样的坏\n女子为好非全都好\n还有黄蜂尾上针\n西边的欧钢有老板\n生儿维特根斯坦\n他言说马户驴又鸟鸡\n到底那马户是驴还是驴是又鸟鸡\n那驴是鸡那个鸡是驴\n那鸡是驴那个驴是鸡\n那马户又鸟\n是我们人类根本的问题\r\n\r\n","slug":"aigc3","date":"2023-09-06T09:19:32.000Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"King's OvO²"},{"id":"310160f9249e4bbe50b479966d05fe26","title":"AIGC生成电影动画视频","content":"AIGC（Artificial Intelligence Generated\r\nContent）是一种先进的技术，用于生成电影和动画视频。它利用强大的人工智能算法和深度学习技术，可以自动创造生动、富有创意的电影和动画内容，无需人工干预。AIGC能够生成高质量的视觉效果、角色动画和情节，以满足各种创作者和制片人的需求。这项技术正在电影和动画制作领域引发革命，使内容创作更加高效，同时拓展了创意的边界，为观众呈现出更多令人惊叹的视觉体验。\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","slug":"aigc5","date":"2022-09-06T09:19:32.000Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"King's OvO²"},{"id":"12637e04fdd5cb76f4fc78ed59ef0d15","title":"NLP技术汇总","content":"Deep learning speech learning library &lt;br&gt;\n\n\n一个轻量级、简单易用的 RNN 唤醒词监听器: https://github.com/MycroftAI/mycroft-precise\nzh:http://fancyerii.github.io/books/mycroft-precise/\n基于树莓派的人工智能小车，实现识别、提示、智能旅游线路、离线图像:https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi\n中文NLP数据集:https://github.com/CLUEbenchmark/CLUEDatasetSearch\n模型：https://github.com/CLUEbenchmark/CLUE\n中文 NLP 资源精选列表 中文自然语言处理相关资料:https://github.com/crownpku/Awesome-Chinese-NLP\n视觉聊天机器人:https://paperswithcode.com/paper/visual-dialog\nBert&#x2F;Transformer模型压缩与优化加速: https://blog.csdn.net/nature553863/article/details/120292394：\n可以压缩 BERT 的所有方式：http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.htmlhttps://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html\nBERT轻量化探索—模型剪枝（BERT Pruning）—Rasa维度剪枝:https://blog.csdn.net/ai_1046067944/article/details/103609152\n压缩 BERT 以加快预测速度:https://rasa.com/blog/compressing-bert-for-faster-prediction-2/\n论文综述与BERT相关最新论文:https://github.com/tomohideshibata/BERT-related-papers\n中文自然语言排行榜及论文查询:https://www.cluebenchmarks.com/index.html\n计算语言学国际会议论文集:https://aclanthology.org/volumes/2020.coling-main/\n计算语言学协会第 58 届年会论文集:https://aclanthology.org/volumes/2020.acl-main/\n计算语言学2协会2021年会论文搜集：https://aclanthology.org/events/acl-2021/\n中文BERT全词掩蔽预训练（中文BERT-wwm系列模型）https://github.com/ymcui/Chinese-BERT-wwm\n一个大规模的中文跨领域面向任务的对话数据集:https://github.com/thu-coai/CrossWOZ\n关于ConvLab-2：用于构建、评估和诊断对话系统的开源工具包（支持中文）：https://github.com/thu-coai/ConvLab-2\n视觉和语言预训练模型 (VL-PTM) 的最新进展(语音视觉融合):https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers\n深度学习和自然语言处理阅读清单:https://github.com/IsaacChanghau/DL-NLP-Readings\n视觉问答 (VQA)（图像&#x2F;视频问答）、视觉问题生成、视觉对话、视觉常识推理和相关领域的精选列表：https://github.com/jokieleung/awesome-visual-question-answering\n","slug":"nlp","date":"2021-04-05T14:57:06.000Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"King's OvO²"},{"id":"28c7d3817ed8352eb1372ac636977eb8","title":"人工智能科技与文献网","content":"\nAI新闻网：https://www.marktechpost.com/\n算法核心基础与AI模型设计【我的CSDN技术博客】：https://blog.csdn.net/weixin_41194129/category_11362509.html\nAI算法学习社区: https://github.com/Algorithm-learning-community-for-python\nYOLO系列资料汇总：https://github.com/KangChou/Cver4s\nNVIDIA-CUDA编程:https://github.com/KangChou/deepcv_project_demo/tree/main/CUDA%E7%BC%96%E7%A8%8B\n自动驾驶点云技术: https://github.com/KangChou/deepcv_project_demo/tree/main/CVPR/point-cloud\n计算机视觉技术： https://github.com/KangChou/deepcv_project_demo/tree/main/CVPR/visual\n\n专业的聊天机器人: https://github.com/salesforce/Converse\n基于开源GPT2.0的初代创作型人工智能 | 可扩展、可进化:https://github.com/EssayKillerBrain/EssayKiller_V2\n高质量中文预训练模型集合:https://github.com/CLUEbenchmark/CLUEPretrainedModels\n自然语言基础模型:https://github.com/lpty/nlp_base\nBERT模型从训练到部署全流程:https://github.com/xmxoxo/BERT-train2deploy\n中文BERT-wwm系列模型:https://github.com/ymcui/Chinese-BERT-wwm\n深度学习入门教程, 优秀文章: https://github.com/Mikoto10032/DeepLearning\n3D视觉、VSLAM、计算机视觉的干货资料: https://github.com/qxiaofan/awesome_3d_slam_resources\n自动驾驶系统实现:https://github.com/sunmiaozju/smartcar\n身份证自动识别,银行卡识别,驾驶证识别,行驶证识别：https://github.com/wenchaosong/OCR_identify\nMVision 机器视觉 机器视觉：https://github.com/Ewenwan/MVision\nComputer Vision: Algorithms and Applications：https://szeliski.org/Book/\n自动驾驶的激光雷达点云处理: https://github.com/beedotkiran/Lidar_For_AD_references\n动态语义SLAM 目标检测+VSLAM+光流&#x2F;多视角几何动态物体检测+octomap地图+目标数据库:https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic\n基于视频的目标检测算法研究:https://github.com/guanfuchen/video_obj\nTensorRT-7 Network: https://github.com/Syencil/tensorRT\nC++ TensorRT-CenterNet: https://github.com/CaoWGG/TensorRT-CenterNet\nyolox-deepsort:https://github.com/Sharpiless/yolox-deepsort\n\nBirdNet+：LiDAR 鸟瞰图中的端到端 3D 对象检测:https://github.com/AlejandroBarrera/birdnet2\n关于nuScenes 数据集的开发套件:https://github.com/nutonomy/nuscenes-devkit\nA robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR:https://github.com/hku-mars/loam_livox\n激光雷达论文：https://arxiv.org/search/?query=+LiDAR&amp;searchtype=all&amp;source=header\n使用CUDA PCL 加速Jetson的点云处理：https://developer.nvidia.com/zh-cn/blog/cuda-pcl-1-0-jetson/\nPCT: Point Cloud Transformer: https://github.com/MenghaoGuo/PCT\n\n","slug":"ai1","date":"2018-06-05T04:57:06.000Z","categories_index":"人工智能","tags_index":"人工智能","author_index":"King's OvO²"},{"id":"bbbddb6726a8dd0e4e94bb6dc760d764","title":"开源书籍与源码","content":"\r\nPython for《Deep Learning》，该书为《深度学习》(花书)\r\n数学推导、原理剖析与源码级别代码实现 https://github.com/MingchaoZhu/DeepLearning\r\nhttps://github.com/ytin16/awesome-machine-learning-1/tree/master\r\n\r\n\r\nimage.png\r\n\r\n\r\n\r\nimage-20200728231437641\r\n\r\n数学分析上下册\r\n\r\npython自动化操作\r\n\r\npython从入门让到实践\r\n\r\n数字信号处理\r\n\r\n《智能问答与深度学习》\r\nhttps://github.com/l11x0m7/book-of-qna-code\r\n人工智能实践：Tensorflow笔记 https://github.com/jlff/tf2_notes\r\n源码下载链接：https://pan.baidu.com/s/19XC28Hz_TwnSQeuVifg1UQ\r\n提取码：mocm\r\n数据科学/人工智能比赛解决方案聚合:https://github.com/apachecn/awesome-data-comp-solution\r\n《学习学习与计算机视觉》配套代码:\r\nhttps://github.com/frombeijingwithlove/dlcv_for_beginners\r\n《算法导论》的C++实现\"代码：https://github.com/huaxz1986/cplusplus-_Implementation_Of_Introduction_to_Algorithms\r\nhttp://www.huaxiaozhuan.com/\r\n《Unix\r\n环境高级编程第三版》笔记：https://github.com/huaxz1986/APUE_notes\r\n算法工程师(人工智能cv方向)面试问题及相关资料的网站收集:https://github.com/lcylmhlcy/Awesome-algorithm-interview\r\n网址资源\r\n\r\nGitHub使用教程\r\n\r\n博客\r\n\r\nOpen AI\r\nDeepMind\r\nFacebook AI\r\nResearch博客\r\nFerencHuszár的博客(剑桥的博士)\r\nDistill致力于清晰地解释机器学习\r\nGraduate\r\nDescent(深度学习的自然语言处理)\r\nAdit\r\nDeshpande的博客(机器学习和深度学习)\r\nAndrew\r\nTrask的博客-神经网络及其解释和实现\r\n特斯拉的人工智能总监Andrej\r\nKarpathy的博客\r\nDenny Britz的博客(Google\r\nBrain团队的前员工)\r\nSebastian\r\nRuder(文本分析初创公司Aylien的研究科学家)\r\nColah的博客\r\nOlah旨在以简单的方式解释神经网络的复杂功能\r\nBAIR博客旨在传播BAIR在人工智能研究方面的研究成果，观点和最新情况\r\n\r\nGithub资源\r\n书籍代码资料\r\n\r\n书籍源码实现：《TensorFlow实战》\r\n书籍《Python计算机视觉中译本》代码实例\r\n书籍《统计学习方法-李航》一书中所有算法实现\r\n书籍源码《Hands-On\r\nTransfer Learning with Python》\r\n书籍《Machine-Learning\r\n《Machine Learning in Action》-中文版 代码\r\n书籍《Deep\r\nLearning《 Deep Learning With Python - 中文版》.pdf 代码\r\n书籍《Deep-Learning\r\n《 Applied Deep Learning with Python》.pdf 》代码\r\n书籍 《Pattern recognition\r\nand machine learing_马春鹏翻译版.pdf 》MatLab代码 ; python版\r\n\r\n面试资源\r\n\r\nDeepLearning-500-questions\r\n2019-Autumn-recruitment-experience\r\n机器学习资源\r\nMachine learning Resources\r\n机器学习&amp;深度学习网站资源汇总（Machine\r\nLearning Resources）\r\n2018/2019/校招/春招/秋招/算法/机器学习(Machine\r\nLearning)/深度学习(Deep\r\nLearning)/自然语言处理(NLP)/C/C++/Python/面试笔记\r\n\r\nGitHub其他awesome资源\r\n\r\nAwesome-pytorch-list\r\nawesome-pytorch-scholarship\r\nAwesome\r\nPython Applications\r\nawesome-deeplearning-resources\r\nTensorFlow - A\r\ncurated list of dedicated resources\r\nAwesome\r\nObject Detection based on handong1587 github\r\nawesome-deep-learning\r\nA curated list of awesome Deep Learning tutorials, projects and\r\ncommunities.\r\n\r\n视频资源\r\nTensorFlow资源\r\n\r\nTF\r\nGirls 修炼指南\r\n炼数成金Tensorflow公开课\r\nDan Does Data: Tensor Flow\r\n谷歌Tensorflow官网上的视频教程\r\nTensorflow与深度学习（人工智能）-初学篇\r\n(Martin Görner)\r\n斯坦福大学Tensorflow系列的课程\r\n；课程主页\r\n；课程实战代码\r\nJulia\r\nFerraioli, Amy Unruh, Eli Bixby - Diving into Machine Learning through\r\nTensorFlow - PyCon 2016\r\nTensorflow代码练习\r\nTensorFlow官方中文教程\r\n从Tensorflow基础知识到有趣的项目应用\r\n构建您的第一款TensorFlow\r\nAndroid应用程序\r\n使用Jupyter\r\nNotebook运行的TensorFlow教程\r\nSimple\r\nand ready-to-use tutorials for TensorFlow\r\ntensorflow实战练习，包括强化学习、推荐系统、nlp等\r\nTensorFlow\r\nTutorial and Examples for Beginners with Latest APIs\r\n\r\n深度学习视频\r\n\r\n台湾国立大学李宏毅教程深度学习的课程\r\n\r\n在线书籍资源\r\n\r\n《动手学\r\n深度学习》\r\nDeep\r\nLearning Book Chinese Translation Companion webpage to the book \"Mathematics\r\nFor Machine Learning\"\r\n\r\n深度学习实战资源\r\n\r\n免费的编程中文书籍索引\r\nILearnDeepLearning.py\r\nAll Algorithms\r\nimplemented in Python\r\n【国赛】【美赛】数学建模相关算法\r\nMATLAB实现\r\nAI比赛相关\r\nText to\r\nimage synthesis using thought vectors\r\nGenerative\r\nAdversarial Text to Image Synthesis\r\nCode for paper\r\n\"Plug and Play Generative Networks\"\r\nTensorFlow\r\nImplementation of \"Show, Attend and Tell\"\r\nEstimate 3D\r\nface pose by a Convolutional Neural Network\r\npracticalAI :\r\nA practical approach to learning machine learning.\r\nConvolutional\r\nNeural Network for Text Classification in Tensorflow\r\nA\r\nrecurrent neural network for generating little stories about\r\nimages\r\nGenerative\r\nHandwriting using LSTM Mixture Density Network with TensorFlow\r\nMask R-CNN for\r\nobject detection and instance segmentation on Keras and\r\nTensorFlow\r\nMachine learning\r\nresources，including algorithm, paper, dataset, example and so\r\non.\r\nSimple\r\nembedding based text classifier inspired by fastText, implemented in\r\ntensorflow\r\nA PyTorch\r\nimplementation of the architecture of Mask RCNN, serves as an\r\nintroduction to working with PyTorch\r\nTranslate darknet to\r\ntensorflow. Load trained weights, retrain/fine-tune using tensorflow,\r\nexport constant graph def to mobile devices\r\n\r\n论文源码复现\r\n\r\nVisualizing\r\nthe Loss Landscape of Neural Nets. NIPS\r\nAdversarially\r\nParameterized Optimization for 3D Human Pose Estimation\r\nNSGA-NET: A\r\nMulti-Objective Genetic Algorithm for Neural Architecture\r\nSearch\r\nPyTorch\r\nimplementation of convolutional neural networks-based text-to-speech\r\nsynthesis models\r\n\r\nPaper\r\n\r\nDeep\r\nLearning Papers Reading Roadmap\r\nActivation Functions:\r\nComparison of Trends in Practice and Research for Deep Learning\r\nPooling is neither\r\nnecessary nor sufficient for appropriate deformation stability in\r\nCNNs\r\n\r\n参考来自\r\nhttps://github.com/ytin16/awesome-machine-learning-1/tree/master\r\n","slug":"books","date":"2017-05-05T14:57:06.000Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"King's OvO²"}]