[{"id":"f73a8e23e6f6f669cf99c7dba8fa0722","title":"","content":"  \r\n\r\n这里收集了关于AIGC的各种精选教程和资源，既适合初学者也适合进阶AI爱好者。\r\n📜 目录\r\n\r\n👋 入门\r\n💬 大语言模型\r\n\r\n💡 提示工程\r\n🔧 大语言模型实践\r\n🔬 大语言模型理论\r\n\r\n🎨 AI绘画\r\n\r\n🧑‍🎨 艺术基础与AI绘画技巧\r\n🌊 Stable\r\nDiffusion原理与应用\r\n\r\n🔊 AI音频\r\n🌈 多模态\r\n🧠 深度学习\r\n💻 AI系统\r\n🗂 其他\r\n\r\n✨ 点赞历史\r\n🤝 友情链接\r\n\r\n\r\n👋 入门\r\n\r\nAI\r\nfor Everyone - 吴恩达  \r\nPractical\r\nAI for Teachers and Students - 沃顿商学院  \r\nArtificial\r\nIntelligence for Beginners - 微软 \r\nGenerative AI\r\nlearning path - 谷歌  \r\n\r\n💬 大语言模型\r\n💡 提示工程\r\n\r\nChatGPT\r\nPrompt Engineering for Developers - DeepLearning.AI  \r\nBuilding\r\nSystems with the ChatGPT API - DeepLearning.AI  \r\nLangChain\r\nfor LLM Application Development - DeepLearning.AI  \r\nLangChain:\r\nChat with Your Data - DeepLearning.AI  \r\nPrompt\r\nEngineering for ChatGPT - 范德堡大学  \r\nLearn Prompting \r\nLangChain\r\nAI Handbook - James Briggs, Francisco Ingham  \r\n\r\n🔧 大语言模型实践\r\n\r\nLLM\r\nBootcamp - The Full Stack  \r\nFinetuning\r\nLarge Language Models - DeepLearning.AI  \r\n\r\n🔬 大语言模型理论\r\n\r\nCS324 -\r\nAdvances in Foundation Models - 斯坦福大学 \r\n\r\n斯坦福大学关于大模型的新课，主要材料是一些notes，介绍了大语言模型的基础知识、能力范围、训练部署以及一些大模型相关的问题（数据安全、法律、危害等），总体来说比较简单，适合入门。2023年的版本对课纲进行了更新，增加了关于图像-文本和多模态的大模型内容。\r\n\r\nCS\r\n601.471/671 NLP: Self-supervised Models - 约翰霍普金斯大学 \r\n\r\nJHU也是NLP大牛校，这门课难度适中，课程主页上各类资源还挺多的，建议大家看一看。\r\n\r\nCS224N: Natural\r\nLanguage Processing with Deep Learning - 斯坦福大学  \r\n\r\n这门课Christopher\r\nManning在斯坦福开了很多年，很经典的课程。前面是NLP的基础知识，后面几节课会涉及到大语言模型。\r\n\r\nSpeech and\r\nLanguage Processing - Dan Jurafsky and James H. Martin  \r\n\r\n最经典的NLP教材，本来计划在大概三四年前就完稿的，但是由于近几年NLP领域发展实在太快，作者干脆就不设DDL了，一直在持续更新中。\r\n\r\nCOS\r\n597G (Fall 2022): Understanding Large Language Models - 普林斯顿大学\r\n\r\n\r\nDanqi\r\nChen的课，课程难度较高，主要材料是PPT和相关的论文，适合深入LLM某个方向的同学。\r\n\r\n\r\n🎨 AI绘画\r\n🧑‍🎨 艺术基础与AI绘画技巧\r\n\r\n系列讲座:每周一个关于艺术基础的有趣话题\r\n- Niji Academy [中文版]\r\n\r\nAIGCTalk-Midjourney学习手册\r\n\r\n【Midjourney】保姆级AI绘画创作系列教学课程\r\n- 莱森 \r\n\r\n\r\n🌊 Stable Diffusion原理与应用\r\n\r\n【AI绘画】Stable\r\nDiffusion 系列教程  \r\n\r\n秋葉aaaki大神喂饭级别Stable Diffusion 系列教程\r\n\r\nHow\r\nDiffusion Models Work - DeepLearning.AI  \r\n扩散模型\r\n- Diffusion Model - 李宏毅  \r\n\r\n偏宏观，比较通俗易懂\r\n\r\nDiffusion扩散模型\r\n- 唐宇迪  \r\n\r\n唐宇迪老师讲stable diffusion和dalle推理讲的比较清楚\r\n\r\nHugging\r\nFace Diffusion Models Course \r\n\r\n🔊 AI音频\r\n\r\nHugging\r\nFace Audio Course \r\nCS224S: Spoken\r\nLanguage Processing - 斯坦福大学 \r\n\r\n🌈 多模态\r\n\r\nTutorial\r\non MultiModal Machine Learning (ICML 2023) - 卡耐基梅隆大学  \r\n11-777:\r\nMultiModal Machine Learning (Fall 2022) - 卡耐基梅隆大学  \r\n11-877:\r\nAdvanced Topics in MultiModal Machine Learning (Fall 2022) -\r\n卡耐基梅隆大学 \r\n\r\n🧠 深度学习\r\n\r\nNeural\r\nNetworks/Deep Learning - StatQuest  \r\nNeural\r\nNetworks - 3Blue1Brown  \r\nNeural Networks:\r\nZero to Hero - Andrej Karpathy  \r\nPractical Deep Learning for Coders\r\n- fast.ai  \r\nDeep\r\nLearning Specialization - 吴恩达  \r\n6.S191: Introduction to\r\nDeep Learning - 麻省理工学院  \r\nCS25: Transformers\r\nUnited V2 - 斯坦福大学  \r\nDeep\r\nLearning Lecture Series 2020 - DeepMind x 伦敦大学学院  \r\nReinforcement\r\nLearning Lecture Series 2021 - DeepMind x 伦敦大学学院  \r\n\r\n💻 AI系统\r\n\r\nAI-Sys-Sp22\r\nMachine Learning Systems - 加州大学伯克利分校 \r\nDeep Learning Systems: Algorithms\r\nand Implementation - Tianqi Chen, Zico Kolter  \r\nCS 329S: Machine\r\nLearning Systems Design - 斯坦福大学 \r\n15-849: Machine\r\nLearning Systems - 卡耐基梅隆大学 \r\nComputer\r\nScience 598D - Systems and Machine Learning - 普林斯顿大学 \r\n\r\nAIGC视频会议&amp;访谈\r\n智源社区\r\n【论文分享】【AugGPT：利用ChatGPT进行文本数据增强\r\n】[link]\r\n【论文分享】【ChatGPT的鲁棒性探究——对抗性和分布外泛化的视角\r\n】[link]\r\n【论文分享】【传统检索模型和大语言模型在信息搜索中的应用和对比\r\n】[link]，[paper]，[code]，[blog]\r\n访谈&amp;视频\r\n【访谈】【OpenAI 的核心研发人员 Jack Rae 在参加\r\nStanford MLSys Seminar 的访谈时进行了一个名为 Compression for\r\nAGI的主题分享 】[访谈记录]\r\n【访谈】【万字长文：想训大模型？这里有一份避坑指南】[访谈记录]\r\n【访谈】【微软Bing版ChatGPT表明想做人类，并且对纽约时报专栏作家表达爱意】[访谈记录]\r\n【访谈】【Midjourney创始人David\r\nHolz关于生成式AI的访谈】[访谈记录]\r\n【访谈】【OpenAI创始人：GPT-4的研究起源和构建心法】[访谈记录]\r\n【访谈】【ABC News\r\n专访OpenAI首席执行官萨姆·奥尔特曼：AI风险和重塑社会的问题】[访谈记录]\r\n【访谈】【OpenAI联合创始人Ilya\r\nSutskever等专访：开源人工智能是不明智的】[访谈记录]\r\n【访谈】【OpenAI董事长、CTO Greg Brockman专访\r\n：GPT-4 并不完美，不过人无完人】[访谈记录]\r\n【访谈】【图灵奖获得者 Yoshua Bengio 认为 ChatGPT\r\n是一个“警钟”】[访谈记录]\r\n【访谈】【《麻省理工科技评论》对 ChatGPT\r\n幕后团队，进行了一次深入的独家专访】[访谈记录]\r\n【访谈】【口述历史，探析ChatGPT的创造历程，ChatGPT内部故事】[访谈记录]\r\n【访谈】【对话ChatGPT之父！AI会改变什么？不会改变什么？】[访谈记录]\r\n【访谈】【对话OpenAI研究科学家：他们是如何让GPT4更像人的？】[访谈记录]\r\n【视频】【邱锡鹏教授介绍以ChatGPT为核心的大规模语言模型的相关知识及未来的发展方向\r\n】[B站]\r\nLLM体验效果&amp;专业评估\r\n【LLM效果对比】【360智脑_VS_讯飞星火】[blog]\r\n【LLM效果对比】【阿里通义千问_VS_讯飞星火】[blog]\r\n【LLM效果对比】【Bard_VS_Baize-7B_VS_文心一言】[blog]\r\n【LLM效果对比】【Bard_VS_Bing_VS_ChatGPT】[blog]\r\n【LLM效果对比】【Bard_VS_文心一言】[blog]\r\n【LLM效果对比】【ChatGPT_VS_GPT4】[blog]\r\n【LLM效果对比】【OpenAssistant_VS_百度文心一言】[blog]\r\n【LLM效果对比】【文心一言新闻发布会内容复现】[blog]\r\n【LLM效果对比】【文心一言_VS_ChatGLM-6B】[blog]\r\n【LLM效果对比】【文心一言 VS GPT-4：20道问答PK】[blog]\r\n【LLM效果对比】【文心一言 vs GPT-4实测！】[blog]\r\n【LLM效果对比】【讯飞星火_VS_文心一言】[blog]\r\n【ChatGPT专业评估】【一文看遍各行业对ChatGPT的专业评估】[blog]\r\n【ChatGPT专业评估】【ChatGPT关于推理、幻觉和交互的多任务、多语言、多通道评估\r\n】[paper]\r\n【ChatGPT专业评估】【如何评价 OpenAI 的超级对话模型\r\nChatGPT ？】[paper]\r\n【ChatGPT专业评估】【用ChatGPT参加计算机科学考试】[paper]\r\n【LLM知识评估】【C-Eval：构造中文大模型的知识评估基准】[主页]，[paper]，[code]，[blog]\r\n【MLLM幻觉评估】【多模态大模型的幻觉问题与评估】[blog]，[paper]，[code]\r\n【各大大模型评测】【粗看大模型ChatGLM、MOSS、Bloomz在中文垂域评测中的性能表现：医学、法律、心理学、教育等四大类试题下的测试报告介绍】[paper]，[code]，[blog]\r\n【国内大模型评测】【评测国内各种对标 ChatGPT\r\n的大语言模型】[blog]，[code]\r\n【大模型排行榜】【OpenLLM大模型排行榜】[主页]，[blog]，[最新进展blog]\r\n【大模型排行榜】【斯坦福发布LLM排行榜AlpacaEval，微软WizardLM登顶开源模型第一】[blog]，[主页]，[code]\r\nLLM垂直领域大模型\r\n法律\r\n【再看基于LLaMA的最新微调模型变体：CaMA、ExpertLLaMA以及第四个中文法律微调模型LexiLaw】[blog]\r\n【基于中文法律知识的大语言模型——LaWGPT】[blog]\r\n医疗\r\n【AD-AutoGPT：用于阿尔茨海默病信息流行病学的自主GPT】[paper]\r\n【MedQA-ChatGLM - 基于真实医疗对话数据在ChatGLM上进行微调】[code]，[主页]\r\n【谷歌医疗大模型登Nature，Med-PaLM重磅揭秘！AI医生成绩比肩人类】[blog]，[paper]\r\n【PULSE：中文医疗大语言模型】[code]\r\n金融\r\n【FinGPT：一个「专用于金融领域」的开源大语言模型（LLM）框架，源码公开！】[blog]，[paper]，[code]\r\n环境\r\n【清华&amp;中国气象局大模型登Nature：预报时效首次达3小时】[blog]，[paper]\r\n网络安全\r\n【专用于网络攻击的模型FraudGPT】[blog]\r\n交通\r\n【北交大开源交通大模型TransGPT·致远，可免费商用】[blog]，[code]\r\n其他\r\n【南洋理工开源海外中文大语言模型Panda LLM |\r\n探索数据因素和训练策略如何影响大模型性能表现】[paper]，[code]，[blog]\r\nLLM文本检测\r\n【论文&amp;代码】【美国麻省大学&amp;谷歌研究院：改写文本可以避开AI生成文本的检测器，但检索则是一种有效的防御】[paper]，[code]\r\n【论文】【人工智能生成的文本能被可靠地检测出来吗？】[paper]，[blog]\r\n【论文】【DetectGPT（斯坦福大学）：利用概率曲率检测文本是否大模型生成】[paper]，[blog]，[code&amp;data]\r\n【论文】【Detecting LLM-Generated-Text综述】[paper]，[blog]\r\n【论文】【一个专为教育者打造的全新\r\nAI 检测模型】[blog]\r\n【论文】【OpenAI重磅发布官方「ChatGPT检测器」】[blog]\r\n【论文】【斯坦福最新研究：不要过度依赖GPT生成内容，其检测器可能存在不利于非母语英语写作者的偏见】[paper]\r\nLLM长文本解决方案\r\n【苏剑林】【Transformer升级之路：一种全局长度外推的新思路】[blog]\r\n【博客】【ChatGPT能写长篇小说了，ETH提出RecurrentGPT实现交互式超长文本生成】[paper]，[code]，[blog]，[demo1]，[demo2]\r\n【博客】【语言大模型100K上下文窗口的秘诀】[blog]\r\n【博客】【RoPE可能是LLM时代的Resnet】[blog]\r\nLLM可控性与安全\r\n【可控性】【微软提出Control-GPT：用GPT-4实现可控文本到图像生成！】[paper]，[blog]\r\n【可控性】【AIGC如何安全可控?中山大学等最新《AIGC中对隐私和安全的挑战及其补救措施：探索隐私计算、区块链潜在应用》全面阐述】[paper]，[blog]\r\n【可控性】【ControlVideo:\r\n可控的Training-free的文本生成视频】[blog]，[paper]，[code]\r\n【安全】【大模型切脑后变身PoisonGPT，虚假信息案例】[blog]，[code]\r\n【安全】【ChatGPT羊驼家族全沦陷！CMU博士击破LLM护栏，人类毁灭计划脱口而出】[blog]，[paper]，[code]\r\nLLM训练、微调、优化以及部署\r\n【LLM学习网站】【训练、微调、优化和部署大模型最新技术LLM\r\nLearning Lab】[官网]\r\nLLM训练\r\n【LLM训练】【DeepSpeed的Tutorials】[主页]，[DeepSpeed\r\nGetting Starte]\r\n【LLM训练】【如何使用 Megatron-LM 训练语言模型】[blog]\r\n【LLM训练】【Muti Query Attention 和 Attention with\r\nLinear Bias（附源码）】[blog]，[paper]\r\nLLM微调\r\n【LLM微调】【PEFT:\r\n在低资源硬件上对十亿规模模型进行参数高效微调 】[blog]\r\n【LLM微调】【大语言模型（LLM）微调技术笔记】[code]\r\n【LLM微调】【大模型LLM-微调经验分享&amp;总结】[code]，[blog]\r\n【LLM微调】【LoRA：卷完图像生成领域，卷文本生成领域的东西，到时是个啥？】[blog]，[code]\r\n【LLM微调】【Washington大学2023年5月新提出一种高效的微调方法QLoRA，通过降低显存使用，实现在单个48GB\r\nGPU上对65B参数的大模型进行微调，只需微调12个小时就可以达到97%的ChatGPT水平。同时只用int4就可以保持fp16精度的效果。】[paper]\r\n【LLM微调】【华盛顿大学提出全新量化和微调方法，在DB-GPT上享受33B参数的LLM】[blog]\r\n【LLM微调】【陈丹琦团队提出低内存高效零阶优化器MeZO，单卡A100可训练300亿参数模型】[paper]，[code]，[blog]\r\nLLM优化\r\n【LLM优化】【LLM，压缩即泛化，泛化即智能】[blog]\r\n【LLM优化】【LLM-Pruner: 剪枝+少量数据+少量训练 =\r\n高效的LLM压缩】[blog]\r\n【LLM优化】【邱锡鹏团队提出新优化器LOMO｜650亿参数，8块GPU全参数微调】[blog]，[paper]\r\n【LLM优化】【伯克利开源LLM推理与服务库：GPU减半、吞吐数十倍猛增】[中文blog]，[英文blog]\r\n【LLM优化】【LLM\r\nAccelerator：使用参考文本无损加速大语言模型推理】[blog]，[paper]，[code]\r\n【LLM优化】【大模型推理性能优化之KV Cache解读】[blog]\r\n【LLM优化】【CAME：大模型训练成本降低近一半】[blog]\r\nLLM部署\r\n【LLM部署】【工程实践！以LLAMA为例的大模型部署方案】[blog]\r\n【LLM部署】【大模型部署框架FastLLM解析，支持X86/Arm/CUDA\r\n3种架构的硬件！】[blog]，[code]\r\nLLM博客、论文以及代码\r\n【综述】【中文大语言模型汇总：医疗、法律、金融、教育、数学微调，\r\n目前已1.1K星】[code]\r\n【综述】【大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写】[paper]\r\n【综述】【大语言模型综述全新出炉：51页论文带你盘点LLM领域专业化技术】[paper]，[blog]\r\n【综述】【AIGC综述:\r\n从GAN到ChatGPT的生成式人工智能简史】[paper]\r\n【综述】【大模型综述来了！一文带你理清全球AI巨头的大模型进化史】[paper]，[code]\r\n【复旦大学】【复旦大学教授肖仰华：ChatGPT\r\n浪潮下，面向大模型如何做数据治理？】[blog]\r\n【谷歌】【面向决策的基础模型: 问题、方法与机会】[paper]\r\n【谷歌】【较大语言模型上下文学习的方式有所不同】[paper]\r\n【谷歌】【通用语音识别大模型已经支持100+语言】[blog]\r\n【谷歌】【发布5620亿参数多模态模型PaLM-E，机器人操控再上台阶】[paper]，[blog]，[twitter]，[video]\r\n【Huawei】【PanGu-Σ:\r\n稀疏异构计算万亿参数语言模型研究参数语言模型】[paper]\r\n【剑桥大学】【奖励聊天机器人在现实世界中与数以百万计的用户进行互动】[paper]\r\n【LeCun】【人工智能系统最终是否需要以现实为基础，而不仅仅是从语言中学习？】[blog]\r\n【LeCun】【大型语言模型是否需要感官基础来理解意义和理解？】[slices]\r\n【LeCun】【ChatGPT是「外星人」，所以才会胡说八道】[paper]，[blog]\r\n【LeCun】【AI聊天机器人并不关注用户的社交属性】[blog]\r\n【LeCun】【LeCun和马库斯齐喷ChatGPT：大语言模型果然是邪路？】[blog]\r\n【LeCun】【ChatGPT无法实现通用人工智能，但ALM技术路线也许可以】[blog]\r\n【LeCun】【「增强语言模型」的综述 】[paper]\r\n【LeCun】【自回归LLM的缺陷之一，大语言模型必须知道的8个要点】[paper]\r\n【MIT】【从词模型到世界模型：从自然语言到思维概率语言的转变】[paper]\r\n【李开复】【AI进入2.0时代，所有应用都会被重写一遍\r\n】[blog]\r\n【纽约大学】【提出ILF（从语言反馈中模仿学习）：利用语言反馈大规模训练语言模型】[paper]\r\n【OpenAI】【GPT就是GPT：大模型对劳动力市场影响潜力的早期研究】[paper]\r\n【OpenAI】【ABC News\r\n专访OpenAI首席执行官萨姆·奥尔特曼：AI风险和重塑社会的问题】[blog]\r\n【OpenAI】【最新发布通用人工智能路线图！AGI比想象中来得更快！】[blog]\r\n【OpenAI】【Sam\r\nAltman 担心“潜在的可怕的”人工智能工具以及“未来的人们如何看待我们” 】[blog]\r\n【OpenAI】【The Age of\r\nAI：拾象大模型及OpenAI投资思考】[blog]\r\n【OpenAI】【为什么ChatGPT用强化学习而非监督学习？】[blog]\r\n【OpenNLPLab】【为什么ChatGPT用强化学习而非监督学习？】[blog]，[paper]，[codel]\r\n【PWC】【ChatGPT和生成式AI的11大安全趋势】[blog]\r\n【人大】【人大最新大语言模型综述，51页全面回顾大语言模型】[paper]\r\n【清华大学】【张学工教授：AI技术前沿——从ChatGPT到更多突破】[blog]\r\n【斯坦福】【研究大语言模型反映了谁的观点？】[paper]，[code]\r\n【斯坦福】【大模型及其公平使用】[paper]\r\n【斯坦福】【构建大模型生态系统图，用于跟踪大模型的足迹】[blog]\r\n【斯坦福】【斯坦福报告：基础模型的机遇与风险】[blog]\r\n【微软】【一种新的大语言模型NLG评估框架】[paper]\r\n【微软】【低代码LLM: LLM的可视化编程】[paper]\r\n【微软】【微软提出LLMA:大型语言模型的无损加速,可以无损地加速带有引用的大型语言模型\r\n(LLM) 推理】[paper]\r\n【微软 &amp;\r\nMeta】【ART：大型语言模型的自动多步骤推理和工具使用】[paper]\r\n【EleutherAI&amp;耶鲁大学】【提出Pythia：\r\n跨越训练和扩展的大型语言模型分析套件】[paper]，[code]\r\n【博客】【ChatGPT的底层逻辑】[blog]\r\n【博客】【智慧信息的压缩：模型智能的涌现之道】[blog]\r\n【博客】【拨动大模型的琴弦｜Delta Tuning 成果登上\r\nNature子刊封面！】[blog]\r\n【博客】【大型人工智能模型中出现的不可预测的能力】[blog\r\n)]\r\n【博客】【为什么现在的大语言模型（LLM）都是Decoder-only的架构？】[blog]\r\n【博客】【大型语言模型的涌现能力】[blog]\r\n【博客】【大型语言模型成本分析】[blog]\r\n【博客】【超越ChatGPT：大模型的智能极限 】[blog]\r\n【博客】【Nature：AI模型越大越好吗? 】[blog]\r\n【博客】【一场关于ChatGPT话语权的深度思考：人类会在大模型中迷失自我吗？】[blog]，[blog译文]\r\n【博客】【马斯克强调的TruthGPT 是什么】[blog]\r\n【博客】【对话式AI搜索的技术路线猜想】[blog]\r\n【博客】【AI走过多少路，才迎来了ChatGPT？】[blog]\r\n【博客】【如何负责任地创建、发布和共享生成式 AI】[blog]\r\n【博客】【大模型时代的“Linux”生态，开启人工智能新十年】[blog]\r\n【博客】【揭秘ChatGPT背后的AI“梦之队”：90后科研“后浪”展示强大创新能力｜智谱研究报告】[blog]\r\n【博客】【In-Context Learning玩法大全 】[blog]\r\n【博客】【一文理解“上下文学习”----大语言模型突现能力】[blog]\r\n【博客】【回应吴军老师 |\r\n\"ChatGPT不算新技术革命\"】[blog]\r\n【博客】【Poe向所有开发者推出Poe\r\nAPI，以便广泛获取基于LLM的服务】[code]\r\n【博客】【【LLM系列之底座模型对比】LLaMA、Palm、GLM、BLOOM、GPT模型结构对比】[blog]\r\n【博客】【大模型实践总结】[blog]\r\n【博客】【【LLM系列之GPT】GPT（Generative\r\nPre-trained Transformer）生成式预训练模型】[blog]\r\n【博客】【【LLM系列之Tokenizer】如何科学地训练一个LLM分词器】[blog]\r\n【博客】【大模型词表扩充必备工具SentencePiece】[blog]\r\n【博客】【大模型知识&amp;推理评估基准】[blog]\r\n【博客】【万字长文说清大模型在自动驾驶领域的应用】[blog]\r\n【博客】【一文速览大语言模型在推荐系统中的应用】[blog]\r\n【博客】【NAACL &amp;\r\nACL：大模型的两种知识继承方案】[方案一]，[方案二]\r\n【博客】【a16Z：大模型应用程序的新兴架构】[中文blog]，[英文blog]\r\n【论文】【RetNet：MSRA提出Transformer全新替代大模型基础架构，推理速度8倍提升，内存占用减少70%】[blog]，[paper]\r\n【论文】【大模型微调指南：当GPU资源不足时的有效解决方案】[paper]\r\n【论文】【TaskMatrix.AI: Completing Tasks by\r\nConnecting Foundation Models with Millions of APIs 】[paper]\r\n【论文】【AnnoLLM: Making Large Language\r\nModels to Be Better Crowdsourced Annotators 】[paper]\r\n【论文】【南加州大学:大语言模型统计偏好的挑战和危险】[paper]\r\n【论文】【卡内基·梅隆大学 |\r\n语言生成模型可能造成危害：那么我们能做些什么呢？】[paper]\r\n【论文】【鹏程实验室等最新《大规模多模态预训练模型》全面综述】[paper]\r\n【论文】【预训练基础模型综合调研：从 BERT 到 ChatGPT\r\n的历史 】[paper]\r\n【论文】【洛桑联邦理工学院提出REFINER框架，用于微调大规模语言模型】[paper]\r\n【论文】【LLM-Adapters：\r\n用于大型语言模型的参数高效微调的适配器系列】[paper]\r\n【论文】【大型语言模型的涌现记忆和可预测记忆】[paper]\r\n【论文】【机器心理学：使用心理学方法研究大型语言模型中的涌现能力和行为】[paper]\r\n【论文】【Chameleon：使用大型语言模型进行即插即用的组合推理】[paper]\r\n【代码】【大型语言模型相关文献资源列表】[code]\r\nLLM数据集\r\n【COIG-PC】【智源研究院发布国内首个大规模、可商用中文开源指令数据集COIG：最大规模中文多任务指令集，上新千个中文数据集】[blog]，[paper]，[COIG-PC数据下载地址]，[COIG数据下载地址]\r\n【Instruct/Prompt\r\nTuning可用数据】【总结当前开源可用的Instruct/Prompt\r\nTuning数据】[blog]\r\n【MiniGPT-4】【GPT-4平替版：MiniGPT-4，支持图像理解和对话，现已开源】[dataset]\r\n【Multimodal\r\nC4】【多模态C4：一个开放的、10亿规模的、与文本交错的图像语料库】[paper]，[code]\r\n【Mind2Web】【Mind2Web:\r\n首个全面衡量大模型上网能力的数据集】[blog]\r\n【OpenAssistant\r\nConversations】【该数据集是一个由人工生成、人工注释的助理式对话语料库，覆盖了广泛的主题和写作风格，由\r\n161443 条消息组成，分布在 66497 个会话树中，使用 35\r\n种不同的语言。该语料库是全球众包工作的产物，涉及超过 13500\r\n名志愿者。为了证明 OpenAssistant Conversations\r\n数据集的有效性，该研究还提出了一个基于聊天的助手\r\nOpenAssistant，其可以理解任务、与第三方系统交互、动态检索信息。】[dataset]，[paper]，[code]\r\n【Panda LLM】【为了让Panda\r\nLLM在中文数据集上获得强大的性能，作者使用了强大的指令微调instruction-tuning技术，将LLaMA基础模型在五个开源的中文数据集进行混合训练，其中包括来自各种语言领域的1530万个样本，例如维基百科语料，新闻语料，百科问答语料，社区问答语料，和翻译语料。】[blog]\r\n【RedPajama】【RedPajama开源项目｜复制超过1.2万亿个令牌的LLaMA训练数据集】[原始blog]，[中文blog]，[dataset]，[code]\r\nPrompt工程\r\n【博客】【OpenAI 应用人工智能研究负责人Lilian\r\nWeng新博文：关于提示工程的介绍】[blog]\r\n【博客】【Prompt Engineering全面自动化】[blog]\r\n【博客】【ChatGPT提示示例集合】[地址]，[code]，huggingface]\r\n【博客】【深入浅出Prompt Learning要旨及常用方法】[blog]\r\n【博客】【ChatGPT火爆，最全prompt工程指南登GitHub热榜，标星4.7k！】[code]，youtube]\r\n【博客】【ChatGPT Prompt工程：设计、实践与思考】[blog]\r\n【博客】【全面的提示工程指南】[blog]\r\n【博客】【指令学习综述｜ChatGPT背后的指令学习是什么】[blog]，[paper]\r\n【博客】【免费教你提示工程，全中文教学】[主页]，[code]\r\n【博客】【吴恩达Prompt课程笔记】[主页]\r\n【博客】【ChatGPT使用进阶，Prompt工程】[blog]\r\n【论文】【面向大型语言模型的提升提示集成】[paper]\r\n【论文】【DTG：一种简单有效的Prompt方法，激发大模型思考判断能力！】[blog]\r\nAGI开源工具&amp;博客&amp;论文\r\n【工具】【Google发布统计深度学习框架平台：OpenXLA】[blog]\r\n【博客】【AGI的火花一作Sébastien\r\nBubeck演讲万字全文】[blog]\r\n【博客】【AGI通用智能发展的思考：是否存在足够通用的处理器？】[blog]\r\n【论文】【OpenAGI:当大语言模型遇到领域专家】[paper]，[code]\r\n文本生成\r\nChatGPT\r\n从GPT3到ChatGPT模型的发展路线图\r\n\r\n\r\nChatGPT_family\r\n\r\nChatGPT 应用篇\r\n【58】【从 GPT 到 ChatGPT 的演进与应用思考】[blog]\r\n【MIT &amp; 哈佛大学 】【语言模型可以预测公众舆论\r\n】[paper]\r\n【中科院】【ChatGPT助力芯片，传统\r\nEDA如何演变成智能EDA】[blog]\r\n【微软】【《ChatGPT机器人:设计原则和模型能力》论文\r\n】[paper]\r\n【微软】【各种环境下的ChatGPT赋能长步机器人控制：\r\n一个案例的应用 】[paper]，[code]\r\n【博客】【ChatGPT获得了「Wolfram」超能力】[blog]\r\n【博客】【OpenAI开发Plugin将 ChatGPT\r\n连接到互联网】[blog]\r\n【博客】【ChatAug：利用ChatGPT进行文本数据增强】[paper]\r\n【博客】【ChatGPT 是数据隐私的另一个障碍吗】[blog]\r\n【博客】【基于ChatGPT的数据增强方法：ChatAug和AugGPT】[blog]\r\n【博客】【Character.AI\r\n在ChatGPT基础上加入个性化、UGC两大武器，有比 ChatGPT\r\n更丰富的使用场景】[blog]\r\n【博客】【让ChatGPT可以语音交互】[blog]\r\n【博客】【“ChatGPT们”的淘金时代】[blog]\r\n【博客】【70 款 ChatGPT 插件评测（含样例分析）】[blog]\r\n【论文】【人大提出WebBrain：NLP新任务，通过网络数据的挖掘生成真实文章】[paper]，[code]\r\n【医疗】【ChatGPT爆火带来思考：医学界或将迎来与AI融合的奇点？】[blog]\r\n【教育】【论ChatGPT大语言模型在教育中的机遇与挑战\r\n】[blog]\r\n【投资】【ChatGPT在投资研究领域的应用初探及原理分析】[blog]\r\n【软件】【OpenAI总裁Greg\r\nBrockman转发｜一种编译语言的调试器，利用ChatGPT旨在增强您使用GDB进行调试体验】[code]\r\n【软件】【不必排队等 OpenAI Plugins，OpenBMB\r\n开源大模型工具学习引擎】[blog]\r\n【其他】【分析了ChatGPT技术以及落地应用场景 】[blog]\r\nChatGPT 工具篇\r\n【工具】【ChatGPT 应用汇总及操作手册】[blog]\r\n【工具】【ChatGPT提示和技巧速查手册】[blog]\r\n【工具】【非常全面的ChatGPT、LLM相关资源整理分享】[code]\r\n【工具】【ChatGPT超全面课程】[blog]\r\n【工具】【BloombergGPT: A Large Language Model for\r\nFinance】[paper]\r\n【工具】【ChatPDF：一键上传PDF文件即可解读 】[blog]，[试用地址]\r\n【工具】【ChatWeb：可爬取网页正文，并根据正文回答问题\r\n】[code]\r\n【工具】【chatgpt_academic：中科院基于 ChatGPT\r\n专属定制的学术研究及日常开发工具】[blog]，[code]，[demo]\r\n【工具】【Einstein GPT：SaaS 行业巨头 Salesforce\r\n宣布与 OpenAI 合作，推出 Einstein\r\nGPT，这是全球首个用于客户关系管理（CRM）的生成式 AI 产品 】[Einstein\r\nGPT地址]，[试用地址]\r\n【工具】【HuggingGPT: Solving AI Tasks with ChatGPT\r\nand its Friends in HuggingFace 】[paper]\r\n【工具】【ImpressionGPT：\r\n利用ChatGPT对放射科报告进行总结的迭代优化框架】[paper]\r\n【工具】【OpenGpt：创建ChatGPT小应用的AI平台】[官网]，[code]\r\n【工具】【TagGPT：腾讯提出零样本多模态标签的大语言模型TagGPT】[paper]，[code]\r\n【工具】【Visual ChatGPT:\r\n在视觉模型加持下的ChatGPT，聊天生图全拿捏了。】[paper]\r\n【工具】【NetGPT：用于网络流量的生成预训练Transformer模型】[paper]\r\nChatGPT 技术篇\r\n【符尧】【深度拆解GPT-3.5能力起源】[原文blog]，[译文blog]\r\n【知乎】【ChatGPT发展历程、原理、技术架构详解和产业未来】[blog]\r\n【斯坦福】【82页PPT ！最新ChatGPT: 提示学习,\r\n指导微调和RLHF 】[blog]，[提取码:chat]\r\n【微软】【让天下没有难训练的大模型，微软亚洲研究院开源TorchScale\r\n】[code]\r\n【亚马逊 】【他们提出了包含视觉特征的\r\nMultimodal-CoT，该架构在参数量小于 10 亿的情况下，在 ScienceQA\r\n基准测试中，比 GPT-3.5 高出 16 个百分点 】[paper]，[code]\r\n【OpenBMB】【Nature ：生成式 AI 的前景与风险】[blog]\r\n【博客】【万字长文解读：从Transformer到ChatGPT，通用人工智能曙光初现】[blog]\r\n【博客】ChatGPT_Inference_Cost\r\n【博客】ChatGPT_Official_API_Learning\r\n【博客】ChatGPT_Parameter_is_not_175B\r\n【博客】ChatGPT_Road_Map_from_yao.fu\r\n【博客】Lessons_Learned_from_ChatGPT_Recurrence\r\n【博客】LLM_Pre-training_Guide（Bloom-175B）\r\n【博客】The_guide_of_training_LLM\r\n【博客】【AI芯片制造商Cerebras发布7个基于GPT的大语言模型，现已开源】[官网地址\r\n)]，[GPT地址]，[Hugging Face地址]\r\n【博客】【大模型论文周报丨GPT-4发布，谷歌开放PaLM\r\nAPI，斯坦福7B开源模型Alpaca媲美GPT-3.5】[blog]\r\n【博客】【LLaMA模型Meta版泄露，GitHub获8K星】[blog]\r\n【博客】【ChatGPT or Grammarly? Evaluating ChatGPT\r\non Grammatical Error Correction Benchmark 】[paper]\r\n【博客】【打造中国版ChatGPT，国内哪家实力最强】[blog]\r\n【博客】【复旦大学邱锡鹏教授解读ChatGPT】[blog]\r\n【博客】【万字长文:可能是全网最晚的ChatGPT技术总结\r\n】[blog]\r\n【博客】【ChatGPT作为知识库问答系统的问答能力评测\r\n】[blog]\r\n【博客】【ChatGPT作者John\r\nShulman：我们成功的秘密武器】[blog]，[blog译文]\r\n【博客】【ChatGPT 是数据隐私的另一个障碍吗】[blog]\r\n【博客】【Hugging Face 每周速递: ChatGPT API\r\n怎么用？我们帮你搭好页面了 】[blog]\r\n【博客】【复旦大学教授肖仰华：ChatGPT\r\n浪潮下，面向大模型如何做数据治理？】[blog]\r\n【博客】【腾讯在ChatGPT的布局】[blog]\r\n【博客】【浅析ChatGPT：历史沿革、应用现状及前景展望】[blog]\r\n【博客】【ChatGPT 背后的“功臣”——人类反馈强化学习RLHF\r\n技术详解】[blog]\r\n【博客】【万字长文解析！复现和使用GPT-3/ChatGPT，你所应该知道的】[blog]\r\n【博客】【想训练ChatGPT？得先弄明白Reward\r\nModel怎么训（附源码） 】[blog]\r\n【博客】【ChatGPT核心技术：强化学习PPO算法】[blog]\r\n【博客】【解读 ChatGPT\r\n背后的技术重点：RLHF、IFT、CoT、红蓝对抗】[blog]\r\n【博客】【OpenAI ChatGPT Code Interpreter入门】[blog]\r\n【伦理】【加拿大魁北克大学教授详述：我们该拿ChatGPT怎么办？】[blog]\r\n【论文】【AIGC时代的ChatGPT全面综述】[paper]\r\n【论文】【ChatGPT is a Knowledgeable but\r\nInexperienced Solver: An Investigation of Commonsense Problem in Large\r\nLanguage Models】[paper]\r\n【论文】【GPT-3 和 GPT-3.5 系列模型的全面分析】[paper]\r\n【论文】【ChatGPT Outperforms Crowd-Workers for\r\nText-Annotation Tasks】[paper]\r\n【论文】【微软&amp;佐治亚理工学院 |\r\nAdaLoRA：自适应预算分配以实现参数有效的微调】[paper]，[code]\r\n【论文】【微软 | 大型语言模型的语境忠实提示法】[paper]\r\n【论文】【KAUST |\r\nChatGPT问，BLIP-2回答模型：面向丰富的视觉描述的自动提问】[paper]，[code]\r\n【论文】【ChatGPT真的可以取代知识图谱问答吗？ 】[paper]，[paper翻译]\r\n【论文】【Meta &amp;\r\n斯坦福大学推出FlexGen：用单个GPU进行大型语言模型的高吞吐量生成性推理】[paper]，[code]\r\n【论文】【ChatGPT破圈的「秘密武器」：详解RLHF如何影响人类社会！\r\n】[paper]，[blog]\r\n【论文】【探讨ChatGPT在对抗攻击和分布外泛化下的鲁棒性】[paper]，[code]\r\n【论文】【复旦清华联合顶刊发文｜ChatGPT：潜力、前景和局限\r\n】[blog]，[paper]\r\n【论文】【引导ChatGPT不要输出有害信息】[paper]\r\n【论文】【Junnan\r\nLi大佬发表最新多模态的杰作BLIP2】[paper]，[code]，[blog]\r\n【论文】【Instruction Tuning：无/少样本学习新范式\r\n】[paper]，[code]\r\n【论文】【GPTScore：一种新的评估语言模型方法】[paper]，[code]\r\n【论文】【ChatGPT内核：InstructGPT，基于反馈指令的PPO强化学习】[blog]，[B站]\r\n【论文】【Fine-tune-CoT：小模型也能做推理，完美逆袭大模型\r\n】[paper]，[code]\r\n【论文】【ChatGPT的潜力解锁：自然语言处理中应用、优势、限制和未来方向的全面探索】[paper]\r\n【论文】【阿里巴巴&amp;清华大学|大型语言模型在算术任务中的表现如何？】[paper]，[code]\r\n【代码】【本科生60行代码教你手搓GPT大模型 】[code]\r\nGPT4\r\nGPT4 官方文档\r\n【博客】【GPT4_System_Card中文翻译】[blog]\r\n【博客】【GPT4_Technical_Report中文翻译】[blog]\r\nGPT4 博客篇\r\n【博客】【【万字长文】GPT-4秘密泄露！所有的信息都在这里！从GPT-4\r\n架构、基础设施、训练数据集、成本、视觉到MoE！】[blog]，[原blog]\r\n【纽约时报】【GPT-4 令人印象深刻但仍在 10\r\n个方面具有缺陷】[blog]\r\n【Open AI】【多模态大模型GPT-4的新突破】[blog]\r\n【OpenAI】【重磅发布GPT-4】[blog]\r\n【OpenAI】【GPT-4 创造者 Ilya Sutskever 谈 AI 幻觉和\r\nAI 民主】[blog]\r\n【OpenAI】【GPT-4创造者：第二次改变AI浪潮的方向】[blog]\r\n【OpenAI】【当GPT-4进入北京市2022高考考场能有什么表现？】[blog]\r\n【博客】GPT4技术细节\r\n【博客】GPT4技术关键点总结\r\n【博客】GPT4和ChatGPT的效果对比\r\n【博客】The\r\nUltimate GPT-4 Guide\r\nGPT4 论文篇\r\n【微软】【用GPT-4进行指令调优】[paper]，[code]\r\n【论文】【点燃通用人工智能的火花：GPT-4的早期实验】[原始paper]，[中文版paper]，[blog]\r\n【论文】【GPT4All：用GPT-3.5-Turbo的大规模数据提炼训练一个助理式聊天机器人】[paper]，[code]\r\n【论文】【美国东北大学：可以通过要求GPT4反思“你为什么错了？”来提高30%的性能】[paper]，[code]\r\n【论文】【对ChatGPT/GPT-4研究的总结以及对大型语言模型未来的展望】[paper]\r\n【论文】【评估日本医疗执照考试的GPT-4和ChatGPT】[paper]\r\n【论文】【Amazon |\r\n深入研究LLMs与AutoGPT的结合：揭示出GPT-4惊人的人类决策能力！】[blog]，[paper]，[code]\r\nAnima\r\n【33B QLoRA大语言模型Anima的性能超越了对比的所有的中文开源模型。】[blog]，[code]，[model]\r\nBard\r\n【谷歌再次开放Bard访问权，向着ChatGPT发起再一次攻击】[报名地址\r\n)]，[blog]，[theverge]\r\nBaize\r\n【用ChatGPT训练羊驼：「Baize」开源，轻松构建专属模型】[blog]，[paper]，[code]，[demo]\r\nbaichuan以及扩展\r\n【baichuan-7b】【王小川大模型首亮相！70亿参数霸榜，清北抢先用｜独家专访】[blog]，[Hugging\r\nFace]，[code]，[Model\r\nScope]，[C-EVAL]\r\n【firefly-baichuan-7b-qlora-sft】[使用Firefly项目中的QLoRA训练流程，在moss-003-sft-data百万多轮指令数据上进行了指令微调baichuan-7b模型]，[blog]，[Hugging\r\nFace model]，[code]，[Model\r\nScope]，[C-EVAL]\r\nBLOOM\r\n【【LLM系列之BLOOM】BLOOM: 多语言大模型】[blog]，[paper]，[code]，[huggingface]\r\nBiomedGPT\r\n【BiomedGPT: 统一通用的生物医学生成式预训练Transformer】[paper]\r\nClaude\r\n【ChatGPT最强竞品Claude今日开放API】[产品地址]，[申请地址]，[API说明]，[blog]，[Claude支持100k上下文]，[Claude2发布]\r\nClaude 2\r\n【ChatGPT最强竞品Claude2来了】[blog]\r\nChatGLM-6B以及扩展\r\n【ChatGLM：千亿基座的对话模型开启内测 ⸺对应单卡版本开源】[blog]，[code]\r\n【chatglm+langchain+互联网，你可以将大模型接入网络了】[blog]，[code]\r\n【Chinese-LangChain】【基于ChatGLM-6b+langchain实现本地化知识库检索与智能答案生成】[code]，[blog]\r\n【ChatGLM_multi_gpu_zero_Tuning：简单高效实现多卡微调大模型】[code]\r\n【浅尝prompt咒语设计：one-shot微调chatglm-6b实践信息抽取】[blog]\r\n【ChatGLM-6B模型结构组件源码阅读】[blog]\r\n【基于1万亿token开源大模型Falcon，超越650亿的LLaMA，可商用】[blog1]，[blog2]\r\nChatYuan\r\n【ChatYuan：基于PromptCLUE-large的中文对话开源大模型】[blog]\r\nCopilot X\r\n【GitHub Copilot X编辑器发布，大大提升编码速度】[blog]\r\nColossalAI\r\n【穷孩子如何体验ColossalAI SFT（Colab篇）】[blog]\r\nCPM-Bee\r\n【中文基座模型CPM-Bee开源了】[blog]，[code]，[HuggingFace]\r\nChatDB\r\n【清华大学和北京智源人工智能研究院的研究者们提出了ChatDB：用数据库作为符号性记忆模块来增强大语言模型】[blog]，[paper]，[主页]，[code]\r\nDolly\r\n【声称它\r\n\"像ChatGPT一样神奇\"，但只需要使用一台机器在不到三个小时的时间里训练的数据少得多。】[blog]，[Databricks Inc地址]\r\nDolly2.0\r\n【Databricks的dolly-v2-12b，是一个在Databricks机器学习平台上训练的指令跟随型大型语言模型】[blog_en]，[blog_zh]\r\nDeepSpeed-Chat\r\n【DeepSpeed对话：易于使用、快速而实惠的RLHF训练，在各种规模下训练ChatGPT模型】[code]，[blog]\r\nFrugalGPT\r\n【斯坦福提出FrugalGPT｜性能媲美GPT4，成本降低98%】[paper]，[blog]\r\nGPT3.5\r\n【GPT3.5试用地址 】[试用地址]\r\nJittorLLMs\r\n【笔记本没有显卡也能跑大模型，具有高性能、配置要求低、中文支持好、可移植等特点】[code]\r\nLLM as Controller\r\n【LLM as Controller—无限拓展LLM的能力边界】[blog]\r\nMetaGPT\r\n【MetaGPT：多角色元编程框架】[code]\r\nMiniGPT-4\r\n【类似GPT-4图像理解与对话能力的AI大模型，已开源】[主页]，[paper]，[code]，[video]，[dataset]，[Demo]，[Demo1]，[Demo2]，[Demo3]，[Demo4]\r\nMOSS\r\n【FudanNLP团队最新成果，借助RLHF实现人类对齐的MOSS-RLHF来了】[blog]，[code]，[测试链接]，[模型权重]，[数据集]\r\nOpenChatKit\r\n【ChatGPT开源平替OpenChatKit：参数量200亿，在4300万条指令上微调而成】[blog]，[code]，[技术报告]\r\nOpenAssistant\r\n【ChatGPT全球最大开源平替OpenAssistant，基于Pythia和LLaMA微调而来，主要用于训练人类标注的数据，支持35种语言，免费可用RLHF数据】[官网]，[paper]，[code]，[dataset]，[youtube]\r\nWebCPM\r\n【首个联网支持中文问答开源模型WebCPM】[paper]，[code]，[blog]\r\nLLaMA以及扩展\r\n【LLaMA】【Meta开放小模型LLaMA，性能超过GPT-3】[paper]，[code]，[blog1]，[blog2]，[详聊LLaMA大模型的技术细节]\r\n【LLaMA 2】【LLaMA 2技术细节详细介绍！】[blog]，[在 Hugging Face\r\n上玩转LLaMA 2]，[伯克利AI博士详解Llama\r\n2的技术细节]，[Chinese-LlaMA2]\r\n【llama2.c】【OpenAI联创Karpathy爱上羊驼：纯C代码实现婴儿Llama2，MacBook可运行，已揽1.6k星】[blog]，[code]\r\n【LLaMA评测】[blog]\r\n【Alpaca】【斯坦福发布了一个由LLaMA\r\n7B微调的模型Alpaca（羊驼），训练3小时，性能比肩GPT-3.5】[blog]，[官网]，[model]，[code]\r\n【Alpaca-CoT】【Alpaca-CoT：多接口统一的轻量级LLM指令微调平台】[code]，[官网]\r\n【BiLLa】【BiLLa 是开源的推理能力增强的中英双语\r\nLLaMA 模型】[blog]，[code]\r\n【CaMA】【一种支持中英语言的LLaMA模型】[code]\r\n【ChatLLaMA】【初创公司 Nebuly\r\nAI在LLaMA基础上加入RLHF 开源 ChatLLaMA 训练方法】[code]\r\n【ColossalAI】【完整复现ChatGPT全流程】[code]\r\n【ColossalChat】【用于克隆 ChatGPT 和完整 RLHF\r\n管道的开源解决方案】[code]，[blog]\r\n【CAMEL】【从LLaMA衍生并适应临床的模型】[code]，[blog]\r\n【草本（原华驼）】【让LLaMA模型成为中医专家】[paper]，[code]，[blog1]，[blog2]\r\n【DB-GPT】【基于vicuna-13b和FastChat的开源实验项目】[code]\r\n【DeepSpeed-Chat】【最强ChatGPT训练框架，一键完成RLHF训练！\r\n】[code]，[blog]\r\n【ExpertLLaMA】【一个使用ExpertPrompting构建的开源聊天机器人，其能力达到ChatGPT的96%。】[code]\r\n【FreedomGPT】【FreedomGPT使用Electron 和\r\nReact构建，它是一个桌面应用程序，允许用户在他们的本地机器上运行LLaMA。】[官网地址]\r\n【FLAN】【【LLM系列之FLAN】Scaling\r\nInstruction-Finetuned Language Models】[blog]\r\n【GoGPT/GoGPT2】【基于Llama/Llama\r\n2训练的底座大模型,再扩充词表+继续预训练】[GoGPT code]，[GoGPT2 code]\r\n【Koala】【加州大学BAIR团队提出Koala：学术研究的对话模型】[blog_zh]，[blog_en]\r\n【LLaMA-Adapter】【LLaMA-Adapter，一种用于微调指令遵循LLaMA模型的轻量级自适应方法，使用Stanford\r\nAlpaca提供的 52K 数据。】[paper]，[code]\r\n【LaVIN】【MMA方案让羊驼模型实现多模态：训练时间减少71.4%，成本节省99.9%】[paper]，[code]，[blog]\r\n【lit-llama】【基于nanoGPT的LLaMA语言模型，支持量化、LoRA微调和预训练】[code]\r\n【LlamaIndex】【面向QA 系统的全新文档摘要索引】[blog]\r\n【llama.cpp】【量化130亿参数LLaMA模型的llama.cpp，推理仅需4GB内存】[blog]\r\n【llama.cpp优化版】【Edge AI 变得更快|在 C/C++\r\n中移植 Facebook 的 LLaMA 模型】[blog]\r\n【LIMA】【使用 LoRA 技术对 LLaMA 65B\r\n大模型进行微调及推理】[blog]\r\n【PaLM】【【LLM系列之PaLM】PaLM: Scaling Language\r\nModeling with Pathways】[blog]\r\n【StackLLaMA】【使用 RLHF 训练 LLaMA 的实践指南】[blog_zh]，[blog_en]\r\n【Vicuna】【通过对从ShareGPT收集的用户共享对话进行微调的LLaMA训练，Vicuna-13B达到了OpenAI\r\nChatGPT和Google Bard 90%*以上的质量 】[Vicuna官网地址]，[blog]\r\n图像、视频生成\r\n【博客】【Genmo\r\nChat】【这是一款创造性的copilot，使用GPT-4和一大套生成人工智能工具创建并编辑您需要的任何视频或图像。\r\n】[blog]\r\n【博客】【BlenderGPT】【一款基于GPT-4的扩展程序BlenderGPT开源，这是一个由GPT3/4驱动的全能AI编辑助手，为Blender提供支持\r\n】[code]\r\n【博客】【Firefly】【Adobe制造了一个人工智能图像生成器--并表示它没有窃取艺术家的作品来做这件事\r\n】[blog]\r\n【博客】【Bing Image Creator】【微软推出Bing Image\r\nCreator，用户可根据文本提示创建图片】[blog]\r\n【博客】【Hugging Face\r\n现已支持使用达摩院text-to-video模型从文本生成视频】[模型地址]\r\n【论文】【最新女娲大模型，中科院提出NUWA-XL：扩散模型中的扩散，生成超长视频】[paper]，[blog]\r\n【论文】【艾伦AI研究院 &amp; 华盛顿大学 |\r\nCHAMPAGNE：从大规模的网络视频中学习真实世界的对话】[paper]，[code]\r\n【论文】【用AI直接复现你在想什么，Stable\r\nDiffusion逼真复现图像】[paper]，[blog]\r\n【论文】【Stable\r\nDiffusion公司新作Gen-1：基于扩散模型的视频合成新模型，加特效杠杠的！】[paper]，[site]\r\n【论文】【使用Diffusers 实现 ControlNet\r\n高速推理】[blog]\r\n【论文】【文生图引入ControlNet，深度、边缘信息全能复用\r\n】[paper]，[code]\r\n【论文】【ChatGPT｜可用于AI绘画，效果飞升47% 】[paper]\r\n【论文】【智源研究院提出SegGPT：\r\n一个用于分割上下文中所有事物的通用模型】[paper]\r\n【论文】【OpenAI开源新模型代码库Consistency\r\nModels，无需对抗训练即可快速获得高质量样本】[paper]，[code]，[blog]\r\n【可控图文大模型】【伯克利&amp;微软｜用GPT-4进行可控的文本-图像生成】[paper]\r\n代码生成\r\n【综述】【代码大模型综述：中科院和MSRA调研27个LLMs，并给出5个有趣挑战】[blog]，[paper]，[项目主页]\r\n【博客】【GPT-Engineer｜提需求即可生成整个代码库，已20K星】[blog]，[code]\r\n【博客】【StarCoder: 最先进的代码大模型】[blog]\r\n【论文】【北京大学：具有大语言模型的自我规划代码生成】[paper]\r\n【论文】【谷歌提出Self-Debugging:教导大型语言模型进行自我调试】[paper]\r\n【论文】【通过自我改进实现更好的代码语言模型，显著提高模型生成任务的性能】[paper]\r\n【论文】【Baldur:\r\n基于大型语言模型的完全证明生成与修复】[paper]\r\n【论文】【CodeGeeX: A Pre-Trained Model for Code\r\nGeneration with Multilingual Evaluations on HumanEval-X 】[paper]，[code]\r\n【论文】【代码模型 CodeGeeX2-6B\r\n开源，最低6GB显存，性能优于StarCoder】[blog]，[code]\r\n【论文】【CodeT5+：非常灵活的、面向代码理解和生成的开放大型代码语言模型】[paper]\r\n【工具】【Cursor：一个集成了 GPT-4\r\n的国内直接可以访问的，优秀而强大的免费代码生成器，可以帮助你快速编写、编辑和讨论代码。】[官网地址]\r\n【论文】【MIT最新研究：利用大预言模型生成Code】[paper]，[code]，[项目网址]\r\n【论文】【MathPrompter:\r\n基于大型语言模型的数学推理】[paper]\r\n【论文】【MIT最新研究：利用大语言模型生成Code】[paper]，[code]，[官网地址]\r\n语音生成\r\n【论文】【Meta AI研究者推出MUSICGEN】[paper]，[blog]，[demo]\r\n【论文】【文字、图片一键生成逼真音效，音频界AIGC来了】[paper]，[code]\r\n【论文】【音乐可视化｜利用大型语言模型和文本到图像模型帮助生成「音乐迪斯科」】[paper]，[blog]\r\n【论文】【MetaAI发布第一个生成的人工智能语音模型Voicebox】[blog]，[paper]\r\n多模态生成\r\n【BLIP-2】【高效训练多模态大模型（BLIP-2）】[paper]，[code]，[demo]，[doc]，[fine-tuing]，[hugging face\r\nspaces]\r\n【VisCPM】【SOTA 开源中文多模态大模型】[blog]，[code]\r\n【HuggingFace Transformers\r\nAgents】【一键控制10万多个AI模型，HuggingFace给类ChatGPT模型们做了个「APP\r\nStore」】[demo]，[blog]\r\n【LLaVA】【熔岩羊驼LLaVA来了：像GPT-4一样可以看图聊天，无需邀请码，在线可玩】[paper]，[introduce]\r\n【UniDiffuser】【清华朱军团队开源UniDiffuser：首个基于Transformer的多模态扩散大模型！文图互生、改写全拿下！】[paper]，[code]\r\n【Video-LLaMA】【人机视频对话｜Video-LLaMA多模态框架，使大型语言模型具备了理解视频内容的能力】[paper]\r\n【X-LLM】【多模态语言训练大模型】[项目地址]，[paper]\r\n🗂 参考链接来源\r\n\r\nAwesome-AIGC-Tutorials/\r\nhttps://github.com/gongminmin/awesome-aigc\r\nhttps://github.com/Moonvy/OpenPromptStudio\r\nhttps://github.com/wshzd/Awesome-AIGC/tree/main\r\n\r\n🤝 友情链接\r\n\r\nWayToAGI\r\n\r\nWaytoAGI.com\r\n是最全面的中文AIGC资源知识库，包括最新AI动态、提示词、学习指南等，长期保持活跃更新。\r\n\r\nAwesome\r\nTool Learning\r\n\r\nAwesome Tool Learning\r\n提供丰富的关于工具学习的资源，包括论文、框架和应用程序。\r\n\r\nAwesome\r\nDomain LLM\r\n\r\n这个GitHub仓库是一个汇集和整理了自ChatGPT等大语言模型出现后，各种垂直领域开源模型、数据集和评测基准的列表，同时鼓励大家为其贡献未收录的资源。\r\n\r\n\r\n🗂声明\r\n以上部分资料来自网络整理，供大家学习参考，如有侵权，麻烦联系我删除！\r\n","slug":"aigcpaper","date":"2023-10-20T10:08:21.379Z","categories_index":"","tags_index":"","author_index":"AIGC生成式人工智能开发者"},{"id":"28c7d3817ed8352eb1372ac636977eb8","title":"人工智能科技与文献网","content":"\r\n\r\nimage\r\n\r\nAI新闻网：https://www.marktechpost.com/\r\n算法核心基础与AI模型设计【我的CSDN技术博客】：https://blog.csdn.net/weixin_41194129/category_11362509.html\r\nAI算法学习社区:\r\nhttps://github.com/Algorithm-learning-community-for-python\r\nYOLO系列资料汇总：https://github.com/KangChou/Cver4s\r\nNVIDIA-CUDA编程:https://github.com/KangChou/deepcv_project_demo/tree/main/CUDA%E7%BC%96%E7%A8%8B\r\n自动驾驶点云技术:\r\nhttps://github.com/KangChou/deepcv_project_demo/tree/main/CVPR/point-cloud\r\n计算机视觉技术：\r\nhttps://github.com/KangChou/deepcv_project_demo/tree/main/CVPR/visual\r\n\r\n\r\njpeg\r\n\r\n专业的聊天机器人: https://github.com/salesforce/Converse\r\n基于开源GPT2.0的初代创作型人工智能 |\r\n可扩展、可进化:https://github.com/EssayKillerBrain/EssayKiller_V2\r\n高质量中文预训练模型集合:https://github.com/CLUEbenchmark/CLUEPretrainedModels\r\n自然语言基础模型:https://github.com/lpty/nlp_base\r\nBERT模型从训练到部署全流程:https://github.com/xmxoxo/BERT-train2deploy\r\n中文BERT-wwm系列模型:https://github.com/ymcui/Chinese-BERT-wwm\r\n深度学习入门教程, 优秀文章:\r\nhttps://github.com/Mikoto10032/DeepLearning\r\n3D视觉、VSLAM、计算机视觉的干货资料:\r\nhttps://github.com/qxiaofan/awesome_3d_slam_resources\r\n自动驾驶系统实现:https://github.com/sunmiaozju/smartcar\r\n身份证自动识别,银行卡识别,驾驶证识别,行驶证识别：https://github.com/wenchaosong/OCR_identify\r\nMVision 机器视觉 机器视觉：https://github.com/Ewenwan/MVision\r\nComputer Vision: Algorithms and\r\nApplications：https://szeliski.org/Book/\r\n自动驾驶的激光雷达点云处理:\r\nhttps://github.com/beedotkiran/Lidar_For_AD_references\r\n动态语义SLAM\r\n目标检测+VSLAM+光流/多视角几何动态物体检测+octomap地图+目标数据库:https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic\r\n基于视频的目标检测算法研究:https://github.com/guanfuchen/video_obj\r\nTensorRT-7 Network: https://github.com/Syencil/tensorRT\r\nC++ TensorRT-CenterNet:\r\nhttps://github.com/CaoWGG/TensorRT-CenterNet\r\nyolox-deepsort:https://github.com/Sharpiless/yolox-deepsort\r\n\r\n\r\njpeg\r\n\r\nBirdNet+：LiDAR 鸟瞰图中的端到端 3D\r\n对象检测:https://github.com/AlejandroBarrera/birdnet2\r\n关于nuScenes\r\n数据集的开发套件:https://github.com/nutonomy/nuscenes-devkit\r\nA robust LiDAR Odometry and Mapping (LOAM) package for\r\nLivox-LiDAR:https://github.com/hku-mars/loam_livox\r\n激光雷达论文：https://arxiv.org/search/?query=+LiDAR&amp;searchtype=all&amp;source=header\r\n使用CUDA PCL\r\n加速Jetson的点云处理：https://developer.nvidia.com/zh-cn/blog/cuda-pcl-1-0-jetson/\r\nPCT: Point Cloud Transformer: https://github.com/MenghaoGuo/PCT\r\n\r\n\r\ntest\r\n\r\n","slug":"ai1","date":"2023-10-20T09:55:43.516Z","categories_index":"人工智能","tags_index":"人工智能","author_index":"AIGC生成式人工智能开发者"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\n\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","slug":"hello-world","date":"2023-10-19T17:04:08.208Z","categories_index":"","tags_index":"","author_index":"AIGC生成式人工智能开发者"},{"id":"bbbddb6726a8dd0e4e94bb6dc760d764","title":"开源书籍与源码","content":"书籍下载链接：https://github.com/KangChou/AI-Technology-and-Algorithm-Programming\r\n\r\n\r\n\r\nimage-20200728231437641\r\n\r\n数学分析上下册\r\n\r\npython自动化操作\r\n\r\npython从入门让到实践\r\n\r\n数字信号处理\r\n\r\n《智能问答与深度学习》\r\nhttps://github.com/l11x0m7/book-of-qna-code\r\n人工智能实践：Tensorflow笔记\r\nhttps://github.com/jlff/tf2_notes\r\n源码下载链接：https://pan.baidu.com/s/19XC28Hz_TwnSQeuVifg1UQ\r\n提取码：mocm\r\n数据科学/人工智能比赛解决方案聚合:https://github.com/apachecn/awesome-data-comp-solution\r\n《学习学习与计算机视觉》配套代码:\r\nhttps://github.com/frombeijingwithlove/dlcv_for_beginners\r\n《算法导论》的C++实现\"代码：https://github.com/huaxz1986/cplusplus-_Implementation_Of_Introduction_to_Algorithms\r\nhttp://www.huaxiaozhuan.com/\r\n《Unix\r\n环境高级编程第三版》笔记：https://github.com/huaxz1986/APUE_notes\r\n算法工程师(人工智能cv方向)面试问题及相关资料的网站收集:https://github.com/lcylmhlcy/Awesome-algorithm-interview\r\n","slug":"books","date":"2021-04-05T14:57:06.000Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"AIGC生成式人工智能开发者"},{"id":"12637e04fdd5cb76f4fc78ed59ef0d15","title":"NLP技术汇总","content":"Deep learning speech learning library \r\n\r\n一个轻量级、简单易用的 RNN 唤醒词监听器: https://github.com/MycroftAI/mycroft-precise\r\nzh:http://fancyerii.github.io/books/mycroft-precise/\r\n基于树莓派的人工智能小车，实现识别、提示、智能旅游线路、离线图像: https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi\r\n中文NLP数据集:https://github.com/CLUEbenchmark/CLUEDatasetSearch\r\n模型：https://github.com/CLUEbenchmark/CLUE\r\n中文 NLP 资源精选列表 中文自然语言处理相关资料: https://github.com/crownpku/Awesome-Chinese-NLP\r\n视觉聊天机器人:https://paperswithcode.com/paper/visual-dialog\r\nBert/Transformer模型压缩与优化加速:\r\nhttps://blog.csdn.net/nature553863/article/details/120292394：\r\n可以压缩 BERT\r\n的所有方式：http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html\r\nhttps://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html\r\nBERT轻量化探索—模型剪枝（BERT\r\nPruning）—Rasa维度剪枝:https://blog.csdn.net/ai_1046067944/article/details/103609152\r\n压缩 BERT\r\n以加快预测速度:https://rasa.com/blog/compressing-bert-for-faster-prediction-2/\r\n论文综述与BERT相关最新论文:https://github.com/tomohideshibata/BERT-related-papers\r\n中文自然语言排行榜及论文查询:https://www.cluebenchmarks.com/index.html\r\n计算语言学国际会议论文集:https://aclanthology.org/volumes/2020.coling-main/\r\n计算语言学协会第 58\r\n届年会论文集:https://aclanthology.org/volumes/2020.acl-main/\r\n计算语言学2协会2021年会论文搜集：https://aclanthology.org/events/acl-2021/\r\n中文BERT全词掩蔽预训练（中文BERT-wwm系列模型）https://github.com/ymcui/Chinese-BERT-wwm\r\n一个大规模的中文跨领域面向任务的对话数据集:https://github.com/thu-coai/CrossWOZ\r\n关于ConvLab-2：用于构建、评估和诊断对话系统的开源工具包（支持中文）：https://github.com/thu-coai/ConvLab-2\r\n视觉和语言预训练模型 (VL-PTM) 的最新进展(语音视觉融合):https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers\r\n深度学习和自然语言处理阅读清单:https://github.com/IsaacChanghau/DL-NLP-Readings\r\n视觉问答\r\n(VQA)（图像/视频问答）、视觉问题生成、视觉对话、视觉常识推理和相关领域的精选列表：https://github.com/jokieleung/awesome-visual-question-answering\r\n","slug":"nlp","date":"2021-04-05T14:57:06.000Z","categories_index":"人工智能","tags_index":"核心算法","author_index":"AIGC生成式人工智能开发者"},{"id":"5abf36706a69cfb501ed7b1f1badf544","title":"值得你阅读的Hexo个人博客搭建：不用购买服务器，不用购买域名，不要钱，不用敲代码等等，是的，你没有看错，快来转载学习吧！","content":"本文的原文在我的微信公众号，欢迎点击下面蓝色字体链接进入主页\r\n值得你阅读的Hexo个人博客搭建\r\n\r\n\r\nimage\r\n\r\nHexo快速搭建个人博客（2019/10/22更新）\r\n\r\n使用到的工具\r\n（本教程统一在Windows系统下搭建）\r\nNode.js、Hexo、Git、Github账号、Sublime Text3\r\n请自行注册一个Github账号\r\n最后的部署到网上的博客展示\r\n\r\n\r\nimage\r\n\r\n文章目录：\r\n前言\r\n值得你知道的话\r\n一、从创建到部署博客\r\n*****二、博客的网页主题*****\r\n*******三、更换域名+博客测试成功*******  # 前言\r\n今天一直在钻研这个博客，并搜索了大量有关hexo搭建博客的教程进行学习。我作为一个第一次使用Hexo搭建个人博客的菜鸟，我发现我踩了不少坑，哈哈，在这里我不得不吐槽一下某些撰写Hexo搭建个人博客的技术人员，用一个字来形容他们的博客就是“乱”，乱是因为我读完他们的博客写的内容发现逻辑顺序简直看得我一头雾水、细节内容对于他们来说就是一个摆设，难怪有好多人看不懂也是应该的。当然，可能是我的水平不够，也或许是在拜读他们的大作时候不够认真和严谨。\r\n但我还是要告诫一下一些技术编辑者：\r\n如果是怕别人偷学你的内容，那就不要发在网上；如果你发在网上，请考虑我们读者的感受，要对自己花费那么多时间撰写的内容负责，要让别人看懂你的文章，让别人欣赏你的作品。其实，有时候还能看出一个人的品性。\r\n吐槽到此结束~下面开始进入博客搭建环节\r\n\r\n值得你知道到话：\r\n是的，你没有看错！\r\n不用服务器，不用注册域名，不用花钱，不用敲大代码等等\r\n一个博客就值得你拥有\r\n\r\n\r\nimage\r\n\r\n\r\n一、从创建到部署博客**\r\n1、安装好Node.js\r\n别忘了用命令npm检验Node.js安装是否完成，\r\n关于hexo的安装教程比较简单，网上有很多完整的教程，在这里就不再赘述。****\r\n安装Hexo 命令：\r\nnpm install -g hexo-cli 补充:安装hexo-helper-live2d\r\n看门动画插件 npm install --save hexo-helper-live2d\r\n一定要在你博客目录的指定路径下（E:\\hexo\\KangChou）执行，否则在node_modules之中安装不了\nnpm uninstall hexo-helper-live2d  \nnpm install --save hexo-helper-live2d\nnpm install live2d-widget-model-hibiki \nnpm install npm install --save live2d-widget-model-xxx来安装你喜欢的模型\n参考：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;349278862 参考文献: https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_36239569&#x2F;article&#x2F;details&#x2F;104104894\nhttps:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;350654582 \n解决办法：\n首先\nnpm config set proxy null 代理置为空\n\n运行npm cache clean --force清理缓存\n\n然后尝试执行\nnpm config set registry http:&#x2F;&#x2F;registry.npmjs.org&#x2F;\n如果嫌安装依赖慢的话 可以使用国内淘宝镜像\nnpm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org 国内镜像下载就是快： \r\n做完了这一步之后，恭喜你，前期的准备工作已经完成，环境这一步结束了。\r\n2、安装好Git\r\n\r\n\r\nimage\r\n\r\n3、在C盘下创建hexo文件夹\r\n\r\n\r\nimage\r\n\r\n\r\n4、打开Hexo文件夹下，右键点击Git bash 下执行命令\r\n工程文件目录：\r\n\r\n\r\nimage\r\n\r\n再使用一次这个命令：npm install hexo-cli\r\n-g在终端使用npm安装hexo\r\n创建博客KangChou：hexo init KangChou\r\ncd KangChou\r\nnpm install\r\n\r\n\r\nimage\r\n\r\n\r\n\r\nimage\r\n\r\n\r\n5、命令hexo server启动github服务器\r\n\r\n\r\nimage\r\n\r\n\r\n6、浏览博客\r\n安照5中提示的网址http://localhost:4000/\r\n复制该网址在浏览器中打开，如下图所示：这样一个博客的架子就出来了。\r\n\r\n\r\nimage\r\n\r\n\r\n\r\n\r\n7、部署前哨（一）：添加Github仓库地址\r\n在部署之前，我们需要先知道博客的部署地址，它需要对应 GitHub 的一个\r\nRepository\r\n的地址，这个信息需要我们来配置一下。(这里我就省略了，自己去布置)，这是我的这个Github仓库\r\nhttps://github.com/KangChou/KangChou.github.io.git\r\n\r\n\r\nimage\r\n\r\n打开文件Hexo下的KangChou文件根目录下的 _config.yml\r\n文件，我使用编辑器Sublime\r\nText3打开的（或者你使用其他代码编辑器打开，千万别用文本编辑器打开），找到\r\nDeployment 这个地方(提示：文件最后)，把刚才新建的 Repository\r\n的地址贴过来\r\n\r\n\r\nimage\r\n\r\n\r\n\r\nimage\r\n\r\n\r\n8、部署前哨（二）：部署插件\r\n需要安装一个支持 Git 的部署插件：\r\nhexo-deployer-git，有了它我们才可以顺利将其部署到\r\nGitHub 上面(如果不安装的话，在执行部署命令时会报错).\r\n\r\n\r\nimage\r\n\r\n\r\n9、下面开始部署到Github\r\n如果8的插件部署没有问题就开始进行部署，首先输入部署命令如下：hexo\r\ndeploy\r\n\r\n\r\nimage\r\n\r\n结尾....\r\n\r\n\r\nimage\r\n\r\n可以发现出现了上面的报错提示：\r\nError: Spawn failed\r\n解决方法第一次：\r\n经搜索大量资料发现了下面的这个博客出现类似上述的一样问题，看了这个解决办法，我就斗胆试一试：\r\nhttps://blog.csdn.net/njc_sec/article/details/89021083\r\n\r\n\r\nimage\r\n\r\n可惜我再重新安装上述的三个命令输入之后还是出先一样的错误。\r\n解决方法第二次：\r\n因此我怀疑可能是仓库的地址出错，因此去看看了看地址\r\n这是原来的地址：\r\ndeploy:\r\n我按照出现错误提示中的网址去打开它：\r\nhttps://hexo.io/docs/troubleshooting.html\r\n\r\n\r\nimage\r\n\r\n并找到了部署到Github目前的语法规定的网页下\r\nhttps://hexo.io/docs/github-pages\r\n\r\n\r\nimage\r\n\r\n按照上面对部署仓库的地址，我将上述的Deploy的源码修改为\r\ndeploy:\r\n于是我再试了上述的三个命令：\r\nhexo clean\r\nhexo g\r\nhexo d\r\n最终出现下面的结果:说明出现的问题解决了\r\n\r\n\r\nimage\r\n\r\n由于我起初没有部署仓库的密钥，所以要去仓库部署。\r\n\r\n10、创建的 ssh 密钥的密码\r\n(1)、我打开了我得仓库，并找到了设置\r\n\r\n\r\nimage\r\n\r\n（2）查看部署密钥指南以了解更多信息\r\nhttps://developer.github.com/v3/guides/managing-deploy-keys/#deploy-keys\r\n找到了设置密码的步骤（经过翻译以后，目前我们进行到下面的5）\r\n\r\n\r\nimage\r\n\r\n（3）怎么创建SSH密钥?\r\n步骤：\r\n\r\n找到本地环境：C:.ssh 这个路径下的用户/名称/.ssh\r\n在这路径下，打开gitbub的命令控制台\r\n(I): git init //初始化一下，看看路径对不对\r\n(II):ssh-keygen -t rsa -C \"你的邮箱\"\r\n\r\n\r\n\r\nimage\r\n\r\n\r\n到本地环境.ssh路径下查看，是否生成id_rsa,id_rsa.pub这个两个文件\r\n\r\n\r\n\r\nimage\r\n\r\n\r\n生成后，现在把id_rsa.pub里面的内容复制到githubd 的add github key\r\n的key里面（也就是刚刚仓库的密钥添加的地方）\r\n\r\n\r\n\r\nimage\r\n\r\n点击Add SSH key获得下面结果\r\n注意：第一次提交，配置密钥，需要输入github的密码，如下就是添加秘钥成功\r\n\r\n\r\nimage\r\n\r\n\r\n密钥配置成功后，要验证一下是否配置成功\r\n命令：ssh -T git@github.com\r\n\r\n\r\n\r\nimage\r\n\r\n出现下面提示，说明配置成功。\r\nHi KangChou/KangChou.github.io! You've successfully authenticated,\r\nbut GitHub does not provide shell access\r\n\r\n11、再次使用密钥部署\r\n仍然使用命令：\r\nhexo clean\r\nhexo g\r\nhexo d\r\n如果都没有问题就会出现下面的结果，输入你刚刚设置的名和SSH的密码\r\n\r\n\r\nimage\r\n\r\n但是这里又出错了\r\n\r\n\r\nimage\r\n\r\n追根溯源，我怀疑还是部署的仓库出现了问题：因此我再次打开，并做了修改\r\n\r\n\r\nimage\r\n\r\n命令hexo d执行又出错\r\n\r\n\r\nimage\r\n\r\n下面终于找到了答案。这里要特别感谢@李典金 @崔庆才两位网络开发大佬的细节点拨才通过了上面的一个小环节，从而我力挽狂澜，一气呵成，搭建了后面的所有框架。\r\n备注：ssh-keygen -t rsa -C\r\n\"kangchou666@gmail.com\"不用输入密码，回车就可以生成。如果需要严密一点，可以进行加密功能的部署。\r\n\r\n\r\nimage\r\n\r\n\r\n12、终于部署成功\r\n最后终于找到了错误的原因，这是因为我创建的仓库下Github的SSH错误\r\n因此，我去仓库找到了\r\n\r\n\r\nimage\r\n\r\n将红色的部分复制到hexo文件目录下也就是你的博客文件末尾，打开修改如下\r\n\r\n\r\nimage\r\n\r\n再次输入命令hexo d执行以后出现下面的结果，\r\n出现Deploy done :git说明已经部署成功\r\n\r\n\r\nimage\r\n\r\n这时候我们访问一下 GitHub Repository 同名的链接，\r\n比如我的 KangChou 博客的 Repository 名称取的是\r\nKangChou.github.io，\r\n那我就访问 http://KangChou.github.io\r\n这时候我们就可以看到跟本地一模一样的博客内容了。\r\n（此时你用手机同样可以打开该网站）\r\n\r\n\r\nimage\r\n\r\n二、博客的网页主题**\r\n主题的设置包括中文页面、整个页面的样式、页面风格等等，\r\n目前 Hexo 里面应用最多的主题基本就是 Next 主题，\r\n这个主题还是挺好看的，并且它支持的一些插件和功能都极为丰富，\r\n配置了这个主题，我们的博客可以支持更多的扩展功能，比如阅览进度条、中英文空格排版、图片懒加载等等。\r\n\r\n13、下载主题\r\n打开我的电脑创建的Hexo文件夹下的KangChou目录，\r\n单击右键Git bush输入下面的命令，执行结果如下：\r\ngit clone https://github.com/theme-next/hexo-theme-next\r\nthemes/next\r\n将下载后的themes主题替换原文件landscape中里所有的文件，并输入启动服务器命令\r\nhexo server\r\n执行结果如下\r\n\r\n\r\nimage\r\n\r\n\r\n备注：使用数学公式需要安装这个工具：npm install hexo-math\r\n14、配置中文环境\r\n在博客kangchou目录下打开_config.yml修改语言为中文汉语zh-Hans\r\n\r\n\r\nimage\r\n\r\n执行的结果如下\r\n\r\n\r\nimage\r\n\r\n由于这只是部分为中文，而我的目的是大部分是中文的，\r\n为了方便还要在网页上手动添加更多中文描述\r\n\r\n****15、配置中文菜单栏****\r\n打开C:\r\n发有三种汉语:简体中文、香港繁体、台湾繁体\r\n\r\n\r\nimage\r\n\r\n然后点开zh-Hans.yml其中的配置项就是已经翻译的文本\r\n\r\n\r\nimage\r\n\r\n网站会根据你站点配置``yml中的语言配置来去读取对应的语言文件\r\n打开你languages``皮肤配置``yml你会看到菜单栏基础配置：\r\n\r\n\r\nimage\r\n\r\n发现home和archives菜单是开启的，\r\n现在我们全部开启，只需要去掉前面的#，刷新浏览器\r\n\r\n\r\nimage\r\n\r\n尝试修改站点配置yml语言，重启服务后刷新浏览器\r\n\r\n\r\nimage\r\n\r\n显然结果很成功，下面关闭git,将结果上传到Github页面：\r\n重新打开输入部署的三个命令：\r\nhexo clean\r\nhexo g\r\nhexo d\r\n结果和上面一样，此时就可以访问了.\r\n访问网站https://kangchou.github.io/\r\n\r\n\r\nimage\r\n\r\n\r\n实际上文章到这里就已经结束博客的搭建了，至于其他的\r\n比如上传文章、上传图片，添加logo等这些我这里就不说了，\r\nhexo官网以及其他网站都能搜索到具体的教程，\r\n想继续完善博客网站部署的朋友可以去搜索相关文献学习。\r\n三、更换域名**\r\n相信所有做互联网开发的科技工作者都知道，如果拥有属于自己的网站一定得看起来很专业、很官方、很大气。因此，有些科技工作者就想更换自己网站的域名，让自己的域名看起来官方标准。也还有另一个原因，因为Github毕竟是外国网站，国内用户访问相对较慢，因此，如果有国内的域名作为辅助会事半功倍。事实上，我个人觉得只要可以搭建网站，即便是不换域名也没什么区别。不过，既然我给大家写这个教程，我还是有必要说一下，毕竟有很多人还是愿意换域名的。如果不想花钱买域名的，这一小节可以跳过。******16、购买域名+注册阿里网+实名认证********自行注册，如果你是在校大学生，包括硕士、博士购买域名都是有学生价优惠的，但是****一定要使用自己在学校注册的电子邮箱****，因为阿里云官网数据库可以识别你的学生信息的学年期限。此外，注册以后一定要进行学生认证、实名认证。******\r\n然后去买域名，域名的形式有很多，按照自己的需求进行设置域名名称和域名后缀。（实名认证最快是两天的时间）https://www.aliyun.com/\r\n\r\n\r\nimage\r\n\r\n************\r\n******17、在阿里云添加域名解析******\r\ncmd+ping你的http://github.io域名，得到一个IP\r\n\r\n\r\nimage\r\n\r\n修改你的域名解析记录\r\n添加两个A记录，用得到的IP，一个主机记录为：“www”，一个为“@”，\r\n这样通过https://coomatrix.com/就能访问到你的博客了\r\n\r\n\r\nimage\r\n\r\n\r\n\r\nimage\r\n\r\n******18、******填写绑定的域名在你的本地文件下也就是hexo—&gt;你的博客（我的是KangChou）本地目录下找到\r\n文件夹source\r\n，并在该文件目录下面新建一个文件CNAME文件，那么一定要注意创建的CNAME文件没有任何扩展名（切记）\r\n\r\n\r\nimage\r\n\r\n再一次使用部署三命令hexo cleanhexo ghexo\r\nd完成以后，进入Github设置，找到 Custom\r\ndomain添加域名后保存即可\r\n\r\n\r\nimage\r\n\r\n******19、刷新网页+更改域名成功******\r\n如果上面的17没有出错的话，那么你填完域名保存以后会出现下面的结果\r\n\r\n\r\nimage\r\n\r\n那么就是更改域名成功了，此时你只需要点击上图的域名就可以访问啦。\r\n\r\n\r\nimage\r\n\r\n……到此完成了本博客的搭建……\r\n\r\n\r\nimage\r\n\r\n投稿---&gt;展示你的才华\r\n请发邮件到\r\nkangsinx@yeah.net\r\n标题注明【投稿】\r\n告诉我们\r\n你是谁，从哪来，投什么\r\n我们会及时回复你\r\n\r\n\r\nimage\r\n\r\n","slug":"blog","date":"2020-04-05T14:57:06.000Z","categories_index":"教程学习","tags_index":"博客","author_index":"AIGC生成式人工智能开发者"}]